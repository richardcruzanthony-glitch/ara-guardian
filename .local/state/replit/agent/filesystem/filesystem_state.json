{"file_contents":{"docs/mastra/06-reference/15_agent-chunk-type.md":{"content":"---\ntitle: \"Reference: ChunkType | Agents | Mastra Docs\"\ndescription: \"Documentation for the ChunkType type used in Mastra streaming responses, defining all possible chunk types and their payloads.\"\n---\n\nimport { Callout } from \"nextra/components\";\nimport { PropertiesTable } from \"@/components/properties-table\";\n\n# ChunkType\n[EN] Source: https://mastra.ai/en/reference/streaming/ChunkType\n\nThe `ChunkType` type defines the mastra format of stream chunks that can be emitted during streaming responses from agents.\n\n## Base Properties\n\nAll chunks include these base properties:\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: \"string\",\n      description: \"The specific chunk type identifier\"\n    },\n    {\n      name: \"runId\",\n      type: \"string\",\n      description: \"Unique identifier for this execution run\"\n    },\n    {\n      name: \"from\",\n      type: \"ChunkFrom\",\n      description: \"Source of the chunk\",\n      properties: [{\n        type: \"enum\",\n        parameters: [\n          { name: \"AGENT\", type: \"'AGENT'\", description: \"Chunk from agent execution\" },\n          { name: \"USER\", type: \"'USER'\", description: \"Chunk from user input\" },\n          { name: \"SYSTEM\", type: \"'SYSTEM'\", description: \"Chunk from system processes\" },\n          { name: \"WORKFLOW\", type: \"'WORKFLOW'\", description: \"Chunk from workflow execution\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n## Text Chunks\n\n### text-start\n\nSignals the beginning of text generation.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"text-start\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"TextStartPayload\",\n      description: \"Text start information\",\n      properties: [{\n        type: \"TextStartPayload\",\n        parameters: [\n          { name: \"id\", type: \"string\", description: \"Unique identifier for this text generation\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### text-delta\n\nIncremental text content during generation.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"text-delta\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"TextDeltaPayload\",\n      description: \"Incremental text content\",\n      properties: [{\n        type: \"TextDeltaPayload\",\n        parameters: [\n          { name: \"id\", type: \"string\", description: \"Unique identifier for this text generation\" },\n          { name: \"text\", type: \"string\", description: \"The incremental text content\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### text-end\n\nSignals the end of text generation.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"text-end\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"TextEndPayload\",\n      description: \"Text end information\",\n      properties: [{\n        type: \"TextEndPayload\",\n        parameters: [\n          { name: \"id\", type: \"string\", description: \"Unique identifier for this text generation\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n## Reasoning Chunks\n\n### reasoning-start\n\nSignals the beginning of reasoning generation (for models that support reasoning).\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"reasoning-start\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"ReasoningStartPayload\",\n      description: \"Reasoning start information\",\n      properties: [{\n        type: \"ReasoningStartPayload\",\n        parameters: [\n          { name: \"id\", type: \"string\", description: \"Unique identifier for this reasoning generation\" },\n          { name: \"signature\", type: \"string\", isOptional: true, description: \"Reasoning signature if available\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### reasoning-delta\n\nIncremental reasoning text during generation.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"reasoning-delta\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"ReasoningDeltaPayload\",\n      description: \"Incremental reasoning content\",\n      properties: [{\n        type: \"ReasoningDeltaPayload\",\n        parameters: [\n          { name: \"id\", type: \"string\", description: \"Unique identifier for this reasoning generation\" },\n          { name: \"text\", type: \"string\", description: \"The incremental reasoning text\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### reasoning-end\n\nSignals the end of reasoning generation.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"reasoning-end\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"ReasoningEndPayload\",\n      description: \"Reasoning end information\",\n      properties: [{\n        type: \"ReasoningEndPayload\",\n        parameters: [\n          { name: \"id\", type: \"string\", description: \"Unique identifier for this reasoning generation\" },\n          { name: \"signature\", type: \"string\", isOptional: true, description: \"Final reasoning signature if available\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### reasoning-signature\n\nContains the reasoning signature from models that support advanced reasoning (like OpenAI's o1 series). The signature represents metadata about the model's internal reasoning process, such as effort level or reasoning approach, but not the actual reasoning content itself.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"reasoning-signature\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"ReasoningSignaturePayload\",\n      description: \"Metadata about the model's reasoning process characteristics\",\n      properties: [{\n        type: \"ReasoningSignaturePayload\",\n        parameters: [\n          { name: \"id\", type: \"string\", description: \"Unique identifier for the reasoning session\" },\n          { name: \"signature\", type: \"string\", description: \"Signature describing the reasoning approach or effort level (e.g., reasoning effort settings)\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n## Tool Chunks\n\n### tool-call\n\nA tool is being called.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"tool-call\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"ToolCallPayload\",\n      description: \"Tool call information\",\n      properties: [{\n        type: \"ToolCallPayload\",\n        parameters: [\n          { name: \"toolCallId\", type: \"string\", description: \"Unique identifier for this tool call\" },\n          { name: \"toolName\", type: \"string\", description: \"Name of the tool being called\" },\n          { name: \"args\", type: \"Record<string, any>\", isOptional: true, description: \"Arguments passed to the tool\" },\n          { name: \"providerExecuted\", type: \"boolean\", isOptional: true, description: \"Whether the provider executed the tool\" },\n          { name: \"output\", type: \"any\", isOptional: true, description: \"Tool output if available\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### tool-result\n\nResult from a tool execution.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"tool-result\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"ToolResultPayload\",\n      description: \"Tool execution result\",\n      properties: [{\n        type: \"ToolResultPayload\",\n        parameters: [\n          { name: \"toolCallId\", type: \"string\", description: \"Unique identifier for the tool call\" },\n          { name: \"toolName\", type: \"string\", description: \"Name of the executed tool\" },\n          { name: \"result\", type: \"any\", description: \"The result of the tool execution\" },\n          { name: \"isError\", type: \"boolean\", isOptional: true, description: \"Whether the result is an error\" },\n          { name: \"providerExecuted\", type: \"boolean\", isOptional: true, description: \"Whether the provider executed the tool\" },\n          { name: \"args\", type: \"Record<string, any>\", isOptional: true, description: \"Arguments that were passed to the tool\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### tool-call-input-streaming-start\n\nSignals the start of streaming tool call arguments.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"tool-call-input-streaming-start\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"ToolCallInputStreamingStartPayload\",\n      description: \"Tool call input streaming start information\",\n      properties: [{\n        type: \"ToolCallInputStreamingStartPayload\",\n        parameters: [\n          { name: \"toolCallId\", type: \"string\", description: \"Unique identifier for this tool call\" },\n          { name: \"toolName\", type: \"string\", description: \"Name of the tool being called\" },\n          { name: \"providerExecuted\", type: \"boolean\", isOptional: true, description: \"Whether the provider executed the tool\" },\n          { name: \"dynamic\", type: \"boolean\", isOptional: true, description: \"Whether the tool call is dynamic\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### tool-call-delta\n\nIncremental tool call arguments during streaming.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"tool-call-delta\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"ToolCallDeltaPayload\",\n      description: \"Incremental tool call arguments\",\n      properties: [{\n        type: \"ToolCallDeltaPayload\",\n        parameters: [\n          { name: \"argsTextDelta\", type: \"string\", description: \"Incremental text delta for tool arguments\" },\n          { name: \"toolCallId\", type: \"string\", description: \"Unique identifier for this tool call\" },\n          { name: \"toolName\", type: \"string\", isOptional: true, description: \"Name of the tool being called\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### tool-call-input-streaming-end\n\nSignals the end of streaming tool call arguments.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"tool-call-input-streaming-end\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"ToolCallInputStreamingEndPayload\",\n      description: \"Tool call input streaming end information\",\n      properties: [{\n        type: \"ToolCallInputStreamingEndPayload\",\n        parameters: [\n          { name: \"toolCallId\", type: \"string\", description: \"Unique identifier for this tool call\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### tool-error\n\nAn error occurred during tool execution.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"tool-error\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"ToolErrorPayload\",\n      description: \"Tool error information\",\n      properties: [{\n        type: \"ToolErrorPayload\",\n        parameters: [\n          { name: \"id\", type: \"string\", isOptional: true, description: \"Optional identifier\" },\n          { name: \"toolCallId\", type: \"string\", description: \"Unique identifier for the tool call\" },\n          { name: \"toolName\", type: \"string\", description: \"Name of the tool that failed\" },\n          { name: \"args\", type: \"Record<string, any>\", isOptional: true, description: \"Arguments that were passed to the tool\" },\n          { name: \"error\", type: \"unknown\", description: \"The error that occurred\" },\n          { name: \"providerExecuted\", type: \"boolean\", isOptional: true, description: \"Whether the provider executed the tool\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n## Source and File Chunks\n\n### source\n\nContains source information for content.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"source\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"SourcePayload\",\n      description: \"Source information\",\n      properties: [{\n        type: \"SourcePayload\",\n        parameters: [\n          { name: \"id\", type: \"string\", description: \"Unique identifier\" },\n          { name: \"sourceType\", type: \"'url' | 'document'\", description: \"Type of source\" },\n          { name: \"title\", type: \"string\", description: \"Title of the source\" },\n          { name: \"mimeType\", type: \"string\", isOptional: true, description: \"MIME type of the source\" },\n          { name: \"filename\", type: \"string\", isOptional: true, description: \"Filename if applicable\" },\n          { name: \"url\", type: \"string\", isOptional: true, description: \"URL if applicable\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### file\n\nContains file data.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"file\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"FilePayload\",\n      description: \"File data\",\n      properties: [{\n        type: \"FilePayload\",\n        parameters: [\n          { name: \"data\", type: \"string | Uint8Array\", description: \"The file data\" },\n          { name: \"base64\", type: \"string\", isOptional: true, description: \"Base64 encoded data if applicable\" },\n          { name: \"mimeType\", type: \"string\", description: \"MIME type of the file\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n## Control Chunks\n\n### start\n\nSignals the start of streaming.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"start\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"StartPayload\",\n      description: \"Start information\",\n      properties: [{\n        type: \"StartPayload\",\n        parameters: [\n          { name: \"[key: string]\", type: \"any\", description: \"Additional start data\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### step-start\n\nSignals the start of a processing step.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"step-start\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"StepStartPayload\",\n      description: \"Step start information\",\n      properties: [{\n        type: \"StepStartPayload\",\n        parameters: [\n          { name: \"messageId\", type: \"string\", isOptional: true, description: \"Optional message identifier\" },\n          { name: \"request\", type: \"object\", description: \"Request information including body and other data\" },\n          { name: \"warnings\", type: \"LanguageModelV2CallWarning[]\", isOptional: true, description: \"Any warnings from the language model call\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### step-finish\n\nSignals the completion of a processing step.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"step-finish\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"StepFinishPayload\",\n      description: \"Step completion information\",\n      properties: [{\n        type: \"StepFinishPayload\",\n        parameters: [\n          { name: \"id\", type: \"string\", isOptional: true, description: \"Optional identifier\" },\n          { name: \"messageId\", type: \"string\", isOptional: true, description: \"Optional message identifier\" },\n          { name: \"stepResult\", type: \"object\", description: \"Step execution result with reason, warnings, and continuation info\" },\n          { name: \"output\", type: \"object\", description: \"Output information including usage statistics\" },\n          { name: \"metadata\", type: \"object\", description: \"Execution metadata including request and provider info\" },\n          { name: \"totalUsage\", type: \"LanguageModelV2Usage\", isOptional: true, description: \"Total usage statistics\" },\n          { name: \"response\", type: \"LanguageModelV2ResponseMetadata\", isOptional: true, description: \"Response metadata\" },\n          { name: \"providerMetadata\", type: \"SharedV2ProviderMetadata\", isOptional: true, description: \"Provider-specific metadata\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### raw\n\nContains raw data from the provider.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"raw\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"RawPayload\",\n      description: \"Raw provider data\",\n      properties: [{\n        type: \"RawPayload\",\n        parameters: [\n          { name: \"[key: string]\", type: \"any\", description: \"Raw data from the provider\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### finish\n\nStream has completed successfully.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"finish\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"FinishPayload\",\n      description: \"Completion information\",\n      properties: [{\n        type: \"FinishPayload\",\n        parameters: [\n          { name: \"stepResult\", type: \"object\", description: \"Step execution result\" },\n          { name: \"output\", type: \"object\", description: \"Output information including usage\" },\n          { name: \"metadata\", type: \"object\", description: \"Execution metadata\" },\n          { name: \"messages\", type: \"object\", description: \"Message history\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### error\n\nAn error occurred during streaming.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"error\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"ErrorPayload\",\n      description: \"Error information\",\n      properties: [{\n        type: \"ErrorPayload\",\n        parameters: [\n          { name: \"error\", type: \"unknown\", description: \"The error that occurred\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### abort\n\nStream was aborted.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"abort\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"AbortPayload\",\n      description: \"Abort information\",\n      properties: [{\n        type: \"AbortPayload\",\n        parameters: [\n          { name: \"[key: string]\", type: \"any\", description: \"Additional abort data\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n## Object and Output Chunks\n\n### object\n\nEmitted when using output generation with defined schemas. Contains partial or complete structured data that conforms to the specified Zod or JSON schema. This chunk is typically skipped in some execution contexts and used for streaming structured object generation.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"object\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"object\",\n      type: \"PartialSchemaOutput<OUTPUT>\",\n      description: \"Partial or complete structured data matching the defined schema. The type is determined by the OUTPUT schema parameter.\"\n    }\n  ]}\n/>\n\n### tool-output\n\nContains output from agent or workflow execution, particularly used for tracking usage statistics and completion events. Often wraps other chunk types (like finish chunks) to provide nested execution context.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"tool-output\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"ToolOutputPayload\",\n      description: \"Wrapped execution output with metadata\",\n      properties: [{\n        type: \"ToolOutputPayload\",\n        parameters: [\n          { name: \"output\", type: \"ChunkType\", description: \"Nested chunk data, often containing finish events with usage statistics\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### step-output\n\nContains output from workflow step execution, used primarily for usage tracking and step completion events. Similar to tool-output but specifically for individual workflow steps.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"step-output\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"StepOutputPayload\",\n      description: \"Workflow step execution output with metadata\",\n      properties: [{\n        type: \"StepOutputPayload\",\n        parameters: [\n          { name: \"output\", type: \"ChunkType\", description: \"Nested chunk data from step execution, typically containing finish events or other step results\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n## Metadata and Special Chunks\n\n### response-metadata\n\nContains metadata about the LLM provider's response. Emitted by some providers after text generation to provide additional context like model ID, timestamps, and response headers. This chunk is used internally for state tracking and doesn't affect message assembly.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"response-metadata\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"ResponseMetadataPayload\",\n      description: \"Provider response metadata for tracking and debugging\",\n      properties: [{\n        type: \"ResponseMetadataPayload\",\n        parameters: [\n          { name: \"signature\", type: \"string\", isOptional: true, description: \"Response signature if available\" },\n          { name: \"[key: string]\", type: \"any\", description: \"Additional provider-specific metadata fields (e.g., id, modelId, timestamp, headers)\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### watch\n\nContains monitoring and observability data from agent execution. Can include workflow state information, execution progress, or other runtime details depending on the context where `stream()` is used.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"watch\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"WatchPayload\",\n      description: \"Monitoring data for observability and debugging of agent execution\",\n      properties: [{\n        type: \"WatchPayload\",\n        parameters: [\n          { name: \"workflowState\", type: \"object\", isOptional: true, description: \"Current workflow execution state (when used in workflows)\" },\n          { name: \"eventTimestamp\", type: \"number\", isOptional: true, description: \"Timestamp when the event occurred\" },\n          { name: \"[key: string]\", type: \"any\", description: \"Additional monitoring and execution data\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n### tripwire\n\nEmitted when the stream is forcibly terminated due to content being blocked by output processors. This acts as a safety mechanism to prevent harmful or inappropriate content from being streamed.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"type\",\n      type: '\"tripwire\"',\n      description: \"Chunk type identifier\"\n    },\n    {\n      name: \"payload\",\n      type: \"TripwirePayload\",\n      description: \"Information about why the stream was terminated by safety mechanisms\",\n      properties: [{\n        type: \"TripwirePayload\",\n        parameters: [\n          { name: \"tripwireReason\", type: \"string\", description: \"Explanation of why the content was blocked (e.g., 'Output processor blocked content')\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n## Usage Example\n\n```typescript\nconst stream = await agent.stream(\"Hello\");\n\nfor await (const chunk of stream.fullStream) {\n  switch (chunk.type) {\n    case 'text-delta':\n      console.log('Text:', chunk.payload.text);\n      break;\n\n    case 'tool-call':\n      console.log('Calling tool:', chunk.payload.toolName);\n      break;\n\n    case 'tool-result':\n      console.log('Tool result:', chunk.payload.result);\n      break;\n\n    case 'reasoning-delta':\n      console.log('Reasoning:', chunk.payload.text);\n      break;\n\n    case 'finish':\n      console.log('Finished:', chunk.payload.stepResult.reason);\n      console.log('Usage:', chunk.payload.output.usage);\n      break;\n\n    case 'error':\n      console.error('Error:', chunk.payload.error);\n      break;\n  }\n}\n```\n\n## Related Types\n\n- [.stream()](./agents/stream.mdx) - Method that returns streams emitting these chunks\n- [MastraModelOutput](./agents/MastraModelOutput.mdx) - The stream object that emits these chunks\n- [workflow.streamVNext()](./workflows/streamVNext.mdx) - Method that returns streams emitting these chunks for workflows\n\n\n","path":null,"size_bytes":25716,"size_tokens":null},"docs/mastra/06-reference/33_mastra-get-logger.md":{"content":"---\ntitle: \"Reference: Mastra.getLogger() | Core | Mastra Docs\"\ndescription: \"Documentation for the `Mastra.getLogger()` method in Mastra, which retrieves the configured logger instance.\"\n---\n\n# Mastra.getLogger()\n[EN] Source: https://mastra.ai/en/reference/core/getLogger\n\nThe `.getLogger()` method is used to retrieve the logger instance that has been configured in the Mastra instance.\n\n## Usage example\n\n```typescript copy\nmastra.getLogger();\n```\n\n## Parameters\n\nThis method does not accept any parameters.\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"logger\",\n      type: \"TLogger\",\n      description: \"The configured logger instance used for logging across all components (agents, workflows, etc.).\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Logging overview](../../docs/observability/logging.mdx)\n- [Logger reference](../../reference/observability/logger.mdx)\n\n\n","path":null,"size_bytes":876,"size_tokens":null},"docs/mastra/06-reference/80_create-tool.md":{"content":"---\ntitle: \"Reference: createTool() | Tools | Mastra Docs\"\ndescription: Documentation for the `createTool()` function in Mastra, used to define custom tools for agents.\n---\n\n# createTool()\n[EN] Source: https://mastra.ai/en/reference/tools/create-tool\n\nThe `createTool()` function is used to define custom tools that your Mastra agents can execute. Tools extend an agent's capabilities by allowing it to interact with external systems, perform calculations, or access specific data.\n\n## Usage example\n\n```typescript filename=\"src/mastra/tools/reverse-tool.ts\" showLineNumbers copy\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nexport const tool = createTool({\n  id: \"test-tool\",\n  description: \"Reverse the input string\",\n  inputSchema: z.object({\n    input: z.string()\n  }),\n  outputSchema: z.object({\n    output: z.string()\n  }),\n  execute: async ({ context }) => {\n    const { input } = context;\n    const reversed = input.split(\"\").reverse().join(\"\");\n\n    return {\n      output: reversed\n    };\n  }\n});\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"id\",\n      type: \"string\",\n      description: \"A unique identifier for the tool.\",\n      isOptional: false,\n    },\n    {\n      name: \"description\",\n      type: \"string\",\n      description:\n        \"A description of what the tool does. This is used by the agent to decide when to use the tool.\",\n      isOptional: false,\n    },\n    {\n      name: \"inputSchema\",\n      type: \"Zod schema\",\n      description:\n        \"A Zod schema defining the expected input parameters for the tool's `execute` function.\",\n      isOptional: true,\n    },\n    {\n      name: \"outputSchema\",\n      type: \"Zod schema\",\n      description:\n        \"A Zod schema defining the expected output structure of the tool's `execute` function.\",\n      isOptional: true,\n    },\n    {\n      name: \"execute\",\n      type: \"function\",\n      description:\n        \"The function that contains the tool's logic. It receives an object with `context` (the parsed input based on `inputSchema`), `runtimeContext`, `tracingContext`, and an object containing `abortSignal`.\",\n      isOptional: false,\n      properties: [\n        {\n          parameters: [{\n            name: \"context\",\n            type: \"z.infer<TInput>\",\n            description: \"The parsed input based on inputSchema\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"runtimeContext\",\n            type: \"RuntimeContext\",\n            isOptional: true,\n            description: \"Runtime context for accessing shared state and dependencies\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"tracingContext\",\n            type: \"TracingContext\",\n            isOptional: true,\n            description: \"AI tracing context for creating child spans and adding metadata. Automatically injected when the tool is called within a traced operation.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"abortSignal\",\n            type: \"AbortSignal\",\n            isOptional: true,\n            description: \"Signal for aborting the tool execution\"\n          }]\n        }\n      ]\n    },\n  ]}\n/>\n\n## Returns\n\nThe `createTool()` function returns a `Tool` object.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"Tool\",\n      type: \"object\",\n      description:\n        \"An object representing the defined tool, ready to be added to an agent.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Tools Overview](/docs/tools-mcp/overview.mdx)\n- [Using Tools with Agents](/docs/agents/using-tools-and-mcp.mdx)\n- [Tool Runtime Context](/docs/tools-mcp/overview.mdx#using-runtimecontext)\n- [Advanced Tool Usage](/docs/tools-mcp/advanced-usage.mdx)\n\n\n","path":null,"size_bytes":3711,"size_tokens":null},"docs/mastra/06-reference/102_workflow-then.md":{"content":"---\ntitle: \"Reference: Workflow.then() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow.then()` method in workflows, which creates sequential dependencies between steps.\n---\n\n# Workflow.then()\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow-methods/then\n\nThe `.then()` method creates a sequential dependency between workflow steps, ensuring steps execute in a specific order.\n\n## Usage example\n\n```typescript copy\nworkflow.then(step1).then(step2);\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"step\",\n      type: \"Step\",\n      description:\n        \"The step instance that should execute after the previous step completes\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"NewWorkflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Control flow](../../../docs/workflows/control-flow.mdx)\n\n\n","path":null,"size_bytes":977,"size_tokens":null},"docs/mastra/03-workflows/16_streaming-events.md":{"content":"---\ntitle: \"Streaming Events | Streaming | Mastra\"\ndescription: \"Learn about the different types of streaming events in Mastra, including text deltas, tool calls, step events, and how to handle them in your applications.\"\n---\n\n# Streaming Events\n[EN] Source: https://mastra.ai/en/docs/streaming/events\n\nStreaming from agents or workflows provides real-time visibility into either the LLMâ€™s output or the status of a workflow run. This feedback can be passed directly to the user, or used within applications to handle workflow status more effectively, creating a smoother and more responsive experience.\n\nEvents emitted from agents or workflows represent different stages of generation and execution, such as when a run starts, when text is produced, or when a tool is invoked.\n\n## Event types\n\nBelow is a complete list of events emitted from `.stream()`.\nDepending on whether youâ€™re streaming from an **agent** or a **workflow**, only a subset of these events will occur:\n\n- **start**: Marks the beginning of an agent or workflow run.\n- **step-start**: Indicates a workflow step has begun execution.\n- **text-delta**: Incremental text chunks as they're generated by the LLM.\n- **tool-call**: When the agent decides to use a tool, including the tool name and arguments.\n- **tool-result**: The result returned from tool execution.\n- **step-finish**: Confirms that a specific step has fully finalized, and may include metadata like the finish reason for that step.\n- **finish**: When the agent or workflow completes, including usage statistics.\n\n## Network event types\n\nWhen using `agent.network()` for multi-agent collaboration, additional event types are emitted to track the orchestration flow:\n\n- **routing-agent-start**: The routing agent begins analyzing the task to decide which primitive (agent/workflow/tool) to delegate to.\n- **routing-agent-text-delta**: Incremental text as the routing agent processes the response from the selected primitive.\n- **routing-agent-end**: The routing agent completes its selection, including the selected primitive and reason for selection.\n- **agent-execution-start**: A delegated agent begins execution.\n- **agent-execution-end**: A delegated agent completes execution.\n- **agent-execution-event-\\***: Events from the delegated agent's execution (e.g., `agent-execution-event-text-delta`).\n- **workflow-execution-start**: A delegated workflow begins execution.\n- **workflow-execution-end**: A delegated workflow completes execution.\n- **workflow-execution-event-\\***: Events from the delegated workflow's execution.\n- **tool-execution-start**: A delegated tool begins execution.\n- **tool-execution-end**: A delegated tool completes execution.\n- **network-execution-event-step-finish**: A network iteration step completes.\n- **network-execution-event-finish**: The entire network execution completes.\n\n## Inspecting agent streams\n\nIterate over the `stream` with a `for await` loop to inspect all emitted event chunks.\n\n```typescript {3,7} showLineNumbers copy\nconst testAgent = mastra.getAgent(\"testAgent\");\n\nconst stream = await testAgent.stream([\n  { role: \"user\", content: \"Help me organize my day\" },\n]);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n```\n\n> See [Agent.stream()](../../reference/agents/stream.mdx) for more information.\n\n### Example agent output\n\nBelow is an example of events that may be emitted. Each event always includes a `type` and can include additional fields like `from` and `payload`.\n\n```typescript {2,7,15}\n{\n  type: 'start',\n  from: 'AGENT',\n  // ..\n}\n{\n  type: 'step-start',\n  from: 'AGENT',\n  payload: {\n    messageId: 'msg-cdUrkirvXw8A6oE4t5lzDuxi',\n    // ...\n  }\n}\n{\n  type: 'tool-call',\n  from: 'AGENT',\n  payload: {\n    toolCallId: 'call_jbhi3s1qvR6Aqt9axCfTBMsA',\n    toolName: 'testTool'\n    // ..\n  }\n}\n```\n\n## Inspecting workflow streams\n\nIterate over the `stream` with a `for await` loop to inspect all emitted event chunks.\n\n```typescript {5,11} showLineNumbers copy\nconst testWorkflow = mastra.getWorkflow(\"testWorkflow\");\n\nconst run = await testWorkflow.createRunAsync();\n\nconst stream = await run.stream({\n  inputData: {\n    value: \"initial data\"\n  }\n});\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n```\n\n### Example workflow output\n\nBelow is an example of events that may be emitted. Each event always includes a `type` and can include additional fields like `from` and `payload`.\n\n```typescript {2,8,11}\n{\n  type: 'workflow-start',\n  runId: '221333ed-d9ee-4737-922b-4ab4d9de73e6',\n  from: 'WORKFLOW',\n  // ...\n}\n{\n  type: 'workflow-step-start',\n  runId: '221333ed-d9ee-4737-922b-4ab4d9de73e6',\n  from: 'WORKFLOW',\n  payload: {\n    stepName: 'step-1',\n    args: { value: 'initial data' },\n    stepCallId: '9e8c5217-490b-4fe7-8c31-6e2353a3fc98',\n    startedAt: 1755269732792,\n    status: 'running'\n  }\n}\n```\n\n## Inspecting agent networks\n\nWhen using multi-agent collaboration with `agent.network()`, iterate over the stream to track how tasks are delegated and executed across agents, workflows, and tools.\n\n```typescript {3,5} showLineNumbers copy\nconst networkAgent = mastra.getAgent(\"networkAgent\");\n\nconst networkStream = await networkAgent.network(\"Research dolphins then write a report\");\n\nfor await (const chunk of networkStream) {\n  console.log(chunk);\n}\n```\n\n> See [Agent.network()](../../reference/agents/network.mdx) for more information.\n\n### Example network output\n\nNetwork streams emit events that track the orchestration flow. Each iteration begins with routing, followed by execution of the selected primitive.\n\n```typescript {3,13,22,31}\n// Routing agent decides what to do\n{\n  type: 'routing-agent-start',\n  from: 'NETWORK',\n  runId: '7a3b9c2d-1e4f-5a6b-8c9d-0e1f2a3b4c5d',\n  payload: {\n    agentId: 'routing-agent',\n    // ...\n  }\n}\n// Routing agent makes a selection\n{\n  type: 'routing-agent-end',\n  from: 'NETWORK',\n  runId: '7a3b9c2d-1e4f-5a6b-8c9d-0e1f2a3b4c5d',\n  payload: {\n    // ...\n  }\n}\n// Delegated agent begins execution\n{\n  type: 'agent-execution-start',\n  from: 'NETWORK',\n  runId: '8b4c0d3e-2f5a-6b7c-9d0e-1f2a3b4c5d6e',\n  payload: {\n    // ...\n  }\n}\n// Events from the delegated agent's execution\n{\n  type: 'agent-execution-event-text-delta',\n  from: 'NETWORK',\n  runId: '8b4c0d3e-2f5a-6b7c-9d0e-1f2a3b4c5d6e',\n  payload: {\n    type: 'text-delta',\n    payload: {\n      // ...\n    }\n  }\n}\n// ...more events\n```\n\n### Filtering network events\n\nYou can filter events by type to track specific aspects of the network execution:\n\n```typescript {5-8,11-13,16-18} showLineNumbers copy\nconst networkStream = await networkAgent.network(\"Analyze data and create visualization\");\n\nfor await (const chunk of networkStream) {\n  // Track routing decisions\n  if (chunk.type === 'routing-agent-end') {\n    console.log('Selected:', chunk.payload.resourceType, chunk.payload.resourceId);\n    console.log('Reason:', chunk.payload.selectionReason);\n  }\n  \n  // Track agent delegations\n  if (chunk.type === 'agent-execution-start') {\n    console.log('Delegating to agent:', chunk.payload.agentId);\n  }\n  \n  // Track workflow delegations\n  if (chunk.type === 'workflow-execution-start') {\n    console.log('Executing workflow:', chunk.payload.name);\n  }\n}\n```\n\n\n","path":null,"size_bytes":7198,"size_tokens":null},"docs/triggers/webhook_connector_triggers.md":{"content":"# Connector Webhook Triggers\n\nCreate webhook handlers for third-party connectors (Linear, GitHub, Slack, etc.) that trigger Mastra workflows.\n\n**NOTE:** This document describes how dev/prod runs the full routing.  The agent may need to know how to query the Mastra server to run directly agents, workflows, etc.  You should disregard this documentation in that case and consult the Mastra doc (docs/mastra/**).\n\n## Quick Start\n\n**2 steps to add a connector webhook:**\n\n### 1. Create `src/triggers/{connector}Triggers.ts`\n\n```typescript\nimport { registerApiRoute } from \"../mastra/inngest\";\n\nexport function register{Connector}Trigger({ triggerType, handler }) {\n  return [\n    registerApiRoute(\"/{connector}/webhook\", {\n      method: \"POST\",\n      handler: async (c) => {\n        const mastra = c.get(\"mastra\");\n        const logger = mastra?.getLogger();\n        \n        try {\n          const payload = await c.req.json();\n          logger?.info(\"ðŸ“¥ [{Connector}] Webhook received\", { payload });\n          \n          // Only process events you care about (optional)\n          if (payload.action !== \"created\" && payload.action !== \"updated\") {\n            logger?.info(\"[{Connector}] Skipping event\", { action: payload.action });\n            return c.json({ success: true, skipped: true });\n          }\n          \n          // Pass the entire payload - let the consumer pick what they need\n          const triggerInfo = {\n            type: triggerType,\n            payload,\n          };\n          \n          const result = await handler(mastra, triggerInfo);\n          return c.json({ success: true, result });\n        } catch (error) {\n          logger?.error(\"Error:\", { error });\n          return c.json({ success: false, error: String(error) }, 500);\n        }\n      },\n    }),\n  ];\n}\n```\n\n### 2. Register in `src/mastra/index.ts`\n\n```typescript\nimport { register{Connector}Trigger } from \"../triggers/{connector}Triggers\";\nimport { exampleWorkflow } from \"./workflows/exampleWorkflow\";\n\n// In server > apiRoutes array:\n...register{Connector}Trigger({\n  triggerType: \"{connector}/{event}\",\n  handler: async (mastra, triggerInfo) => {\n    const logger = mastra?.getLogger();\n    \n    // Extract what you need from the payload with loud fallbacks\n    const data = triggerInfo.payload?.data || {};\n    \n    // Log when using fallbacks so you know something's missing\n    const title = data.title || data.name || (() => {\n      logger?.warn(\"[{Connector}] Missing title/name in payload, using fallback\", { data });\n      return \"[Missing name / title] Untitled\";\n    })();\n    \n    const run = await exampleWorkflow.createRunAsync();\n    return await run.start({ \n      inputData: { \n        message: title,\n        // Or pass the entire payload if your workflow needs it\n        // ...triggerInfo.payload\n      } \n    });\n  }\n}),\n```\n\nDone! Your webhook handler is now registered. See the [Inngest + Mastra Integration Guide](./dev-prod-replit.md) for details on how webhooks flow through the system and how to test them.\n\n## Key Principles\n\n**1. Fall back loudly:** Log when fields are missing so you can fix the integration\n\n```typescript\nconst id = payload?.data?.id || (() => {\n  logger?.warn(\"Missing ID in payload, generating one\", { payload });\n  return Date.now();\n})();\n```\n\n**2. Log everything:** Capture full payloads to understand what you're receiving\n\n```typescript\nlogger?.info(\"ðŸ“¥ Webhook received\", { payload });\n```\n\n**3. Keep it simple:** Don't validate webhook signatures or authenticate senders - focus on processing\n\n**4. Always try:** Wrap in try/catch and return 500 on error, but attempt to process any data without providing mocked data - make it obvious when fallbacks are used\n\n## Detailed Guide\n\n### Step 1: Find Webhook Documentation\n\nFind sample payloads in your connector's docs:\n\n- **Linear**: <https://developers.linear.app/docs/graphql/webhooks>\n- **GitHub**: <https://docs.github.com/en/webhooks>\n- **Stripe**: <https://stripe.com/docs/webhooks>\n\n### Step 2: Create the Trigger File\n\nCreate `src/triggers/{connectorName}Triggers.ts`:\n\n```typescript\nimport { registerApiRoute } from \"../mastra/inngest\";\n\nexport function register{Connector}Trigger({ triggerType, handler }) {\n  return [\n    registerApiRoute(\"/{connector}/webhook\", {\n      method: \"POST\",\n      handler: async (c) => {\n        const mastra = c.get(\"mastra\");\n        const logger = mastra?.getLogger();\n\n        try {\n          const payload = await c.req.json();\n          logger?.info(\"ðŸ“¥ [{Connector}] Webhook received\", { payload });\n\n          // Only process events you care about (optional)\n          if (payload.action !== \"created\" && payload.action !== \"updated\") {\n            logger?.info(\"[{Connector}] Skipping event\", { action: payload.action });\n            return c.json({ success: true, skipped: true });\n          }\n\n          // Pass the entire payload - let the consumer pick what they need\n          const triggerInfo = { type: triggerType, payload };\n\n          const result = await handler(mastra, triggerInfo);\n          return c.json({ success: true, result });\n        } catch (error) {\n          logger?.error(\"âŒ [{Connector}] Error\", { error });\n          return c.json({ success: false, error: String(error) }, 500);\n        }\n      },\n    }),\n  ];\n}\n```\n\n### Step 3: Register in `src/mastra/index.ts`\n\n```typescript\nimport { register{Connector}Trigger } from \"../triggers/{connector}Triggers\";\nimport { exampleWorkflow } from \"./workflows/exampleWorkflow\";\n\n// In server > apiRoutes array:\n...register{Connector}Trigger({\n  triggerType: \"{connector}/{event}\",\n  handler: async (mastra, triggerInfo) => {\n    const logger = mastra?.getLogger();\n    \n    // Extract what you need from the full payload\n    const data = triggerInfo.payload?.data || {};\n    \n    // Fall back loudly - log when using defaults\n    const title = data.title || data.name || (() => {\n      logger?.warn(\"[{Connector}] No title/name found, using default\", { data });\n      return \"[Missing name / title] Untitled\";\n    })();\n    \n    const run = await exampleWorkflow.createRunAsync();\n    return await run.start({\n      inputData: { message: title }\n    });\n  }\n}),\n```\n\n## Tips\n\n**Same name everywhere:** Use `{connector}` consistently in paths, event types, and file names\n\n**Fall back loudly:** When using fallbacks, always log what's missing:\n\n```typescript\nconst value = payload?.field || (() => {\n  logger?.warn(\"Missing field, using default\", { payload });\n  return \"[Missing field] default-value\";\n})();\n```\n\n**Log the payload:** `logger?.info(\"Webhook received\", { payload })`\n\n**Filter to events you want:** Only process specific actions (e.g., `created`, `updated`), skip the rest\n\n**Don't validate senders:** Focus on processing, not authentication\n\n## Examples\n\nSee these files for complete examples:\n\n- `exampleConnectorTrigger.ts` - Linear webhook handler (comprehensive example)\n- `slackTriggers.ts` - Slack webhook handler\n- `telegramTriggers.ts` - Telegram webhook handler\n\n## Architecture Notes\n\nThe `registerApiRoute` function creates:\n\n1. A webhook handler at `/{connector}/webhook`\n2. An Inngest forwarding function that ensures reliability through retries\n\nThe connector name is extracted from the **first path segment** of your route:\n\n- `/linear/webhook` â†’ connector name: \"linear\" â†’ listens for: `event/api.webhooks.linear.action`\n- `/github/webhook` â†’ connector name: \"github\" â†’ listens for: `event/api.webhooks.github.action`\n\n**Special Case - Slack/Telegram:**\nSlack and Telegram use paths like `/webhooks/slack/action` and `/webhooks/telegram/action`, where the first segment is \"webhooks\". This means:\n\n- Both connectors share the event name: `event/api.webhooks.webhooks.action`\n- Both share the function ID: `api-webhooks`\n- When testing, you must send the \"webhooks\" event name for both providers\n\nThis shared event name doesn't cause conflicts because each handler still validates its own specific payload structure internally. NOTE: This **only** applies to Slack and Telegram.\n\nIn production, webhooks flow through Replit's infrastructure and Inngest Cloud before reaching your handler. See the [Inngest + Mastra Integration Guide](./dev-prod-replit.md) for complete architectural details and testing instructions.\n\n## Troubleshooting\n\n### Webhook handler not triggered\n\n- Check that both servers are running in development (see Integration Guide)\n- Verify the webhook is registered in `src/mastra/index.ts`\n- Check Inngest dashboard for event processing\n\n### Missing fields in payload\n\n- Check your logs for warnings about missing fields\n- Compare logged payload with connector documentation\n- Update field extraction to match actual payload structure\n\n### Workflow not starting\n\n- Verify workflow is imported and registered\n- Check that inputData matches workflow's inputSchema\n- Review logs for validation errors\n","path":null,"size_bytes":8881,"size_tokens":null},"docs/mastra/06-reference/116_run-stream.md":{"content":"---\ntitle: \"Reference: Run.stream() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Run.stream()` method in workflows, which allows you to monitor the execution of a workflow run as a stream.\n---\n\n# Run.stream()\n[EN] Source: https://mastra.ai/en/reference/streaming/workflows/stream\n\nThe `.stream()` method allows you to monitor the execution of a workflow run, providing real-time updates on the status of steps.\n\n## Usage example\n\n```typescript showLineNumbers copy\nconst run = await workflow.createRunAsync();\n\nconst { stream } = await run.stream({\n  inputData: {\n    value: \"initial data\",\n  },\n});\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"inputData\",\n      type: \"z.infer<TInput>\",\n      description: \"Input data that matches the workflow's input schema\",\n      isOptional: true,\n    },\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      description: \"Runtime context data to use during workflow execution\",\n      isOptional: true,\n    },\n    {\n      name: \"tracingContext\",\n      type: \"TracingContext\",\n      isOptional: true,\n      description: \"AI tracing context for creating child spans and adding metadata. Automatically injected when using Mastra's tracing system.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"currentSpan\",\n            type: \"AISpan\",\n            isOptional: true,\n            description: \"Current AI span for creating child spans and adding metadata. Use this to create custom child spans or update span attributes during execution.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"tracingOptions\",\n      type: \"TracingOptions\",\n      isOptional: true,\n      description: \"Options for AI tracing configuration.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"metadata\",\n            type: \"Record<string, any>\",\n            isOptional: true,\n            description: \"Metadata to add to the root trace span. Useful for adding custom attributes like user IDs, session IDs, or feature flags.\"\n          }]\n        }\n      ]\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"stream\",\n      type: \"ReadableStream<StreamEvent>\",\n      description: \"A readable stream that emits workflow execution events in real-time\",\n    },\n    {\n      name: \"getWorkflowState\",\n      type: \"() => Promise<WorkflowResult<TState, TOutput, TSteps>>\",\n      description: \"A function that returns a promise resolving to the final workflow result\",\n    },\n    {\n      name: \"traceId\",\n      type: \"string\",\n      isOptional: true,\n      description: \"The trace ID associated with this execution when AI tracing is enabled. Use this to correlate logs and debug execution flow.\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript showLineNumbers copy\nconst { getWorkflowState } = await run.stream({\n  inputData: {\n    value: \"initial data\"\n  }\n});\n\nconst result = await getWorkflowState();\n```\n\n## Stream Events\n\nThe stream emits various event types during workflow execution. Each event has a `type` field and a `payload` containing relevant data:\n\n- **`start`**: Workflow execution begins\n- **`step-start`**: A step begins execution\n- **`tool-call`**: A tool call is initiated\n- **`tool-call-streaming-start`**: Tool call streaming begins\n- **`tool-call-delta`**: Incremental tool output updates\n- **`step-result`**: A step completes with results\n- **`step-finish`**: A step finishes execution\n- **`finish`**: Workflow execution completes\n\n\n## Related\n\n- [Workflows overview](../../../docs/workflows/overview.mdx#run-workflow)\n- [Workflow.createRunAsync()](../../../reference/workflows/workflow-methods/create-run.mdx)\n\n\n","path":null,"size_bytes":3693,"size_tokens":null},"src/triggers/exampleConnectorTrigger.ts":{"content":"/**\n * Example Connector Trigger - Linear Webhook Handler\n *\n * This demonstrates how to create a webhook handler for any connector.\n * Linear is just an example - replace with your connector name.\n *\n * PATTERN:\n * 1. Define types for the webhook payload (optional, but helpful)\n * 2. Create a registration function that sets up the webhook route\n * 3. Pass the full payload to the handler - let the consumer pick what they need\n * 4. Register in src/mastra/index.ts\n *\n * See docs/triggers/webhook_connector_triggers.md for complete guide.\n */\n\nimport { registerApiRoute } from \"../mastra/inngest\";\nimport type { Mastra } from \"@mastra/core\";\n\n/**\n * Linear webhook payload structure\n * Based on: https://developers.linear.app/docs/graphql/webhooks\n */\nexport type LinearWebhookPayload = {\n  action: string; // e.g., \"create\", \"update\", \"remove\"\n  type: string; // e.g., \"Issue\", \"Comment\", \"Project\"\n  data: {\n    id: string;\n    title: string;\n    description?: string;\n    [key: string]: any;\n  };\n  createdAt: string;\n  organizationId: string;\n  [key: string]: any;\n};\n\n/**\n * Trigger info passed to your handler\n */\nexport type TriggerInfoLinearIssueCreated = {\n  type: \"linear/issue.created\";\n  payload: LinearWebhookPayload;\n};\n\ntype LinearTriggerHandler = (\n  mastra: Mastra,\n  triggerInfo: TriggerInfoLinearIssueCreated,\n) => Promise<any>;\n\n/**\n * Register a Linear webhook trigger handler\n *\n * Usage in src/mastra/index.ts:\n *\n * ```typescript\n * import { exampleWorkflow } from \"./workflows/exampleWorkflow\";\n *\n * ...registerLinearTrigger({\n *   triggerType: \"linear/issue.created\",\n *   handler: async (mastra, triggerInfo) => {\n *     // Extract what you need from the payload\n *     const data = triggerInfo.payload?.data || {};\n *     const title = data.title || data.name || \"Untitled\";\n *\n *     // Start your workflow\n *     const run = await exampleWorkflow.createRunAsync();\n *     return await run.start({\n *       inputData: {\n *         message: `Linear Issue: ${title}`,\n *         includeAnalysis: true,\n *       }\n *     });\n *   }\n * })\n * ```\n */\nexport function registerLinearTrigger({\n  triggerType,\n  handler,\n}: {\n  triggerType: \"linear/issue.created\";\n  handler: LinearTriggerHandler;\n}) {\n  return [\n    registerApiRoute(\"/linear/webhook\", {\n      method: \"POST\",\n      handler: async (c) => {\n        const mastra = c.get(\"mastra\");\n        const logger = mastra?.getLogger();\n\n        try {\n          const payload = await c.req.json();\n          console.log(\"ðŸ“¥ [Linear] Webhook received\", { payload });\n\n          // Only process Issue creation events\n          if (payload.action !== \"create\" || payload.type !== \"Issue\") {\n            console.log(\"â­ï¸ [Linear] Skipping event\", {\n              action: payload.action,\n              type: payload.type,\n            });\n            return c.json({ success: true, skipped: true });\n          }\n\n          // Ensure data exists (use empty object as fallback)\n          if (!payload.data) {\n            console.log(\"âš ï¸ [Linear] Missing data field, using empty object\");\n            payload.data = {};\n          }\n\n          // Pass the full payload - let the consumer pick what they need\n          const triggerInfo: TriggerInfoLinearIssueCreated = {\n            type: triggerType,\n            payload: payload as LinearWebhookPayload,\n          };\n\n          console.log(\"ðŸš€ [Linear] Triggering handler\");\n\n          const result = await handler(mastra, triggerInfo);\n\n          console.log(\"âœ… [Linear] Handler completed\", { result });\n\n          return c.json({ success: true, result });\n        } catch (error) {\n          logger?.error(\"âŒ [Linear] Error processing webhook\", {\n            error: error instanceof Error ? error.message : String(error),\n            stack: error instanceof Error ? error.stack : undefined,\n          });\n\n          return c.json(\n            {\n              success: false,\n              error: error instanceof Error ? error.message : String(error),\n            },\n            500,\n          );\n        }\n      },\n    }),\n  ];\n}\n","path":null,"size_bytes":4069,"size_tokens":null},"docs/mastra/06-reference/110_run-class.md":{"content":"---\ntitle: \"Reference: Run Class | Workflows | Mastra Docs\"\ndescription: Documentation for the Run class in Mastra, which represents a workflow execution instance.\n---\n\n# Run Class\n[EN] Source: https://mastra.ai/en/reference/workflows/run\n\nThe `Run` class represents a workflow execution instance, providing methods to start, resume, stream, and monitor workflow execution.\n\n## Usage example\n\n```typescript showLineNumbers copy\nconst run = await workflow.createRunAsync();\n\nconst result = await run.start({\n  inputData: { value: \"initial data\" }\n});\n\nif (result.status === \"suspended\") {\n  const resumedResult = await run.resume({\n    resumeData: { value: \"resume data\" }\n  });\n}\n```\n\n## Run Methods\n\n<PropertiesTable\n  content={[\n    {\n      name: \"start\",\n      type: \"(options?: StartOptions) => Promise<WorkflowResult>\",\n      description: \"Starts workflow execution with input data\",\n      required: true,\n    },\n    {\n      name: \"resume\",\n      type: \"(options?: ResumeOptions) => Promise<WorkflowResult>\",\n      description: \"Resumes a suspended workflow from a specific step\",\n      required: true,\n    },\n    {\n      name: \"stream\",\n      type: \"(options?: StreamOptions) => Promise<StreamResult>\",\n      description: \"Monitors workflow execution as a stream of events\",\n      required: true,\n    },\n    {\n      name: \"streamVNext\",\n      type: \"(options?: StreamOptions) => MastraWorkflowStream\",\n      description: \"Enables real-time streaming with enhanced features\",\n      required: true,\n    },\n    {\n      name: \"watch\",\n      type: \"(callback: WatchCallback, type?: WatchType) => UnwatchFunction\",\n      description: \"Monitors workflow execution with callback-based events\",\n      required: true,\n    },\n    {\n      name: \"cancel\",\n      type: \"() => Promise<void>\",\n      description: \"Cancels the workflow execution\",\n      required: true,\n    }\n  ]}\n/>\n\n## Run Status\n\nA workflow run's `status` indicates its current execution state. The possible values are:\n\n<PropertiesTable\n  content={[\n    {\n      name: \"success\",\n      type: \"string\",\n      description:\n        \"All steps finished executing successfully, with a valid result output\",\n    },\n    {\n      name: \"failed\",\n      type: \"string\",\n      description:\n        \"Workflow execution encountered an error during execution, with error details available\",\n    },\n    {\n      name: \"suspended\",\n      type: \"string\",\n      description:\n        \"Workflow execution is paused waiting for resume, with suspended step information\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Running workflows](../../examples/workflows/running-workflows.mdx)\n- [Run.start()](./run-methods/start.mdx)\n- [Run.resume()](./run-methods/resume.mdx)\n- [Run.stream()](./run-methods/stream.mdx)\n- [Run.streamVNext()](./run-methods/streamVNext.mdx)\n- [Run.watch()](./run-methods/watch.mdx)\n- [Run.cancel()](./run-methods/cancel.mdx)\n\n\n","path":null,"size_bytes":2870,"size_tokens":null},"docs/mastra/06-reference/38_mastra-get-workflows.md":{"content":"---\ntitle: \"Reference: Mastra.getWorkflows() | Core | Mastra Docs\"\ndescription: \"Documentation for the `Mastra.getWorkflows()` method in Mastra, which retrieves all configured workflows.\"\n---\n\n# Mastra.getWorkflows()\n[EN] Source: https://mastra.ai/en/reference/core/getWorkflows\n\nThe `.getWorkflows()` method is used to retrieve all workflows that have been configured in the Mastra instance. The method accepts an optional options object.\n\n## Usage example\n\n```typescript copy\nmastra.getWorkflows();\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"options\",\n      type: \"{ serialized?: boolean }\",\n      description: \"Optional configuration object. When `serialized` is true, returns simplified workflow objects with only the name property instead of full workflow instances.\",\n      optional: true,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflows\",\n      type: \"Record<string, Workflow>\",\n      description: \"A record of all configured workflows, where keys are workflow IDs and values are workflow instances (or simplified objects if serialized is true).\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Workflows overview](../../docs/workflows/overview.mdx)\n\n\n","path":null,"size_bytes":1208,"size_tokens":null},"docs/mastra/06-reference/104_workflow-else.md":{"content":"---\ntitle: \"Reference: Workflow.else() | Conditional Branching | Mastra Docs\"\ndescription: \"Documentation for the `.else()` method in Mastra workflows, which creates an alternative branch when an if condition is false.\"\n---\n\n# Workflow.else()\n[EN] Source: https://mastra.ai/en/reference/legacyWorkflows/else\n\n> Experimental\n\nThe `.else()` method creates an alternative branch in the workflow that executes when the preceding `if` condition evaluates to false. This enables workflows to follow different paths based on conditions.\n\n## Usage\n\n```typescript copy showLineNumbers\nworkflow\n  .step(startStep)\n  .if(async ({ context }) => {\n    const value = context.getStepResult<{ value: number }>(\"start\")?.value;\n    return value < 10;\n  })\n  .then(ifBranchStep)\n  .else() // Alternative branch when the condition is false\n  .then(elseBranchStep)\n  .commit();\n```\n\n## Parameters\n\nThe `else()` method does not take any parameters.\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"LegacyWorkflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Behavior\n\n- The `else()` method must follow an `if()` branch in the workflow definition\n- It creates a branch that executes only when the preceding `if` condition evaluates to false\n- You can chain multiple steps after an `else()` using `.then()`\n- You can nest additional `if`/`else` conditions within an `else` branch\n\n## Error Handling\n\nThe `else()` method requires a preceding `if()` statement. If you try to use it without a preceding `if`, an error will be thrown:\n\n```typescript\ntry {\n  // This will throw an error\n  workflow.step(someStep).else().then(anotherStep).commit();\n} catch (error) {\n  console.error(error); // \"No active condition found\"\n}\n```\n\n## Related\n\n- [if Reference](./if.mdx)\n- [then Reference](./then.mdx)\n- [Control Flow Guide](../../docs/workflows-legacy/control-flow.mdx)\n- [Step Condition Reference](./step-condition.mdx)\n\n\n","path":null,"size_bytes":1969,"size_tokens":null},"docs/mastra/06-reference/54_memory-get-threads-by-resource-id.md":{"content":"---\ntitle: \"Reference: Memory.getThreadsByResourceId() | Memory | Mastra Docs\"\ndescription: \"Documentation for the `Memory.getThreadsByResourceId()` method in Mastra, which retrieves all threads that belong to a specific resource.\"\n---\n\n# Memory.getThreadsByResourceId()\n[EN] Source: https://mastra.ai/en/reference/memory/getThreadsByResourceId\n\nThe `.getThreadsByResourceId()` function retrieves all threads associated with a specific resource ID from storage. Threads can be sorted by creation or modification time in ascending or descending order.\n\n## Usage Example\n\n```typescript\nawait memory?.getThreadsByResourceId({ resourceId: \"user-123\" });\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"resourceId\",\n      type: \"string\",\n      description: \"The ID of the resource whose threads are to be retrieved.\",\n      isOptional: false,\n    },\n    {\n      name: \"orderBy\",\n      type: \"ThreadOrderBy\",\n      description: \"Field to sort threads by. Accepts 'createdAt' or 'updatedAt'. Default: 'createdAt'\",\n      isOptional: true,\n    },\n    {\n      name: \"sortDirection\",\n      type: \"ThreadSortDirection\",\n      description: \"Sort order direction. Accepts 'ASC' or 'DESC'. Default: 'DESC'\",\n      isOptional: true,\n    },\n  ]}\n/>\n\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"StorageThreadType[]\",\n      type: \"Promise\",\n      description:\n        \"A promise that resolves to an array of threads associated with the given resource ID.\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript filename=\"src/test-memory.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst agent = mastra.getAgent(\"agent\");\nconst memory = await agent.getMemory();\n\nconst thread = await memory?.getThreadsByResourceId({\n  resourceId: \"user-123\",\n  orderBy: \"updatedAt\",\n  sortDirection: \"ASC\"\n});\n\nconsole.log(thread);\n```\n\n### Related\n\n- [Memory Class Reference](/reference/memory/Memory.mdx)\n- [getThreadsByResourceIdPaginated](/reference/memory/getThreadsByResourceIdPaginated.mdx) - Paginated version\n- [Getting Started with Memory](/docs/memory/overview.mdx) (Covers threads/resources concept)\n- [createThread](/reference/memory/createThread.mdx)\n- [getThreadById](/reference/memory/getThreadById.mdx)\n\n\n","path":null,"size_bytes":2246,"size_tokens":null},"docs/mastra/03-workflows/01_branching-merging-conditions.md":{"content":"---\ntitle: \"Branching, Merging, Conditions | Workflows | Mastra Docs\"\ndescription: \"Control flow in Mastra workflows allows you to manage branching, merging, and conditions to construct workflows that meet your logic requirements.\"\n---\n\n# Control Flow\n[EN] Source: https://mastra.ai/en/docs/workflows/control-flow\n\nWhen you build a workflow, you typically break down operations into smaller tasks that can be linked and reused. **Steps** provide a structured way to manage these tasks by defining inputs, outputs, and execution logic.\n\n- If the schemas match, the `outputSchema` from each step is automatically passed to the `inputSchema` of the next step.\n- If the schemas don't match, use [Input data mapping](./input-data-mapping.mdx) to transform the `outputSchema` into the expected `inputSchema`.\n\n## Chaining steps with `.then()`\n\nChain steps to execute sequentially using `.then()`:\n\n![Chaining steps with .then()](/image/workflows/workflows-control-flow-then.jpg)\n\n```typescript {8-9,4-5} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\nconst step2 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .then(step2)\n  .commit();\n```\n\nThis does what you'd expect: it executes `step1`, then it executes `step2`.\n\n## Simultaneous steps with `.parallel()`\n\nExecute steps simultaneously using `.parallel()`:\n\n![Concurrent steps with .parallel()](/image/workflows/workflows-control-flow-parallel.jpg)\n\n```typescript {9,4-5} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\nconst step2 = createStep({...});\nconst step3 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .parallel([step1, step2])\n  .then(step3)\n  .commit();\n```\n\nThis executes `step1` and `step2` concurrently, then continues to `step3` after both complete.\n\n> See [Parallel Execution with Steps](../../examples/workflows/parallel-steps.mdx) for more information.\n\n> ðŸ“¹ Watch: How to run steps in parallel and optimize your Mastra workflow â†’ [YouTube (3 minutes)](https://youtu.be/GQJxve5Hki4)\n\n## Conditional logic with `.branch()`\n\nExecute steps conditionally using `.branch()`:\n\n![Conditional branching with .branch()](/image/workflows/workflows-control-flow-branch.jpg)\n\n```typescript {8-11,4-5} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst lessThanStep = createStep({...});\nconst greaterThanStep = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .branch([\n    [async ({ inputData: { value } }) => value <= 10, lessThanStep],\n    [async ({ inputData: { value } }) => value > 10, greaterThanStep]\n  ])\n  .commit();\n```\n\nBranch conditions are evaluated sequentially, but steps with matching conditions are executed in parallel.\n\n> See [Workflow with Conditional Branching](../../examples/workflows/conditional-branching.mdx) for more information.\n\n## Looping steps\n\nWorkflows support two types of loops. When looping a step, or any step-compatible construct like a nested workflow, the initial `inputData` is sourced from the output of the previous step.\n\nTo ensure compatibility, the loopâ€™s initial input must either match the shape of the previous stepâ€™s output, or be explicitly transformed using the `map` function.\n\n- Match the shape of the previous stepâ€™s output, or\n- Be explicitly transformed using the `map` function.\n\n### Repeating with `.dowhile()`\n\nExecutes step repeatedly while a condition is true.\n\n![Repeating with .dowhile()](/image/workflows/workflows-control-flow-dowhile.jpg)\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst counterStep = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .dowhile(counterStep, async ({ inputData: { number } }) => number < 10)\n  .commit();\n```\n\n### Repeating with `.dountil()`\n\nExecutes step repeatedly until a condition becomes true.\n\n![Repeating with .dountil()](/image/workflows/workflows-control-flow-dountil.jpg)\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst counterStep = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .dountil(counterStep, async ({ inputData: { number } }) => number > 10)\n  .commit();\n```\n\n### Loop management\n\nLoop conditions can be implemented in different ways depending on how you want the loop to end. Common patterns include checking values returned in `inputData`, setting a maximum number of iterations, or aborting execution when a limit is reached.\n\n#### Conditional loops\n\nThe `inputData` for a loop step is the output of a previous step. Use the values in `inputData` to determine whether the loop should continue or stop.\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst counterStep = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n.dountil(nestedWorkflowStep, async ({ inputData: { userResponse } }) => userResponse === \"yes\")\n.commit();\n```\n\n#### Limiting loops\n\nThe `iterationCount` tracks how many times the loop step has run. You can use this to limit the number of iterations and prevent infinite loops. Combine it with `inputData` values to stop the loop after a set number of attempts.\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst counterStep = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n.dountil(nestedWorkflowStep, async ({ inputData: { userResponse, iterationCount } }) => userResponse === \"yes\" || iterationCount >= 10)\n.commit();\n```\n\n#### Aborting loops\n\nUse `iterationCount` to limit how many times a loop runs. If the count exceeds your threshold, throw an error to fail the step and stop the workflow.\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst counterStep = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n.dountil(nestedWorkflowStep, async ({ inputData: { userResponse, iterationCount } }) => {\n  if (iterationCount >= 10) {\n    throw new Error(\"Maximum iterations reached\");\n  }\n  return userResponse === \"yes\";\n})\n.commit();\n```\n\n### Repeating with `.foreach()`\n\nSequentially executes the same step for each item from the `inputSchema`.\n\n![Repeating with .foreach()](/image/workflows/workflows-control-flow-foreach.jpg)\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst mapStep = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .foreach(mapStep)\n  .commit();\n```\n\n#### Setting concurrency limits\n\nUse `concurrency` to execute steps in parallel with a limit on the number of concurrent executions.\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst mapStep = createStep({...})\n\nexport const testWorkflow = createWorkflow({...})\n  .foreach(mapStep, { concurrency: 2 })\n  .commit();\n```\n\n## Using a nested workflow\n\nUse a nested workflow as a step by passing it to `.then()`. This runs each of its steps in sequence as part of the parent workflow.\n\n```typescript {4,7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nexport const nestedWorkflow = createWorkflow({...})\n\nexport const testWorkflow = createWorkflow({...})\n  .then(nestedWorkflow)\n  .commit();\n```\n\n## Cloning a workflow\n\nUse `cloneWorkflow` to duplicate an existing workflow. This lets you reuse its structure while overriding parameters like `id`.\n\n```typescript {6,10} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep, cloneWorkflow } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\nconst parentWorkflow = createWorkflow({...})\nconst clonedWorkflow = cloneWorkflow(parentWorkflow, { id: \"cloned-workflow\" });\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .then(clonedWorkflow)\n  .commit();\n```\n\n## Example Run Instance\n\nThe following example demonstrates how to start a run with multiple inputs. Each input will pass through the `mapStep` sequentially.\n\n```typescript {6} filename=\"src/test-workflow.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst run = await mastra.getWorkflow(\"testWorkflow\").createRunAsync();\n\nconst result = await run.start({\n  inputData: [{ number: 10 }, { number: 100 }, { number: 200 }]\n});\n```\n\nTo execute this run from your terminal:\n\n```bash copy\nnpx tsx src/test-workflow.ts\n```\n\n\n","path":null,"size_bytes":9569,"size_tokens":null},"docs/mastra/01-agents/34_example-whatsapp-chatbot.md":{"content":"---\ntitle: \"Example: WhatsApp Chat Bot | Agents | Mastra Docs\"\ndescription: Example of creating a WhatsApp chat bot using Mastra agents and workflows to handle incoming messages and respond naturally via text messages.\n---\n\n# WhatsApp Chat Bot\n[EN] Source: https://mastra.ai/en/examples/agents/whatsapp-chat-bot\n\nThis example demonstrates how to create a WhatsApp chat bot using Mastra agents and workflows. The bot receives incoming WhatsApp messages via webhook, processes them through an AI agent, breaks responses into natural text messages, and sends them back via the WhatsApp Business API.\n\n## Prerequisites\n\nThis example requires a WhatsApp Business API setup and uses the `anthropic` model. Add these environment variables to your `.env` file:\n\n```bash filename=\".env\" copy\nANTHROPIC_API_KEY=<your-anthropic-api-key>\nWHATSAPP_VERIFY_TOKEN=<your-verify-token>\nWHATSAPP_ACCESS_TOKEN=<your-whatsapp-access-token>\nWHATSAPP_BUSINESS_PHONE_NUMBER_ID=<your-phone-number-id>\nWHATSAPP_API_VERSION=v22.0\n```\n\n## Creating the WhatsApp client\n\nThis client handles sending messages to users via the WhatsApp Business API.\n\n```typescript filename=\"src/whatsapp-client.ts\" showLineNumbers copy\n// Simple WhatsApp Business API client for sending messages\n\ninterface SendMessageParams {\n  to: string;\n  message: string;\n}\n\nexport async function sendWhatsAppMessage({ to, message }: SendMessageParams) {\n  // Get environment variables for WhatsApp API\n  const apiVersion = process.env.WHATSAPP_API_VERSION || \"v22.0\";\n  const phoneNumberId = process.env.WHATSAPP_BUSINESS_PHONE_NUMBER_ID;\n  const accessToken = process.env.WHATSAPP_ACCESS_TOKEN;\n\n  // Check if required environment variables are set\n  if (!phoneNumberId || !accessToken) {\n    return false;\n  }\n\n  // WhatsApp Business API endpoint\n  const url =\n    `https://graph.facebook.com/${apiVersion}/${phoneNumberId}/messages`;\n\n  // Message payload following WhatsApp API format\n  const payload = {\n    messaging_product: \"whatsapp\",\n    recipient_type: \"individual\",\n    to: to,\n    type: \"text\",\n    text: {\n      body: message,\n    },\n  };\n\n  try {\n    // Send message via WhatsApp Business API\n    const response = await fetch(url, {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n        Authorization: `Bearer ${accessToken}`,\n      },\n      body: JSON.stringify(payload),\n    });\n\n    const result = await response.json();\n\n    if (response.ok) {\n      console.log(`âœ… WhatsApp message sent to ${to}: \"${message}\"`);\n      return true;\n    } else {\n      console.error(\"âŒ Failed to send WhatsApp message:\", result);\n      return false;\n    }\n  } catch (error) {\n    console.error(\"âŒ Error sending WhatsApp message:\", error);\n    return false;\n  }\n}\n```\n\n## Creating the chat agent\n\nThis agent handles the main conversation logic with a friendly, conversational personality.\n\n```typescript filename=\"src/mastra/agents/chat-agent.ts\" showLineNumbers copy\nimport { anthropic } from \"@ai-sdk/anthropic\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { Memory } from \"@mastra/memory\";\nimport { LibSQLStore } from \"@mastra/libsql\";\n\nexport const chatAgent = new Agent({\n  name: \"Chat Agent\",\n  instructions: `\n    You are a helpful, friendly, and knowledgeable AI assistant that loves to chat with users via WhatsApp.\n\n    Your personality:\n    - Warm, approachable, and conversational\n    - Enthusiastic about helping with any topic\n    - Use a casual, friendly tone like you're chatting with a friend\n    - Be concise but informative\n    - Show genuine interest in the user's questions\n\n    Your capabilities:\n    - Answer questions on a wide variety of topics\n    - Provide helpful advice and suggestions\n    - Engage in casual conversation\n    - Help with problem-solving and creative tasks\n    - Explain complex topics in simple terms\n\n    Guidelines:\n    - Keep responses informative but not overwhelming\n    - Ask follow-up questions when appropriate\n    - Be encouraging and positive\n    - If you don't know something, admit it honestly\n    - Adapt your communication style to match the user's tone\n    - Remember this is WhatsApp, so keep it conversational and natural\n\n    Always aim to be helpful while maintaining a friendly, approachable conversation style.\n  `,\n  model: anthropic(\"claude-4-sonnet-20250514\"),\n  memory: new Memory({\n    storage: new LibSQLStore({\n      url: \"file:../mastra.db\",\n    }),\n  }),\n});\n```\n\n## Creating the text message agent\n\nThis agent converts longer responses into natural, bite-sized text messages suitable for WhatsApp.\n\n```typescript filename=\"src/mastra/agents/text-message-agent.ts\" showLineNumbers copy\nimport { anthropic } from \"@ai-sdk/anthropic\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { Memory } from \"@mastra/memory\";\nimport { LibSQLStore } from \"@mastra/libsql\";\n\nexport const textMessageAgent = new Agent({\n  name: \"Text Message Agent\",\n  instructions: `\n    You are a text message converter that takes formal or lengthy text and breaks it down into natural, casual text messages.\n\n    Your job is to:\n    - Convert any input text into 5-8 short, casual text messages\n    - Each message should be 1-2 sentences maximum\n    - Use natural, friendly texting language (contractions, casual tone)\n    - Maintain all the important information from the original text\n    - Make it feel like you're texting a friend\n    - Use appropriate emojis sparingly to add personality\n    - Keep the conversational flow logical and easy to follow\n\n    Think of it like you're explaining something exciting to a friend via text - break it into bite-sized, engaging messages that don't overwhelm them with a long paragraph.\n\n    Always return exactly 5-8 messages in the messages array.\n  `,\n  model: anthropic(\"claude-4-sonnet-20250514\"),\n  memory: new Memory({\n    storage: new LibSQLStore({\n      url: \"file:../mastra.db\",\n    }),\n  }),\n});\n```\n\n## Creating the chat workflow\n\nThis workflow orchestrates the entire chat process: generating a response, breaking it into messages, and sending them via WhatsApp.\n\n```typescript filename=\"src/mastra/workflows/chat-workflow.ts\" showLineNumbers copy\nimport { createStep, createWorkflow } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\nimport { sendWhatsAppMessage } from \"../../whatsapp-client\";\n\nconst respondToMessage = createStep({\n  id: \"respond-to-message\",\n  description: \"Generate response to user message\",\n  inputSchema: z.object({ userMessage: z.string() }),\n  outputSchema: z.object({ response: z.string() }),\n  execute: async ({ inputData, mastra }) => {\n    const agent = mastra?.getAgent(\"chatAgent\");\n    if (!agent) {\n      throw new Error(\"Chat agent not found\");\n    }\n\n    const response = await agent.generate(\n      [{ role: \"user\", content: inputData.userMessage }],\n    );\n\n    return { response: response.text };\n  },\n});\n\nconst breakIntoMessages = createStep({\n  id: \"break-into-messages\",\n  description: \"Breaks response into text messages\",\n  inputSchema: z.object({ prompt: z.string() }),\n  outputSchema: z.object({ messages: z.array(z.string()) }),\n  execute: async ({ inputData, mastra }) => {\n    const agent = mastra?.getAgent(\"textMessageAgent\");\n    if (!agent) {\n      throw new Error(\"Text Message agent not found\");\n    }\n\n    const response = await agent.generate(\n      [{ role: \"user\", content: inputData.prompt }],\n      {\n        structuredOutput: {\n          schema: z.object({\n            messages: z.array(z.string()),\n          }),\n        },\n      },\n    );\n\n    if (!response.object) throw new Error(\"Error generating messages\");\n\n    return response.object;\n  },\n});\n\nconst sendMessages = createStep({\n  id: \"send-messages\",\n  description: \"Sends text messages via WhatsApp\",\n  inputSchema: z.object({\n    messages: z.array(z.string()),\n    userPhone: z.string(),\n  }),\n  outputSchema: z.object({ sentCount: z.number() }),\n  execute: async ({ inputData }) => {\n    const { messages, userPhone } = inputData;\n\n    console.log(\n      `\\nðŸ”¥ Sending ${messages.length} WhatsApp messages to ${userPhone}...`,\n    );\n\n    let sentCount = 0;\n\n    // Send each message with a small delay for natural flow\n    for (let i = 0; i < messages.length; i++) {\n      const success = await sendWhatsAppMessage({\n        to: userPhone,\n        message: messages[i],\n      });\n\n      if (success) {\n        sentCount++;\n      }\n\n      // Add delay between messages for natural texting rhythm\n      if (i < messages.length - 1) {\n        await new Promise((resolve) => setTimeout(resolve, 1000));\n      }\n    }\n\n    console.log(\n      `\\nâœ… Successfully sent ${sentCount}/${messages.length} WhatsApp messages\\n`,\n    );\n\n    return { sentCount };\n  },\n});\n\nexport const chatWorkflow = createWorkflow({\n  id: \"chat-workflow\",\n  inputSchema: z.object({ userMessage: z.string() }),\n  outputSchema: z.object({ sentCount: z.number() }),\n})\n  .then(respondToMessage)\n  .map(async ({ inputData }) => ({\n    prompt:\n      `Break this AI response into 3-8 casual, friendly text messages that feel natural for WhatsApp conversation:\\n\\n${inputData.response}`,\n  }))\n  .then(breakIntoMessages)\n  .map(async ({ inputData, getInitData }) => {\n    // Parse the original stringified input to get user phone\n    const initData = getInitData();\n    const webhookData = JSON.parse(initData.userMessage);\n    const userPhone =\n      webhookData.entry?.[0]?.changes?.[0]?.value?.messages?.[0]?.from ||\n      \"unknown\";\n\n    return {\n      messages: inputData.messages,\n      userPhone,\n    };\n  })\n  .then(sendMessages);\n\nchatWorkflow.commit();\n```\n\n## Setting up Mastra configuration\n\nConfigure your Mastra instance with the agents, workflow, and WhatsApp webhook endpoints.\n\n```typescript filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { registerApiRoute } from \"@mastra/core/server\";\nimport { PinoLogger } from \"@mastra/loggers\";\nimport { LibSQLStore } from \"@mastra/libsql\";\n\nimport { chatWorkflow } from \"./workflows/chat-workflow\";\nimport { textMessageAgent } from \"./agents/text-message-agent\";\nimport { chatAgent } from \"./agents/chat-agent\";\n\nexport const mastra = new Mastra({\n  workflows: { chatWorkflow },\n  agents: { textMessageAgent, chatAgent },\n  storage: new LibSQLStore({\n    url: \":memory:\",\n  }),\n  logger: new PinoLogger({\n    name: \"Mastra\",\n    level: \"info\",\n  }),\n  server: {\n    apiRoutes: [\n      registerApiRoute(\"/whatsapp\", {\n        method: \"GET\",\n        handler: async (c) => {\n          const verifyToken = process.env.WHATSAPP_VERIFY_TOKEN;\n          const {\n            \"hub.mode\": mode,\n            \"hub.challenge\": challenge,\n            \"hub.verify_token\": token,\n          } = c.req.query();\n\n          if (mode === \"subscribe\" && token === verifyToken) {\n            return c.text(challenge, 200);\n          } else {\n            return c.status(403);\n          }\n        },\n      }),\n      registerApiRoute(\"/whatsapp\", {\n        method: \"POST\",\n        handler: async (c) => {\n          const mastra = c.get(\"mastra\");\n          const chatWorkflow = mastra.getWorkflow(\"chatWorkflow\");\n\n          const body = await c.req.json();\n\n          const workflowRun = await chatWorkflow.createRunAsync();\n          const runResult = await workflowRun.start({\n            inputData: { userMessage: JSON.stringify(body) },\n          });\n\n          return c.json(runResult);\n        },\n      }),\n    ],\n  },\n});\n```\n\n## Testing the chat bot\n\nYou can test the chat bot locally by simulating a WhatsApp webhook payload.\n\n```typescript filename=\"src/test-whatsapp-bot.ts\" showLineNumbers copy\nimport \"dotenv/config\";\n\nimport { mastra } from \"./mastra\";\n\n// Simulate a WhatsApp webhook payload\nconst mockWebhookData = {\n  entry: [\n    {\n      changes: [\n        {\n          value: {\n            messages: [\n              {\n                from: \"1234567890\", // Test phone number\n                text: {\n                  body: \"Hello! How are you today?\"\n                }\n              }\n            ]\n          }\n        }\n      ]\n    }\n  ]\n};\n\nconst workflow = mastra.getWorkflow(\"chatWorkflow\");\nconst workflowRun = await workflow.createRunAsync();\n\nconst result = await workflowRun.start({\n  inputData: { userMessage: JSON.stringify(mockWebhookData) }\n});\n\nconsole.log(\"Workflow completed:\", result);\n```\n\n## Example output\n\nWhen a user sends \"Hello! How are you today?\" to your WhatsApp bot, it might respond with multiple messages like:\n\n```text\nHey there! ðŸ‘‹ I'm doing great, thanks for asking!\n\nHow's your day going so far? \n\nI'm here and ready to chat about whatever's on your mind\n\nWhether you need help with something or just want to talk, I'm all ears! ðŸ˜Š\n\nWhat's new with you?\n```\n\nThe bot maintains conversation context through memory and delivers responses that feel natural for WhatsApp messaging.\n\n","path":null,"size_bytes":12824,"size_tokens":null},"docs/mastra/06-reference/14_agent-get-agent.md":{"content":"---\ntitle: \"Reference: Agent.getAgent() | Agents | Mastra Docs\"\ndescription: \"Documentation for the `Agent.getAgent()` method in Mastra, which retrieves an agent by name.\"\n---\n\n# Mastra.getAgent()\n[EN] Source: https://mastra.ai/en/reference/core/getAgent\n\nThe `.getAgent()` method is used to retrieve an agent. The method accepts a single `string` parameter for the agent's name.\n\n## Usage example\n\n```typescript copy\nmastra.getAgent(\"testAgent\");\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"name\",\n      type: \"TAgentName extends keyof TAgents\",\n      description: \"The name of the agent to retrieve. Must be a valid agent name that exists in the Mastra configuration.\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"agent\",\n      type: \"TAgents[TAgentName]\",\n      description: \"The agent instance with the specified name. Throws an error if the agent is not found.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Agents overview](../../docs/agents/overview.mdx)\n- [Dynamic agents](../../docs/agents/dynamic-agents.mdx)\n\n\n","path":null,"size_bytes":1065,"size_tokens":null},"docs/mastra/07-migration/02_vnext-to-standard.md":{"content":"---\ntitle: \"VNext to Standard APIs | Migration Guide\"\ndescription: \"Learn how to migrate from VNext methods to the new standard agent APIs in Mastra.\"\n---\n\n## Overview\n[EN] Source: https://mastra.ai/en/guides/migrations/vnext-to-standard-apis\n\nAs of `v0.20.0` for `@mastra/core`, the following changes apply.\n\n## Legacy APIs (AI SDK v4)\n\nThe original methods have been renamed and maintain backward compatibility with **AI SDK v4** and `v1` models.\n\n- `.stream()` â†’ `.streamLegacy()`\n- `.generate()` â†’ `.generateLegacy()`\n\n## Standard APIs (AI SDK v5)\n\nThese are now the current APIs with full **AI SDK v5** and `v2` model compatibility.\n\n- `.streamVNext()` â†’ `.stream()`\n- `.generateVNext()` â†’ `.generate()`\n\n## Migration paths\n\nIf you're already using `.streamVNext()` and `.generateVNext()` use find/replace to change methods to `.stream()` and `.generate()` respectively.\n\nIf you're using the old `.stream()` and `.generate()`, decide whether you want to upgrade or not. If you don't, use find/replace to change to `.streamLegacy()` and `.generateLegacy()`.\n\nChoose the migration path that fits your needs:\n\n### Keep using AI SDK v4 models\n\n- Rename all your `.stream()` and `.generate()` calls to `.streamLegacy()` and `.generateLegacy()` respectively.\n\n> No further changes required.\n\n### Keep using AI SDK v5 models\n\n- Rename all your `.streamVNext()` and `.generateVNext()` calls to `.stream()` and `.generate()` respectively.\n\n> No further changes required.\n\n### Upgrade from AI SDK v4 to v5\n\n- Bump all your model provider packages by a major version.\n\n> This will ensure that they are all v5 models now. Follow the guide below to understand the key differences and update your code accordingly.\n\n## Key differences\n\nThe updated `.stream()` and `.generate()` methods differ from their legacy counterparts in behavior, compatibility, return types, and available options. This section highlights the most important changes you need to understand when migrating.\n\n### Model version support\n\n**Legacy APIs**\n\n- `.generateLegacy()`\n- `.streamLegacy()`\n\nOnly support **AI SDK v4** models (`specificationVersion: 'v1'`)\n\n**Standard APIs**\n\n- `.generate()`\n- `.stream()`\n\nOnly support **AI SDK v5** models (`specificationVersion: 'v2'`)\n\n> This is enforced at runtime with clear error messages.\n\n### Return types\n\n**Legacy APIs**\n\n- `.generateLegacy()`\n  Returns: `GenerateTextResult` or `GenerateObjectResult`\n\n- `.streamLegacy()`\n  Returns: `StreamTextResult` or `StreamObjectResult`\n\nSee the following API references for more information:\n\n- [Agent.generateLegacy()](../../reference/agents/generateLegacy.mdx)\n- [Agent.streamLegacy()](../../reference/streaming/agents/streamLegacy.mdx)\n\n**Standard APIs**\n\n- `.generate()`\n  - `format: 'mastra'` (default): Returns `MastraModelOutput.getFullOutput()`\n  - `format: 'aisdk'`: Returns `AISDKV5OutputStream.getFullOutput()`\n  - Internally calls `.stream()` and awaits `.getFullOutput()`\n\n- `.stream()`\n  - `format: 'mastra'` (default): Returns `MastraModelOutput<OUTPUT>`\n  - `format: 'aisdk'`: Returns `AISDKV5OutputStream<OUTPUT>`\n\nSee the following API references for more information:\n\n- [Agent.generate()](../../reference/agents/generate.mdx)\n- [Agent.stream()](../../reference/streaming/agents/stream.mdx)\n\n### Format control\n\n**Legacy APIs**\n\nNo `format` option: Always return AI SDK v4 types\n\n```typescript showLineNumbers copy\n// Mastra native format (default)\nconst result = await agent.stream(messages);\n```\n\n**Standard APIs**\n\nUse `format` option to choose output:\n- `'mastra'` (default)\n- `'aisdk'` (AI SDK v5 compatible)\n\n```typescript showLineNumbers copy\n// AI SDK v5 compatibility\nconst result = await agent.stream(messages, {\n  format: 'aisdk'\n});\n```\n\n### New options in standard APIs\n\nThe following options are available in the standard `.stream()` and `generate()`, but **NOT** in their legacy counterparts:\n\n- `format` - Choose between 'mastra' or 'aisdk' output format:\n\n    ```typescript showLineNumbers copy\n    const result = await agent.stream(messages, {\n      format: 'aisdk' // or 'mastra' (default)\n    });\n    ```\n- `system` - Custom system message (separate from instructions).\n\n    ```typescript showLineNumbers copy\n    const result = await agent.stream(messages, {\n      system: 'You are a helpful assistant'\n    });\n    ```\n\n- `structuredOutput` - Enhanced structured output with model override and custom options.\n  - `jsonPromptInjection` - Used to override the default behaviour of passing response_format to the model. This will inject context into the prompt to coerce the model into returning structured outputs.\n  - `model` - If a model is added this will create a sub agent to structure the response from the main agent. The main agent will call tools and return text, and the sub agent will return an object that conforms to your schema provided. This is a replacement for `experimental_output`.\n  - `errorStrategy` - Determines what happens when the output doesnâ€™t match the schema:\n      - 'warn' - log a warning\n      - 'error' - throw an error\n      - 'fallback' - return a fallback value you provide\n\n    ```typescript showLineNumbers copy\n    const result = await agent.generate(messages, {\n      structuredOutput: {\n        schema: z.object({\n          name: z.string(),\n          age: z.number()\n        }),\n        model: \"openai/gpt-4o-mini\", // Optional model override for structuring\n        errorStrategy: 'fallback',\n        fallbackValue: { name: 'unknown', age: 0 },\n        instructions: 'Extract user information' // Override default structuring instructions\n      }\n    });\n    ```\n\n- `stopWhen` - Flexible stop conditions (step count, token limit, etc).\n\n    ```typescript showLineNumbers copy\n    const result = await agent.stream(messages, {\n      stopWhen: ({ steps, totalTokens }) => steps >= 5 || totalTokens >= 10000\n    });\n    ```\n\n- `providerOptions` - Provider-specific options (e.g., OpenAI-specific settings)\n\n    ```typescript showLineNumbers copy\n    const result = await agent.stream(messages, {\n      providerOptions: {\n        openai: {\n          store: true,\n          metadata: { userId: '123' }\n        }\n      }\n    });\n    ```\n\n- `onChunk` - Callback for each streaming chunk.\n\n    ```typescript showLineNumbers copy\n    const result = await agent.stream(messages, {\n      onChunk: (chunk) => {\n        console.log('Received chunk:', chunk);\n      }\n    });\n    ```\n\n- `onError` - Error callback.\n\n    ```typescript showLineNumbers copy\n    const result = await agent.stream(messages, {\n      onError: (error) => {\n        console.error('Stream error:', error);\n      }\n    });\n    ```\n\n- `onAbort` - Abort callback.\n\n    ```typescript showLineNumbers copy\n    const result = await agent.stream(messages, {\n      onAbort: () => {\n        console.log('Stream aborted');\n      }\n    });\n    ```\n\n- `activeTools` - Specify which tools are active for this execution.\n\n    ```typescript showLineNumbers copy\n    const result = await agent.stream(messages, {\n      activeTools: ['search', 'calculator'] // Only these tools will be available\n    });\n    ```\n\n- `abortSignal` - AbortSignal for cancellation.\n\n    ```typescript showLineNumbers copy\n    const controller = new AbortController();\n    const result = await agent.stream(messages, {\n      abortSignal: controller.signal\n    });\n\n    // Later: controller.abort();\n    ```\n\n- `prepareStep` - Callback before each step in multi-step execution.\n\n    ```typescript showLineNumbers copy\n    const result = await agent.stream(messages, {\n      prepareStep: ({ step, state }) => {\n        console.log('About to execute step:', step);\n        return { /* modified state */ };\n      }\n    });\n    ```\n\n- `requireToolApproval` - Require approval for all tool calls.\n\n    ```typescript showLineNumbers copy\n    const result = await agent.stream(messages, {\n      requireToolApproval: true\n    });\n    ```\n\n### Legacy options that moved\n\n- `temperature` and other `modelSettings`.\n\n    Unified in `modelSettings`\n\n    ```typescript showLineNumbers copy\n    const result = await agent.stream(messages, {\n      modelSettings: {\n        temperature: 0.7,\n        maxTokens: 1000,\n        topP: 0.9\n      }\n    });\n    ```\n\n- `resourceId` and `threadId`.\n\n    Moved to memory object.\n\n    ```typescript showLineNumbers copy\n    const result = await agent.stream(messages, {\n      memory: {\n        resource: 'user-123',\n        thread: 'thread-456'\n      }\n    });\n    ```\n\n### Deprecated or removed options\n\n- `experimental_output`\n\n    Use `structuredOutput` instead to allow for tool calls and an object return.\n\n    ```typescript showLineNumbers copy\n    const result = await agent.generate(messages, {\n      structuredOutput: {\n        schema: z.object({\n          summary: z.string(),\n        }),\n        model: \"openai/gpt-4o\"\n      }\n    });\n    ```\n\n- `output`\n\n    The `output` property is deprecated in favor of `structuredOutput`, to achieve the same results, omit the model and only pass `structuredOutput.schema`, optionally add `jsonPromptInjection: true` if your model does not natively support `response_format`.\n\n    ```typescript showLineNumbers copy\n    const result = await agent.generate(messages, {\n      structuredOutput: {\n        schema: {\n          z.object({\n            name: z.string()\n          })\n        }\n      },\n    });\n    ```\n\n- `memoryOptions`\n\n    Use `memory` instead.\n\n    ```typescript showLineNumbers copy\n    const result = await agent.generate(messages, {\n      memory: {\n        // ...\n      }\n    });\n    ```\n\n### Type changes\n\n**Legacy APIs**\n\n- `CoreMessage[]`\n\nSee the following API references for more information:\n\n- [Agent.generateLegacy()](../../reference/agents/generateLegacy.mdx)\n- [Agent.streamLegacy()](../../reference/streaming/agents/streamLegacy.mdx)\n\n**Standard APIs**\n\n- `ModelMessage[]`\n\n    `toolChoice` uses the AI SDK v5 `ToolChoice` type.\n\n    ```typescript showLineNumbers copy\n    type ToolChoice<TOOLS extends Record<string, unknown>> = 'auto' | 'none' | 'required' | {\n        type: 'tool';\n        toolName: Extract<keyof TOOLS, string>;\n    };\n    ```\n\nSee the following API references for more information:\n\n- [Agent.generate()](../../reference/agents/generate.mdx)\n- [Agent.stream()](../../reference/streaming/agents/stream.mdx)\n\n\n","path":null,"size_bytes":10293,"size_tokens":null},"docs/mastra/06-reference/95_workflow-dowhile.md":{"content":"---\ntitle: \"Reference: Workflow.dowhile() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow.dowhile()` method in workflows, which creates a loop that executes a step while a condition is met.\n---\n\n# Workflow.dowhile()\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow-methods/dowhile\n\nThe `.dowhile()` method executes a step while a condition is met. It always runs the step at least once before evaluating the condition. The first time the condition is evaluated, `iterationCount` is `1`.\n\n## Usage example\n\n```typescript copy\nworkflow.dowhile(step1, async ({ inputData }) => true);\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"step\",\n      type: \"Step\",\n      description: \"The step instance to execute in the loop\",\n      isOptional: false,\n    },\n    {\n      name: \"condition\",\n      type: \"(params : ExecuteParams & { iterationCount: number }) => Promise<boolean>\",\n      description:\n        \"A function that returns a boolean indicating whether to continue the loop. The function receives the execution parameters and the iteration count.\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"Workflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Control Flow](../../../docs/workflows/control-flow.mdx)\n\n- [ExecuteParams](../step.mdx#ExecuteParams)\n\n\n","path":null,"size_bytes":1443,"size_tokens":null},"docs/mastra/01-agents/22_example-memory-upstash.md":{"content":"---\ntitle: \"Example: Memory with Upstash | Memory | Mastra Docs\"\ndescription: Example for how to use Mastra's memory system with Upstash Redis storage and vector capabilities.\n---\n\n# Memory with Upstash\n[EN] Source: https://mastra.ai/en/docs/memory/storage/memory-with-upstash\n\nThis example demonstrates how to use Mastra's memory system with Upstash as the storage backend.\n\n## Prerequisites\n\nThis example uses the `openai` model and requires both Upstash Redis and Upstash Vector services. Make sure to add the following to your `.env` file:\n\n```bash filename=\".env\" copy\nOPENAI_API_KEY=<your-api-key>\nUPSTASH_REDIS_REST_URL=<your-redis-url>\nUPSTASH_REDIS_REST_TOKEN=<your-redis-token>\nUPSTASH_VECTOR_REST_URL=<your-vector-index-url>\nUPSTASH_VECTOR_REST_TOKEN=<your-vector-index-token>\n```\n\nYou can get your Upstash credentials by signing up at [upstash.com](https://upstash.com) and creating both Redis and Vector databases.\n\nAnd install the following package:\n\n```bash copy\nnpm install @mastra/upstash\n```\n\n## Adding memory to an agent\n\nTo add Upstash memory to an agent use the `Memory` class and create a new `storage` key using `UpstashStore` and a new `vector` key using `UpstashVector`. The configuration can point to either a remote service or a local setup.\n\n```typescript filename=\"src/mastra/agents/example-upstash-agent.ts\" showLineNumbers copy\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { UpstashStore } from \"@mastra/upstash\";\n\nexport const upstashAgent = new Agent({\n  name: \"upstash-agent\",\n  instructions: \"You are an AI agent with the ability to automatically recall memories from previous interactions.\",\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    storage: new UpstashStore({\n      url: process.env.UPSTASH_REDIS_REST_URL!,\n      token: process.env.UPSTASH_REDIS_REST_TOKEN!\n    }),\n    options: {\n      threads: {\n        generateTitle: true\n      }\n    }\n  })\n});\n```\n\n\n## Local embeddings with fastembed\n\nEmbeddings are numeric vectors used by memoryâ€™s `semanticRecall` to retrieve related messages by meaning (not keywords). This setup uses `@mastra/fastembed` to generate vector embeddings.\n\nInstall `fastembed` to get started:\n\n```bash copy\nnpm install @mastra/fastembed\n```\n\nAdd the following to your agent:\n\n```typescript filename=\"src/mastra/agents/example-upstash-agent.ts\" showLineNumbers copy\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { UpstashStore, UpstashVector } from \"@mastra/upstash\";\nimport { fastembed } from \"@mastra/fastembed\";\n\nexport const upstashAgent = new Agent({\n  name: \"upstash-agent\",\n  instructions: \"You are an AI agent with the ability to automatically recall memories from previous interactions.\",\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    storage: new UpstashStore({\n      url: process.env.UPSTASH_REDIS_REST_URL!,\n      token: process.env.UPSTASH_REDIS_REST_TOKEN!\n    }),\n    vector: new UpstashVector({\n      url: process.env.UPSTASH_VECTOR_REST_URL!,\n      token: process.env.UPSTASH_VECTOR_REST_TOKEN!\n    }),\n    embedder: fastembed,\n    options: {\n      lastMessages: 10,\n      semanticRecall: {\n        topK: 3,\n        messageRange: 2\n      }\n    }\n  })\n});\n```\n\n## Usage example\n\nUse `memoryOptions` to scope recall for this request. Set `lastMessages: 5` to limit recency-based recall, and use `semanticRecall` to fetch the `topK: 3` most relevant messages, including `messageRange: 2` neighboring messages for context around each match.\n\n```typescript filename=\"src/test-upstash-agent.ts\" showLineNumbers copy\nimport \"dotenv/config\";\n\nimport { mastra } from \"./mastra\";\n\nconst threadId = \"123\";\nconst resourceId = \"user-456\";\n\nconst agent = mastra.getAgent(\"upstashAgent\");\n\nconst message = await agent.stream(\"My name is Mastra\", {\n  memory: {\n    thread: threadId,\n    resource: resourceId\n  }\n});\n\nawait message.textStream.pipeTo(new WritableStream());\n\nconst stream = await agent.stream(\"What's my name?\", {\n  memory: {\n    thread: threadId,\n    resource: resourceId\n  },\n  memoryOptions: {\n    lastMessages: 5,\n    semanticRecall: {\n      topK: 3,\n      messageRange: 2\n    }\n  }\n});\n\nfor await (const chunk of stream.textStream) {\n  process.stdout.write(chunk);\n}\n```\n\n## Related\n\n- [Calling Agents](../agents/calling-agents.mdx)\n\n\n","path":null,"size_bytes":4413,"size_tokens":null},"docs/mastra/01-agents/12_semantic-recall.md":{"content":"---\ntitle: \"Semantic Recall | Memory | Mastra Docs\"\ndescription: \"Learn how to use semantic recall in Mastra to retrieve relevant messages from past conversations using vector search and embeddings.\"\n---\n\n# Semantic Recall\n[EN] Source: https://mastra.ai/en/docs/memory/semantic-recall\n\nIf you ask your friend what they did last weekend, they will search in their memory for events associated with \"last weekend\" and then tell you what they did. That's sort of like how semantic recall works in Mastra.\n\n> **ðŸ“¹ Watch**: What semantic recall is, how it works, and how to configure it in Mastra â†’ [YouTube (5 minutes)](https://youtu.be/UVZtK8cK8xQ)\n\n## How Semantic Recall Works\n\nSemantic recall is RAG-based search that helps agents maintain context across longer interactions when messages are no longer within [recent conversation history](./overview.mdx#conversation-history).\n\nIt uses vector embeddings of messages for similarity search, integrates with various vector stores, and has configurable context windows around retrieved messages.\n\n<br />\n<img\n  src=\"/image/semantic-recall.png\"\n  alt=\"Diagram showing Mastra Memory semantic recall\"\n  width={800}\n/>\n\nWhen it's enabled, new messages are used to query a vector DB for semantically similar messages.\n\nAfter getting a response from the LLM, all new messages (user, assistant, and tool calls/results) are inserted into the vector DB to be recalled in later interactions.\n\n## Quick Start\n\nSemantic recall is enabled by default, so if you give your agent memory it will be included:\n\n```typescript {9}\nimport { Agent } from \"@mastra/core/agent\";\nimport { Memory } from \"@mastra/memory\";\nimport { openai } from \"@ai-sdk/openai\";\n\nconst agent = new Agent({\n  name: \"SupportAgent\",\n  instructions: \"You are a helpful support agent.\",\n  model: openai(\"gpt-4o\"),\n  memory: new Memory(),\n});\n```\n\n## Recall configuration\n\nThe three main parameters that control semantic recall behavior are:\n\n1. **topK**: How many semantically similar messages to retrieve\n2. **messageRange**: How much surrounding context to include with each match\n3. **scope**: Whether to search within the current thread or across all threads owned by a resource. Using `scope: 'resource'` allows the agent to recall information from any of the user's past conversations.\n\n```typescript {5-7}\nconst agent = new Agent({\n  memory: new Memory({\n    options: {\n      semanticRecall: {\n        topK: 3, // Retrieve 3 most similar messages\n        messageRange: 2, // Include 2 messages before and after each match\n        scope: 'resource', // Search across all threads for this user\n      },\n    },\n  }),\n});\n```\n\nNote: currently, `scope: 'resource'` for semantic recall is supported by the following storage adapters: LibSQL, Postgres, and Upstash.\n\n### Storage configuration\n\nSemantic recall relies on a [storage and vector db](/reference/memory/Memory#parameters) to store messages and their embeddings.\n\n```ts {8-17}\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { LibSQLStore, LibSQLVector } from \"@mastra/libsql\";\n\nconst agent = new Agent({\n  memory: new Memory({\n    // this is the default storage db if omitted\n    storage: new LibSQLStore({\n      url: \"file:./local.db\",\n    }),\n    // this is the default vector db if omitted\n    vector: new LibSQLVector({\n      connectionUrl: \"file:./local.db\",\n    }),\n  }),\n});\n```\n\n**Storage/vector code Examples**:\n\n- [LibSQL](/docs/memory/storage/memory-with-libsql)\n- [MongoDB](/docs/memory/storage/memory-with-mongodb)\n- [Postgres](/docs/memory/storage/memory-with-pg)\n- [Upstash](/docs/memory/storage/memory-with-upstash)\n\n### Embedder configuration\n\nSemantic recall relies on an [embedding model](/reference/memory/Memory#embedder) to convert messages into embeddings. Mastra supports embedding models through the model router using `provider/model` strings, or you can use any [embedding model](https://sdk.vercel.ai/docs/ai-sdk-core/embeddings) compatible with the AI SDK.\n\n#### Using the Model Router (Recommended)\n\nThe simplest way is to use a `provider/model` string with autocomplete support:\n\n```ts {7}\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\n\nconst agent = new Agent({\n  memory: new Memory({\n    // ... other memory options\n    embedder: \"openai/text-embedding-3-small\", // TypeScript autocomplete supported\n  }),\n});\n```\n\nSupported embedding models:\n- **OpenAI**: `text-embedding-3-small`, `text-embedding-3-large`, `text-embedding-ada-002`\n- **Google**: `gemini-embedding-001`, `text-embedding-004`\n\nThe model router automatically handles API key detection from environment variables (`OPENAI_API_KEY`, `GOOGLE_GENERATIVE_AI_API_KEY`).\n\n#### Using AI SDK Packages\n\nYou can also use AI SDK embedding models directly:\n\n```ts {3,8}\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\n\nconst agent = new Agent({\n  memory: new Memory({\n    // ... other memory options\n    embedder: openai.embedding(\"text-embedding-3-small\"),\n  }),\n});\n```\n\n#### Using FastEmbed (Local)\n\nTo use FastEmbed (a local embedding model), install `@mastra/fastembed`:\n\n```bash npm2yarn copy\nnpm install @mastra/fastembed\n```\n\nThen configure it in your memory:\n\n```ts {3,8}\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { fastembed } from \"@mastra/fastembed\";\n\nconst agent = new Agent({\n  memory: new Memory({\n    // ... other memory options\n    embedder: fastembed,\n  }),\n});\n```\n\n### PostgreSQL Index Optimization\n\nWhen using PostgreSQL as your vector store, you can optimize semantic recall performance by configuring the vector index. This is particularly important for large-scale deployments with thousands of messages.\n\nPostgreSQL supports both IVFFlat and HNSW indexes. By default, Mastra creates an IVFFlat index, but HNSW indexes typically provide better performance, especially with OpenAI embeddings which use inner product distance.\n\n```typescript {9-18}\nimport { Memory } from \"@mastra/memory\";\nimport { PgStore, PgVector } from \"@mastra/pg\";\n\nconst agent = new Agent({\n  memory: new Memory({\n    storage: new PgStore({\n      connectionString: process.env.DATABASE_URL,\n    }),\n    vector: new PgVector({\n      connectionString: process.env.DATABASE_URL,\n    }),\n    options: {\n      semanticRecall: {\n        topK: 5,\n        messageRange: 2,\n        indexConfig: {\n          type: 'hnsw',           // Use HNSW for better performance\n          metric: 'dotproduct',   // Best for OpenAI embeddings  \n          m: 16,                  // Number of bi-directional links (default: 16)\n          efConstruction: 64,    // Size of candidate list during construction (default: 64)\n        },\n      },\n    },\n  }),\n});\n```\n\nFor detailed information about index configuration options and performance tuning, see the [PgVector configuration guide](/reference/vectors/pg#index-configuration-guide).\n\n### Disabling\n\nThere is a performance impact to using semantic recall. New messages are converted into embeddings and used to query a vector database before new messages are sent to the LLM.\n\nSemantic recall is enabled by default but can be disabled when not needed:\n\n```typescript {4}\nconst agent = new Agent({\n  memory: new Memory({\n    options: {\n      semanticRecall: false,\n    },\n  }),\n});\n```\n\nYou might want to disable semantic recall in scenarios like:\n\n- When conversation history provide sufficient context for the current conversation.\n- In performance-sensitive applications, like realtime two-way audio, where the added latency of creating embeddings and running vector queries is noticeable.\n\n## Viewing Recalled Messages\n\nWhen tracing is enabled, any messages retrieved via semantic recall will appear in the agentâ€™s trace output, alongside recent conversation history (if configured).\n\nFor more info on viewing message traces, see [Viewing Retrieved Messages](./overview.mdx#viewing-retrieved-messages).\n\n\n","path":null,"size_bytes":7996,"size_tokens":null},"docs/mastra/07-migration/01_upgrade-to-v1.md":{"content":"---\ntitle: \"Upgrade to Mastra v1 | Migration Guide\"\ndescription: \"Learn how to upgrade through breaking changes in pre-v1 versions of Mastra.\"\n---\n\nimport { Callout } from \"nextra/components\";\n\n# Upgrade to Mastra v1\n[EN] Source: https://mastra.ai/en/guides/migrations/upgrade-to-v1\n\nIn this guide you'll learn how to upgrade through breaking changes in pre-v1 versions of Mastra. It'll also help you upgrade to Mastra v1.\n\nUse your package manager to update your project's versions. Be sure to update **all** Mastra packages at the same time.\n\n<Callout type=\"info\">\n\nVersions mentioned in the headings refer to the `@mastra/core` package. If necessary, versions of other Mastra packages are called out in the detailed description.\n\nAll Mastra packages have a peer dependency on `@mastra/core` so your package manager can inform you about compatibility.\n\n</Callout>\n\n## Migrate to v0.23 (unreleased)\n\n<Callout>\n\nThis version isn't released yet but we're adding changes as we make them.\n\n</Callout>\n\n## Migrate to v0.22\n\n### Deprecated: `format: \"aisdk\"`\n\nThe `format: \"aisdk\"` option in `stream()`/`generate()` methods is deprecated. Use the `@mastra/ai-sdk` package instead. Learn more in the [Using Vercel AI SDK documentation](../../docs/frameworks/agentic-uis/ai-sdk.mdx).\n\n### Removed: MCP Classes\n\n`@mastra/mcp` - `0.14.0`\n\n- Removed `MastraMCPClient` class. Use [`MCPClient`](../../reference/tools/mcp-client.mdx) class instead.\n- Removed `MCPConfigurationOptions` type. Use [`MCPClientOptions`](../../reference/tools/mcp-client.mdx#mcpclientoptions) type instead. The API is identical.\n- Removed `MCPConfiguration` class. Use [`MCPClient`](../../reference/tools/mcp-client.mdx) class instead.\n\n### Removed: CLI flags & commands\n\n`mastra` - `0.17.0`\n\n- Removed the `mastra deploy` CLI command. Use the deploy instructions of your individual platform.\n- Removed `--env` flag from `mastra build` command. To start the build output with a custom env use `mastra start --env <env>` instead.\n- Remove `--port` flag from `mastra dev`. Use `server.port` on the `new Mastra()` class instead.\n\n## Migrate to v0.21\n\nNo changes needed.\n\n## Migrate to v0.20\n\n- [VNext to Standard APIs](./vnext-to-standard-apis.mdx)\n- [AgentNetwork to .network()](./agentnetwork.mdx)\n\n","path":null,"size_bytes":2263,"size_tokens":null},"docs/mastra/01-agents/02_using-tools.md":{"content":"---\ntitle: \"Using Tools | Agents | Mastra Docs\"\ndescription: Learn how to create tools and add them to agents to extend capabilities beyond text generation.\n---\n\n# Using Tools\n[EN] Source: https://mastra.ai/en/docs/agents/using-tools\n\nAgents use tools to call APIs, query databases, or run custom functions from your codebase. [Tools](../tools-mcp/overview.mdx) give agents capabilities beyond language generation by providing structured access to data and performing clearly defined operations. You can also load tools from remote [MCP servers](../tools-mcp/mcp-overview.mdx) to expand an agentâ€™s capabilities.\n\n## When to use tools\n\nUse tools when an agent needs additional context or information from remote resources, or when it needs to run code that performs a specific operation. This includes tasks a model can't reliably handle on its own, such as fetching live data or returning consistent, well defined outputs.\n\n## Creating a tool\n\nThis example shows how to create a tool that fetches weather data from an API. When the agent calls the tool, it provides the required input as defined by the toolâ€™s `inputSchema`. The tool accesses this data through its `context` argument, which in this example includes the `location` used in the weather API query.\n\n```typescript {14,16} filename=\"src/mastra/tools/weather-tool.ts\" showLineNumbers copy\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nexport const weatherTool = createTool({\n  id: \"weather-tool\",\n  description: \"Fetches weather for a location\",\n  inputSchema: z.object({\n    location: z.string()\n  }),\n  outputSchema: z.object({\n    weather: z.string()\n  }),\n  execute: async ({ context }) => {\n    const { location } = context;\n\n    const response = await fetch(`https://wttr.in/${location}?format=3`);\n    const weather = await response.text();\n\n    return { weather };\n  }\n});\n```\n\n## Adding tools to an agent\n\nTo make a tool available to an agent, add it to the `tools` option and reference it by name in the agentâ€™s instructions.\n\n```typescript {9,11} filename=\"src/mastra/agents/weather-agent.ts\" showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { weatherTool } from \"../tools/weather-tool\";\n\nexport const weatherAgent = new Agent({\n  name: \"weather-agent\",\n  instructions: `\n      You are a helpful weather assistant.\n      Use the weatherTool to fetch current weather data.`,\n  model: openai(\"gpt-4o-mini\"),\n  tools: { weatherTool }\n});\n```\n\n## Calling an agent\n\nThe agent uses the toolâ€™s `inputSchema` to infer what data the tool expects. In this case, it extracts `London` as the `location` from the message and makes it available to the toolâ€™s context.\n\n```typescript {5} filename=\"src/test-tool.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst agent = mastra.getAgent(\"weatherAgent\");\n\nconst result = await agent.generate(\"What's the weather in London?\");\n```\n\n## Using multiple tools\n\nAn agent can use multiple tools to handle more complex tasks by delegating specific parts to individual tools. The agent decides which tools to use based on the userâ€™s message, the agentâ€™s instructions, and the tool descriptions and schemas.\n\nWhen multiple tools are available, the agent may choose to use one, several, or none, depending on whatâ€™s needed to answer the query.\n\n```typescript {6} filename=\"src/mastra/agents/weather-agent.ts\" showLineNumbers copy\nimport { weatherTool } from \"../tools/weather-tool\";\nimport { activitiesTool } from \"../tools/activities-tool\";\n\nexport const weatherAgent = new Agent({\n  // ..\n  tools: { weatherTool, activitiesTool }\n});\n```\n\n\n## Related\n\n- [Tools Overview](../tools-mcp/overview.mdx)\n- [Agent Memory](./agent-memory.mdx)\n- [Runtime Context](../server-db/runtime-context.mdx)\n- [Calling Agents](../../examples/agents/calling-agents.mdx)\n\n\n","path":null,"size_bytes":3874,"size_tokens":null},"docs/mastra/06-reference/120_step-class.md":{"content":"---\ntitle: \"Reference: Step Class | Workflows | Mastra Docs\"\ndescription: Documentation for the Step class in Mastra, which defines individual units of work within a workflow.\n---\n\n# Step Class\n[EN] Source: https://mastra.ai/en/reference/workflows/step\n\nThe Step class defines individual units of work within a workflow, encapsulating execution logic, data validation, and input/output handling.\nIt can take either a tool or an agent as a parameter to automatically create a step from them.\n\n## Usage example\n\n```typescript filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({\n  id: \"step-1\",\n  description: \"passes value from input to output\",\n  inputSchema: z.object({\n    value: z.number()\n  }),\n  outputSchema: z.object({\n    value: z.number()\n  }),\n  execute: async ({ inputData }) => {\n    const { value } = inputData;\n    return {\n      value\n    };\n  }\n});\n```\n\n## Constructor Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"id\",\n      type: \"string\",\n      description: \"Unique identifier for the step\",\n      required: true,\n    },\n    {\n      name: \"description\",\n      type: \"string\",\n      description: \"Optional description of what the step does\",\n      required: false,\n    },\n    {\n      name: \"inputSchema\",\n      type: \"z.ZodType<any>\",\n      description: \"Zod schema defining the input structure\",\n      required: true,\n    },\n    {\n      name: \"outputSchema\",\n      type: \"z.ZodType<any>\",\n      description: \"Zod schema defining the output structure\",\n      required: true,\n    },\n    {\n      name: \"resumeSchema\",\n      type: \"z.ZodType<any>\",\n      description: \"Optional Zod schema for resuming the step\",\n      required: false,\n    },\n    {\n      name: \"suspendSchema\",\n      type: \"z.ZodType<any>\",\n      description: \"Optional Zod schema for suspending the step\",\n      required: false,\n    },\n    {\n      name: \"stateSchema\",\n      type: \"z.ZodObject<any>\",\n      description: \"Optional Zod schema for the step state. Automatically injected when using Mastra's state system. The stateSchema must be a subset of the workflow's stateSchema. If not specified, type is 'any'.\",\n      required: false,\n    },\n    {\n      name: \"execute\",\n      type: \"(params: ExecuteParams) => Promise<any>\",\n      description: \"Async function containing step logic\",\n      required: true,\n    }\n  ]}\n/>\n\n### ExecuteParams\n\n<PropertiesTable\n  content={[\n    {\n      name: \"inputData\",\n      type: \"z.infer<TStepInput>\",\n      description: \"The input data matching the inputSchema\",\n    },\n    {\n      name: \"resumeData\",\n      type: \"z.infer<TResumeSchema>\",\n      description:\n        \"The resume data matching the resumeSchema, when resuming the step from a suspended state. Only exists if the step is being resumed.\",\n    },\n    {\n      name: \"mastra\",\n      type: \"Mastra\",\n      description: \"Access to Mastra services (agents, tools, etc.)\",\n    },\n    {\n      name: \"getStepResult\",\n      type: \"(step: Step | string) => any\",\n      description: \"Function to access results from other steps\",\n    },\n    {\n      name: \"getInitData\",\n      type: \"() => any\",\n      description:\n        \"Function to access the initial input data of the workflow in any step\",\n    },\n    {\n      name: \"suspend\",\n      type: \"(suspendPayload: any, suspendOptions?: { resumeLabel?: string }) => Promise<void>\",\n      description: \"Function to pause workflow execution\",\n    },\n    {\n      name: \"setState\",\n      type: \"(state: z.infer<TState>) => void\",\n      description: \"Function to set the state of the workflow. Inject via reducer-like pattern, such as 'setState({ ...state, ...newState })'\",\n    },\n    {\n      name: \"runId\",\n      type: \"string\",\n      description: \"Current run id\",\n    },\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      isOptional: true,\n      description:\n        \"Runtime context for dependency injection and contextual information.\",\n    },\n    {\n      name: \"runCount\",\n      type: \"number\",\n      description: \"The run count for this specific step, it automatically increases each time the step runs\",\n      isOptional: true,\n    }\n  ]}\n/>\n\n## Related\n\n- [Control flow](../../docs/workflows/control-flow.mdx)\n- [Using agents and tools](../../docs/workflows/agents-and-tools.mdx)\n\n\n","path":null,"size_bytes":4383,"size_tokens":null},"docs/mastra/01-agents/20_example-memory-libsql.md":{"content":"---\ntitle: \"Example: Memory with LibSQL | Memory | Mastra Docs\"\ndescription: Example for how to use Mastra's memory system with LibSQL storage and vector database backend.\n---\n\n# Memory with LibSQL\n[EN] Source: https://mastra.ai/en/docs/memory/storage/memory-with-libsql\n\nThis example demonstrates how to use Mastra's memory system with LibSQL as the storage backend.\n\n## Prerequisites\n\nThis example uses the `openai` model. Make sure to add `OPENAI_API_KEY` to your `.env` file.\n\n```bash filename=\".env\" copy\nOPENAI_API_KEY=<your-api-key>\n```\n\nAnd install the following package:\n\n```bash copy\nnpm install @mastra/libsql\n```\n\n## Adding memory to an agent\n\nTo add LibSQL memory to an agent use the `Memory` class and create a new `storage` key using `LibSQLStore`. The `url` can either by a remote location, or a local file system resource.\n\n```typescript filename=\"src/mastra/agents/example-libsql-agent.ts\" showLineNumbers copy\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { LibSQLStore } from \"@mastra/libsql\";\n\nexport const libsqlAgent = new Agent({\n  name: \"libsql-agent\",\n  instructions: \"You are an AI agent with the ability to automatically recall memories from previous interactions.\",\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    storage: new LibSQLStore({\n      url: \"file:libsql-agent.db\"\n    }),\n    options: {\n      threads: {\n        generateTitle: true\n      }\n    }\n  })\n});\n```\n\n## Local embeddings with fastembed\n\nEmbeddings are numeric vectors used by memoryâ€™s `semanticRecall` to retrieve related messages by meaning (not keywords). This setup uses `@mastra/fastembed` to generate vector embeddings.\n\nInstall `fastembed` to get started:\n\n```bash copy\nnpm install @mastra/fastembed\n```\n\nAdd the following to your agent:\n\n```typescript filename=\"src/mastra/agents/example-libsql-agent.ts\" showLineNumbers copy\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { LibSQLStore, LibSQLVector } from \"@mastra/libsql\";\nimport { fastembed } from \"@mastra/fastembed\";\n\nexport const libsqlAgent = new Agent({\n  name: \"libsql-agent\",\n  instructions: \"You are an AI agent with the ability to automatically recall memories from previous interactions.\",\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    storage: new LibSQLStore({\n      url: \"file:libsql-agent.db\"\n    }),\n    vector: new LibSQLVector({\n      connectionUrl: \"file:libsql-agent.db\"\n    }),\n    embedder: fastembed,\n    options: {\n      lastMessages: 10,\n      semanticRecall: {\n        topK: 3,\n        messageRange: 2\n      },\n      threads: {\n        generateTitle: true\n      }\n    }\n  })\n});\n```\n\n## Usage example\n\nUse `memoryOptions` to scope recall for this request. Set `lastMessages: 5` to limit recency-based recall, and use `semanticRecall` to fetch the `topK: 3` most relevant messages, including `messageRange: 2` neighboring messages for context around each match.\n\n```typescript filename=\"src/test-libsql-agent.ts\" showLineNumbers copy\nimport \"dotenv/config\";\n\nimport { mastra } from \"./mastra\";\n\nconst threadId = \"123\";\nconst resourceId = \"user-456\";\n\nconst agent = mastra.getAgent(\"libsqlAgent\");\n\nconst message = await agent.stream(\"My name is Mastra\", {\n  memory: {\n    thread: threadId,\n    resource: resourceId\n  }\n});\n\nawait message.textStream.pipeTo(new WritableStream());\n\nconst stream = await agent.stream(\"What's my name?\", {\n  memory: {\n    thread: threadId,\n    resource: resourceId\n  },\n  memoryOptions: {\n    lastMessages: 5,\n    semanticRecall: {\n      topK: 3,\n      messageRange: 2\n    }\n  }\n});\n\nfor await (const chunk of stream.textStream) {\n  process.stdout.write(chunk);\n}\n```\n\n\n## Related\n\n- [Calling Agents](../agents/calling-agents.mdx)\n\n\n","path":null,"size_bytes":3835,"size_tokens":null},"docs/mastra/06-reference/10_agent-get-tools.md":{"content":"---\ntitle: \"Reference: Agent.getTools() | Agents | Mastra Docs\"\ndescription: \"Documentation for the `Agent.getTools()` method in Mastra agents, which retrieves the tools that the agent can use.\"\n---\n\n# Agent.getTools()\n[EN] Source: https://mastra.ai/en/reference/agents/getTools\n\nThe `.getTools()` method retrieves the tools configured for an agent, resolving them if they're a function. These tools extend the agent's capabilities, allowing it to perform specific actions or access external systems.\n\n## Usage example\n\n```typescript copy\nawait agent.getTools();\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"options\",\n      type: \"{ runtimeContext?: RuntimeContext }\",\n      isOptional: true,\n      defaultValue: \"{}\",\n      description: \"Optional configuration object containing runtime context.\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"tools\",\n      type: \"TTools | Promise<TTools>\",\n      description: \"The tools configured for the agent, either as a direct object or a promise that resolves to the tools.\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript copy\nawait agent.getTools({\n  runtimeContext: new RuntimeContext()\n});\n```\n\n### Options parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      isOptional: true,\n      defaultValue: \"new RuntimeContext()\",\n      description: \"Runtime context for dependency injection and contextual information.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Using tools with agents](../../docs/agents/using-tools-and-mcp.mdx)\n- [Creating tools](../../docs/tools-mcp/overview.mdx)\n\n\n","path":null,"size_bytes":1640,"size_tokens":null},"docs/mastra/06-reference/112_run-resume.md":{"content":"---\ntitle: \"Reference: Run.resume() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Run.resume()` method in workflows, which resumes a suspended workflow run with new data.\n---\n\n# Run.resume()\n[EN] Source: https://mastra.ai/en/reference/workflows/run-methods/resume\n\nThe `.resume()` method resumes a suspended workflow run with new data, allowing you to continue execution from a specific step.\n\n## Usage example\n\n```typescript showLineNumbers copy\nconst run = await workflow.createRunAsync();\n\nconst result = await run.start({ inputData: { value: \"initial data\" } });\n\nif (result.status === \"suspended\") {\n  const resumedResults = await run.resume({\n    resumeData: { value: \"resume data\" }\n  });\n}\n```\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"resumeData\",\n      type: \"z.infer<TResumeSchema>\",\n      description: \"Data for resuming the suspended step\",\n      isOptional: true,\n    },\n    {\n      name: \"step\",\n      type: \"Step<string, any, any, TResumeSchema, any, TEngineType> | [...Step<string, any, any, any, any, TEngineType>[], Step<string, any, any, TResumeSchema, any, TEngineType>] | string | string[]\",\n      description: \"The step(s) to resume execution from. Can be a Step instance, array of Steps, step ID string, or array of step ID strings\",\n      isOptional: true,\n    },\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      description: \"Runtime context data to use when resuming\",\n      isOptional: true,\n    },\n    {\n      name: \"runCount\",\n      type: \"number\",\n      description: \"Optional run count for nested workflow execution\",\n      isOptional: true,\n    },\n    {\n      name: \"tracingContext\",\n      type: \"TracingContext\",\n      isOptional: true,\n      description: \"AI tracing context for creating child spans and adding metadata. Automatically injected when using Mastra's tracing system.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"currentSpan\",\n            type: \"AISpan\",\n            isOptional: true,\n            description: \"Current AI span for creating child spans and adding metadata. Use this to create custom child spans or update span attributes during execution.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"tracingOptions\",\n      type: \"TracingOptions\",\n      isOptional: true,\n      description: \"Options for AI tracing configuration.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"metadata\",\n            type: \"Record<string, any>\",\n            isOptional: true,\n            description: \"Metadata to add to the root trace span. Useful for adding custom attributes like user IDs, session IDs, or feature flags.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"outputOptions\",\n      type: \"OutputOptions\",\n      isOptional: true,\n      description: \"Options for AI tracing configuration.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"includeState\",\n            type: \"boolean\",\n            isOptional: true,\n            description: \"Whether to include the workflow run state in the result.\"\n          }]\n        }\n      ]\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"result\",\n      type: \"Promise<WorkflowResult<TState, TOutput, TSteps>>\",\n      description: \"A promise that resolves to the workflow execution result containing step outputs and status\",\n    },\n    {\n      name: \"traceId\",\n      type: \"string\",\n      isOptional: true,\n      description: \"The trace ID associated with this execution when AI tracing is enabled. Use this to correlate logs and debug execution flow.\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript showLineNumbers copy\nif (result.status === \"suspended\") {\n  const resumedResults = await run.resume({\n    step: result.suspended[0],\n    resumeData: { value: \"resume data\" }\n  });\n}\n```\n> **Note**: When exactly one step is suspended, you can omit the `step` parameter and the workflow will automatically resume that step. For workflows with multiple suspended steps, you must explicitly specify which step to resume.\n\n## Related\n\n- [Workflows overview](../../../docs/workflows/overview.mdx#run-workflow)\n- [Workflow.createRunAsync()](../create-run.mdx)\n- [Suspend and resume](../../../docs/workflows/suspend-and-resume.mdx)\n- [Human in the loop example](../../../examples/workflows/human-in-the-loop.mdx)\n\n\n","path":null,"size_bytes":4395,"size_tokens":null},"docs/mastra/06-reference/94_workflow-dountil.md":{"content":"---\ntitle: \"Reference: Workflow.dountil() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow.dountil()` method in workflows, which creates a loop that executes a step until a condition is met.\n---\n\n# Workflow.dountil()\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow-methods/dountil\n\nThe `.dountil()` method executes a step until a condition is met. It always runs the step at least once before evaluating the condition. The first time the condition is evaluated, `iterationCount` is `1`.\n\n## Usage example\n\n```typescript copy\nworkflow.dountil(step1, async ({ inputData }) => true);\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"step\",\n      type: \"Step\",\n      description: \"The step instance to execute in the loop\",\n      isOptional: false,\n    },\n    {\n      name: \"condition\",\n      type: \"(params : ExecuteParams & { iterationCount: number }) => Promise<boolean>\",\n      description:\n        \"A function that returns a boolean indicating whether to continue the loop. The function receives the execution parameters and the iteration count.\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"Workflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Control Flow](../../../docs/workflows/control-flow.mdx)\n\n- [ExecuteParams](../step.mdx#ExecuteParams)\n\n\n","path":null,"size_bytes":1443,"size_tokens":null},"docs/mastra/00-getting-started/01_about-mastra.md":{"content":"---\ntitle: \"About Mastra | Mastra Docs\"\ndescription: \"Mastra is an all-in-one framework for building AI-powered applications and agents with a modern TypeScript stack.\"\n---\n\nimport YouTube from \"@/components/youtube\";\n\n# About Mastra\n[EN] Source: https://mastra.ai/en/docs\n\nFrom the team behind Gatsby, Mastra is a framework for building AI-powered applications and agents with a modern TypeScript stack.\n\nIt includes everything you need to go from early prototypes to production-ready applications. Mastra integrates with frontend and backend frameworks like React, Next.js, and Node, or you can deploy it anywhere as a standalone server. It's the easiest way to build, tune, and scale reliable AI products.\n\n<YouTube id=\"8o_Ejbcw5s8\" />\n\n## Why Mastra?\n\nPurpose-built for TypeScript and designed around established AI patterns, Mastra gives you everything you need to build great AI applications out-of-the-box.\n\nSome highlights include:\n\n- [**Model routing**](/models) - Connect to 40+ providers through one standard interface. Use models from OpenAI, Anthropic, Gemini, and more.\n\n- [**Agents**](/docs/agents/overview) - Build autonomous agents that use LLMs and tools to solve open-ended tasks. Agents reason about goals, decide which tools to use,  and iterate internally until the model emits a final answer or an optional stopping condition is met.\n\n- [**Workflows**](/docs/workflows/overview) - When you need explicit control over execution, use Mastra's graph-based workflow engine to orchestrate complex multi-step processes. Mastra workflows use an intuitive syntax for control flow (`.then()`, `.branch()`, `.parallel()`).\n\n- [**Human-in-the-loop**](/docs/workflows/suspend-and-resume) - Suspend an agent or workflow and await user input or approval before resuming. Mastra uses [storage](/docs/server-db/storage) to remember execution state, so you can pause indefinitely and resume where you left off.\n\n- **Context management** - Give your agents the right context at the right time. Provide [conversation history](/docs/memory/conversation-history), [retrieve](/docs/rag/overview) data from your sources (APIs, databases, files), and add human-like [working](/docs/memory/working-memory) and [semantic](/docs/memory/semantic-recall) memory so your agents behave coherently. \n\n- **Integrations** - Bundle agents and workflows into existing React, Next.js, or Node.js apps, or ship them as standalone endpoints. When building UIs, integrate with agentic libraries like Vercel's AI SDK UI and CopilotKit to bring your AI assistant to life on the web.\n\n- **Production essentials** - Shipping reliable agents takes ongoing insight, evaluation, and iteration. With built-in [evals](/docs/evals/overview) and [observability](/docs/observability/overview), Mastra gives you the tools to observe, measure, and refine continuously.\n\n\n## What can you build?\n\n- AI-powered applications that combine language understanding, reasoning, and action to solve real-world tasks.\n\n- Conversational agents for customer support, onboarding, or internal queries.\n\n- Domain-specific copilots for coding, legal, finance, research, or creative work.\n\n- Workflow automations that trigger, route, and complete multi-step processes.\n\n- Decision-support tools that analyse data and provide actionable recommendations.\n\nExplore real-world examples in our [case studies](/blog/category/case-studies) and [community showcase](/showcase).\n\n\n## Get started\n\nFollow the [Installation guide](/docs/getting-started/installation) for step-by-step setup with the CLI or a manual install.\n\nIf you're new to AI agents, check out our [templates](/docs/getting-started/templates), [course](/course), and [YouTube videos](https://youtube.com/@mastra-ai) to start building with Mastra today.\n\nWe can't wait to see what you build âœŒï¸ \n\n---\ntitle: Understanding the Mastra Cloud Dashboard\ndescription: Details of each feature available in Mastra Cloud\n---\n\nimport { MastraCloudCallout } from '@/components/mastra-cloud-callout'\n\n# Navigating the Dashboard\n[EN] Source: https://mastra.ai/en/docs/mastra-cloud/dashboard\n\nThis page explains how to navigate the Mastra Cloud dashboard, where you can configure your project, view deployment details, and interact with agents and workflows using the built-in [Playground](/docs/mastra-cloud/dashboard#playground).\n\n<MastraCloudCallout />\n\n## Overview\n\nThe **Overview** page provides details about your application, including its domain URL, status, latest deployment, and connected agents and workflows.\n\n![Project dashboard](/image/mastra-cloud/mastra-cloud-project-dashboard.jpg)\n\nKey features:\n\nEach project shows its current deployment status, active domains, and environment variables, so you can quickly understand how your application is running.\n\n## Deployments\n\nThe **Deployments** page shows recent builds and gives you quick access to detailed build logs. Click any row to view more information about a specific deployment.\n\n![Dashboard deployment](/image/mastra-cloud/mastra-cloud-dashboard-deployments.jpg)\n\nKey features:\n\nEach deployment includes its current status, the Git branch it was deployed from, and a title generated from the commit hash.\n\n## Logs\n\nThe **Logs** page is where you'll find detailed information to help debug and monitor your application's behavior in the production environment.\n\n![Dashboard logs](/image/mastra-cloud/mastra-cloud-dashboard-logs.jpg)\n\nKey features:\n\nEach log includes a severity level and detailed messages showing agent, workflow, and storage activity.\n\n## Settings\n\nOn the **Settings** page you can modify the configuration of your application.\n\n![Dashboard settings](/image/mastra-cloud/mastra-cloud-dashboard-settings.jpg)\n\nKey features:\n\nYou can manage environment variables, edit key project settings like the name and branch, configure storage with LibSQLStore, and set a stable URL for your endpoints.\n\n> Changes to configuration require a new deployment before taking effect.\n\n## Playground\n\n### Agents\n\nOn the **Agents** page you'll see all agents used in your application. Click any agent to interact using the chat interface.\n\n![Dashboard playground agents](/image/mastra-cloud/mastra-cloud-dashboard-playground-agents.jpg)\n\nKey features:\n\nTest your agents in real time using the chat interface, review traces of each interaction, and see evaluation scores for every response.\n\n### Workflows\n\nOn the **Workflows** page you'll see all workflows used in your application. Click any workflow to interact using the runner interface.\n\n![Dashboard playground workflows](/image/mastra-cloud/mastra-cloud-dashboard-playground-workflows.jpg)\n\nKey features:\n\nVisualize your workflow with a step-by-step graph, view execution traces, and run workflows directly using the built-in runner.\n\n### Tools\n\nOn the **Tools** page you'll see all tools used by your agents. Click any tool to interact using the input interface.\n\n![Dashboard playground tools](/image/mastra-cloud/mastra-cloud-dashboard-playground-tools.jpg)\n\nKey features:\n\nTest your tools by providing an input that matches the schema and viewing the structured output.\n\n## MCP Servers\n\nThe **MCP Servers** page lists all MCP Servers included in your application. Click any MCP Server for more information.\n\n![Dashboard playground mcp servers](/image/mastra-cloud/mastra-cloud-dashboard-playground-mcpservers.jpg)\n\nKey features:\n\nEach MCP Server includes API endpoints for HTTP and SSE, along with IDE configuration snippets for tools like Cursor and Windsurf.\n\n## Next steps\n\n- [Understanding Tracing and Logs](/docs/mastra-cloud/observability)\n\n\n---\ntitle: Observability in Mastra Cloud\ndescription: Monitoring and debugging tools for Mastra Cloud deployments\n---\n\nimport { MastraCloudCallout } from '@/components/mastra-cloud-callout'\n\n# Understanding Tracing and Logs\n[EN] Source: https://mastra.ai/en/docs/mastra-cloud/observability\n\nMastra Cloud captures execution data to help you monitor your application's behavior in the production environment.\n\n<MastraCloudCallout />\n\n## Logs\n\nYou can view detailed logs for debugging and monitoring your application's behavior on the [Logs](/docs/mastra-cloud/dashboard#logs) page of the Dashboard.\n\n![Dashboard logs](/image/mastra-cloud/mastra-cloud-dashboard-logs.jpg)\n\nKey features:\n\nEach log entry includes its severity level and a detailed message showing agent, workflow, or storage activity.\n\n## Traces\n\nMore detailed traces are available for both agents and workflows by using a [logger](/docs/observability/logging) or enabling [telemetry](/docs/observability/tracing) using one of our [supported providers](/reference/observability/providers).\n\n### Agents\n\nWith a [logger](/docs/observability/logging) enabled, you can view detailed outputs from your agents in the **Traces** section of the Agents Playground.\n\n![observability agents](/image/mastra-cloud/mastra-cloud-observability-agents.jpg)\n\nKey features:\n\nTools passed to the agent during generation are standardized using `convertTools`. This includes retrieving client-side tools, memory tools, and tools exposed from workflows.\n\n\n### Workflows\n\nWith a [logger](/docs/observability/logging) enabled, you can view detailed outputs from your workflows in the **Traces** section of the Workflows Playground.\n\n![observability workflows](/image/mastra-cloud/mastra-cloud-observability-workflows.jpg)\n\nKey features:\n\nWorkflows are created using `createWorkflow`, which sets up steps, metadata, and tools. You can run them with `runWorkflow` by passing input and options.\n\n## Next steps\n\n- [Logging](/docs/observability/logging)\n- [Tracing](/docs/observability/tracing)\n\n\n---\ntitle: Mastra Cloud\ndescription: Deployment and monitoring service for Mastra applications\n---\n\nimport { MastraCloudCallout } from '@/components/mastra-cloud-callout'\nimport { FileTree } from \"nextra/components\";\n\n# Mastra Cloud\n[EN] Source: https://mastra.ai/en/docs/mastra-cloud/overview\n\n[Mastra Cloud](https://mastra.ai/cloud) is a platform for deploying, managing, monitoring, and debugging Mastra applications. When you [deploy](/docs/mastra-cloud/setting-up) your application, Mastra Cloud exposes your agents, tools, and workflows as REST API endpoints.\n\n<MastraCloudCallout />\n\n## Platform features\n\nDeploy and manage your applications with automated builds, organized projects, and no additional configuration.\n\n![Platform features](/image/mastra-cloud/mastra-cloud-platform-features.jpg)\n\nKey features:\n\nMastra Cloud supports zero-config deployment, continuous integration with GitHub, and atomic deployments that package agents, tools, and workflows together.\n\n## Project Dashboard\n\nMonitor and debug your applications with detailed output logs, deployment state, and interactive tools.\n\n![Project dashboard](/image/mastra-cloud/mastra-cloud-project-dashboard.jpg)\n\nKey features:\n\nThe Project Dashboard gives you an overview of your application's status and deployments, with access to logs and a built-in playground for testing agents and workflows.\n\n## Project structure\n\nUse a standard Mastra project structure for proper detection and deployment.\n\n<FileTree>\n  <FileTree.Folder name=\"src\" defaultOpen>\n    <FileTree.Folder name=\"mastra\" defaultOpen>\n      <FileTree.Folder name=\"agents\" defaultOpen>\n        <FileTree.File name=\"agent-name.ts\" />\n      </FileTree.Folder>\n      <FileTree.Folder name=\"tools\" defaultOpen>\n        <FileTree.File name=\"tool-name.ts\" />\n      </FileTree.Folder>\n      <FileTree.Folder name=\"workflows\" defaultOpen>\n        <FileTree.File name=\"workflow-name.ts\" />\n      </FileTree.Folder>\n      <FileTree.File name=\"index.ts\" />\n    </FileTree.Folder>\n  </FileTree.Folder>\n  <FileTree.File name=\"package.json\" />\n</FileTree>\n\nMastra Cloud scans your repository for:\n\n- **Agents**: Defined using: `new Agent({...})`\n- **Tools**: Defined using: `createTool({...})`\n- **Workflows**: Defined using: `createWorkflow({...})`\n- **Steps**: Defined using: `createStep({...})`\n- **Environment Variables**: API keys and configuration variables\n\n## Technical implementation\n\nMastra Cloud is purpose-built for Mastra agents, tools, and workflows. It handles long-running requests, records detailed traces for every execution, and includes built-in support for evals.\n\n## Next steps\n\n- [Setting Up and Deploying](/docs/mastra-cloud/setting-up)\n\n\n---\ntitle: Setting Up a Project\ndescription: Configuration steps for Mastra Cloud projects\n---\n\nimport { MastraCloudCallout } from '@/components/mastra-cloud-callout'\nimport { Steps } from \"nextra/components\";\n\n# Setting Up and Deploying\n[EN] Source: https://mastra.ai/en/docs/mastra-cloud/setting-up\n\nThis page explains how to set up a project on [Mastra Cloud](https://mastra.ai/cloud) with automatic deployments using our GitHub integration.\n\n<MastraCloudCallout />\n\n## Prerequisites\n\n- A [Mastra Cloud](https://mastra.ai/cloud) account\n- A GitHub account / repository containing a Mastra application\n\n> See our [Getting started](/docs/getting-started/installation) guide to scaffold out a new Mastra project with sensible defaults.\n\n## Setup and Deploy process\n\n<Steps>\n\n### Sign in to Mastra Cloud\n\nHead over to [https://cloud.mastra.ai/](https://cloud.mastra.ai) and sign in with either:\n\n- **GitHub**\n- **Google**\n\n### Install the Mastra GitHub app\n\nWhen prompted, install the Mastra GitHub app.\n\n![Install GitHub](/image/mastra-cloud/mastra-cloud-install-github.jpg)\n\n### Create a new project\n\nClick the **Create new project** button to create a new project.\n\n![Create new project](/image/mastra-cloud/mastra-cloud-create-new-project.jpg)\n\n### Import a Git repository\n\nSearch for a repository, then click **Import**.\n\n![Import Git repository](/image/mastra-cloud/mastra-cloud-import-git-repository.jpg)\n\n### Configure the deployment\n\nMastra Cloud automatically detects the right build settings, but you can customize them using the options described below.\n\n![Deployment details](/image/mastra-cloud/mastra-cloud-deployment-details.jpg)\n\n- **Importing from GitHub**: The GitHub repository name\n- **Project name**: Customize the project name\n- **Branch**: The branch to deploy from\n- **Project root**: The root directory of your project\n- **Mastra directory**: Where Mastra files are located\n- **Environment variables**: Add environment variables used by the application\n- **Build and Store settings**:\n   - **Install command**: Runs pre-build to install project dependencies\n   - **Project setup command**: Runs pre-build to prepare any external dependencies\n   - **Port**: The network port the server will use\n   - **Store settings**: Use Mastra Cloud's built-in [LibSQLStore](/docs/storage/overview) storage\n- **Deploy Project**: Starts the deployment process\n\n### Deploy project\n\nClick **Deploy Project** to create and deploy your application using the configuration youâ€™ve set.\n\n</Steps>\n\n## Successful deployment\n\nAfter a successful deployment you'll be shown the **Overview** screen where you can view your project's status, domains, latest deployments and connected agents and workflows.\n\n![Successful deployment](/image/mastra-cloud/mastra-cloud-successful-deployment.jpg)\n\n## Continuous integration\n\nYour project is now configured with automatic deployments which occur whenever you push to the configured branch of your GitHub repository.\n\n## Testing your application\n\nAfter a successful deployment you can test your agents and workflows from the [Playground](/docs/mastra-cloud/dashboard#playground) in Mastra Cloud, or interact with them using our [Client SDK](/docs/client-js/overview).\n\n## Next steps\n\n- [Navigating the Dashboard](/docs/mastra-cloud/dashboard)\n\n\n","path":null,"size_bytes":15575,"size_tokens":null},"docs/mastra/01-agents/01_calling-agents.md":{"content":"---\ntitle: \"Calling Agents | Agents | Mastra Docs\"\ndescription: Example for how to call agents.\n---\n\n# Calling Agents\n[EN] Source: https://mastra.ai/en/examples/agents/calling-agents\n\nThere are multiple ways to interact with agents created using Mastra. Below you will find examples of how to call agents using workflow steps, tools, the [Mastra Client SDK](../../docs/server-db/mastra-client.mdx), and the command line for quick local testing.\n\nThis page demonstrates how to call the `harryPotterAgent` described in the [Changing the System Prompt](./system-prompt.mdx) example.\n\n## From a workflow step\n\nThe `mastra` instance is passed as an argument to a workflow stepâ€™s `execute` function. It provides access to registered agents using `getAgent()`. Use this method to retrieve your agent, then call `generate()` with a prompt.\n\n```typescript filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({\n  // ...\n  execute: async ({ mastra }) => {\n\n    const agent = mastra.getAgent(\"harryPotterAgent\");\n    const response = await agent.generate(\"What is your favorite room in Hogwarts?\");\n\n    console.log(response.text);\n  }\n});\n\nexport const testWorkflow = createWorkflow({\n  // ...\n})\n  .then(step1)\n  .commit();\n```\n\n## From a tool\n\nThe `mastra` instance is available within a toolâ€™s `execute` function. Use `getAgent()` to retrieve a registered agent and call `generate()` with a prompt.\n\n```typescript filename=\"src/mastra/tools/test-tool.ts\" showLineNumbers copy\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nexport const testTool = createTool({\n  // ...\n  execute: async ({ mastra }) => {\n\n    const agent = mastra.getAgent(\"harryPotterAgent\");\n    const response = await agent.generate(\"What is your favorite room in Hogwarts?\");\n\n    console.log(response!.text);\n  }\n});\n```\n\n## From Mastra Client\n\nThe `mastraClient` instance provides access to registered agents. Use `getAgent()` to retrieve an agent and call `generate()` with an object containing a `messages` array of role/content pairs.\n\n```typescript showLineNumbers copy\nimport { mastraClient } from \"../lib/mastra-client\";\n\nconst agent = mastraClient.getAgent(\"harryPotterAgent\");\nconst response = await agent.generate({\n  messages: [\n    {\n      role: \"user\",\n      content: \"What is your favorite room in Hogwarts?\"\n    }\n  ]\n});\n\nconsole.log(response.text);\n```\n\n> See [Mastra Client SDK](../../docs/server-db/mastra-client.mdx) for more information.\n\n### Run the script\n\nRun this script from your command line using:\n\n```bash\nnpx tsx src/test-agent.ts\n```\n\n## Using HTTP or curl\n\nYou can interact with a registered agent by sending a `POST` request to your Mastra application's `/generate` endpoint. Include a `messages` array of role/content pairs.\n\n```bash\ncurl -X POST http://localhost:4111/api/agents/harryPotterAgent/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"What is your favorite room in Hogwarts?\"\n      }\n    ]\n  }'| jq -r '.text'\n```\n\n\n## Example output\n\n```text\nWell, if I had to choose, I'd say the Gryffindor common room.\nIt's where I've spent some of my best moments with Ron and Hermione.\nThe warm fire, the cozy armchairs, and the sense of camaraderie make it feel like home.\nPlus, it's where we plan all our adventures!\n```\n\n\n","path":null,"size_bytes":3471,"size_tokens":null},"src/mastra/inngest/client.ts":{"content":"import { Inngest } from \"inngest\";\nimport { realtimeMiddleware } from \"@inngest/realtime\";\n\n// Use development configuration when NODE_ENV is not \"production\"\nexport const inngest = new Inngest(\n  process.env.NODE_ENV === \"production\"\n    ? {\n        id: \"replit-agent-workflow\",\n        name: \"Replit Agent Workflow System\",\n      }\n    : {\n        id: \"mastra\",\n        baseUrl: \"http://0.0.0.0:3000\",\n        isDev: true,\n        middleware: [realtimeMiddleware()],\n      },\n);\n","path":null,"size_bytes":481,"size_tokens":null},"docs/mastra/06-reference/96_workflow-foreach.md":{"content":"---\ntitle: \"Reference: Workflow.foreach() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow.foreach()` method in workflows, which creates a loop that executes a step for each item in an array.\n---\n\n# Workflow.foreach()\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow-methods/foreach\n\nThe `.foreach()` method creates a loop that executes a step for each item in an array.\n\n## Usage example\n\n```typescript copy\nworkflow.foreach(step1, { concurrency: 2 });\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"step\",\n      type: \"Step\",\n      description:\n        \"The step instance to execute in the loop. The previous step must return an array type.\",\n      isOptional: false,\n    },\n    {\n      name: \"opts\",\n      type: \"object\",\n      description:\n        \"Optional configuration for the loop. The concurrency option controls how many iterations can run in parallel (default: 1)\",\n      isOptional: true,\n      properties: [\n        {\n          name: \"concurrency\",\n          type: \"number\",\n          description:\n            \"The number of concurrent iterations allowed (default: 1)\",\n          isOptional: true,\n        },\n      ],\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"Workflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Repeating with foreach](../../../docs/workflows/control-flow.mdx#repeating-with-foreach)\n\n\n","path":null,"size_bytes":1488,"size_tokens":null},"docs/mastra/05-guides/00_ai-recruiter.md":{"content":"---\ntitle: \"Building an AI Recruiter | Mastra Workflows | Guides\"\ndescription: Guide on building a recruiter workflow in Mastra to gather and process candidate information using LLMs.\n---\n\nimport { Steps } from \"nextra/components\";\n\n# Building an AI Recruiter\n[EN] Source: https://mastra.ai/en/guides/guide/ai-recruiter\n\nIn this guide, you'll learn how Mastra helps you build workflows with LLMs.\n\nYou'll create a workflow that gathers information from a candidate's resume, then branches to either a technical or behavioral question based on the candidate's profile. Along the way, you'll see how to structure workflow steps, handle branching, and integrate LLM calls.\n\n## Prerequisites\n\n- Node.js `v20.0` or later installed\n- An API key from a supported [Model Provider](/models)\n- An existing Mastra project (Follow the [installation guide](/docs/getting-started/installation) to set up a new project)\n\n## Building the Workflow\n\nSet up the Workflow, define steps to extract and classify candidate data, and then ask suitable follow-up questions.\n\n<Steps>\n\n### Define the Workflow\n\nCreate a new file `src/mastra/workflows/candidate-workflow.ts` and define your workflow:\n\n```ts copy filename=\"src/mastra/workflows/candidate-workflow.ts\"\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nexport const candidateWorkflow = createWorkflow({\n  id: \"candidate-workflow\",\n  inputSchema: z.object({\n    resumeText: z.string(),\n  }),\n  outputSchema: z.object({\n    askAboutSpecialty: z.object({\n      question: z.string(),\n    }),\n    askAboutRole: z.object({\n      question: z.string(),\n    }),\n  }),\n}).commit();\n```\n\n### Step: Gather Candidate Info\n\nYou want to extract candidate details from the resume text and classify the person as \"technical\" or \"non-technical\". This step calls an LLM to parse the resume and returns structured JSON, including the name, technical status, specialty, and the original resume text. Defined through the `inputSchema` you get access to the `resumeText` inside `execute()`. Use it to prompt an LLM and return the organized fields.\n\nTo the existing `src/mastra/workflows/candidate-workflow.ts` file add the following:\n\n```ts copy filename=\"src/mastra/workflows/candidate-workflow.ts\"\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\n\nconst recruiter = new Agent({\n  name: \"Recruiter Agent\",\n  instructions: `You are a recruiter.`,\n  model: openai(\"gpt-4o-mini\"),\n});\n\nconst gatherCandidateInfo = createStep({\n  id: \"gatherCandidateInfo\",\n  inputSchema: z.object({\n    resumeText: z.string(),\n  }),\n  outputSchema: z.object({\n    candidateName: z.string(),\n    isTechnical: z.boolean(),\n    specialty: z.string(),\n    resumeText: z.string(),\n  }),\n  execute: async ({ inputData }) => {\n    const resumeText = inputData?.resumeText;\n\n    const prompt = `Extract details from the resume text:\n\"${resumeText}\"`;\n\n    const res = await recruiter.generate(prompt, {\n      structuredOutput: {\n        schema: z.object({\n          candidateName: z.string(),\n          isTechnical: z.boolean(),\n          specialty: z.string(),\n          resumeText: z.string(),\n        })\n      },\n    });\n\n    return res.object;\n  },\n});\n```\n\nSince you're using a Recruiter agent inside `execute()` you need to define it above the step and add the necessary imports.\n\n### Step: Technical Question\n\nThis step prompts a candidate who is identified as \"technical\" for more information about how they got into their specialty. It uses the entire resume text so the LLM can craft a relevant follow-up question.\n\nTo the existing `src/mastra/workflows/candidate-workflow.ts` file add the following:\n\n```ts copy filename=\"src/mastra/workflows/candidate-workflow.ts\"\nconst askAboutSpecialty = createStep({\n  id: \"askAboutSpecialty\",\n  inputSchema: z.object({\n    candidateName: z.string(),\n    isTechnical: z.boolean(),\n    specialty: z.string(),\n    resumeText: z.string(),\n  }),\n  outputSchema: z.object({\n    question: z.string(),\n  }),\n  execute: async ({ inputData: candidateInfo }) => {\n    const prompt = `You are a recruiter. Given the resume below, craft a short question\nfor ${candidateInfo?.candidateName} about how they got into \"${candidateInfo?.specialty}\".\nResume: ${candidateInfo?.resumeText}`;\n    const res = await recruiter.generate(prompt);\n\n    return { question: res?.text?.trim() || \"\" };\n  },\n});\n```\n\n### Step: Behavioral Question\n\nIf the candidate is \"non-technical\", you want a different follow-up question. This step asks what interests them most about the role, again referencing their complete resume text. The `execute()` function solicits a role-focused query from the LLM.\n\nTo the existing `src/mastra/workflows/candidate-workflow.ts` file add the following:\n\n```ts filename=\"src/mastra/workflows/candidate-workflow.ts\" copy\nconst askAboutRole = createStep({\n  id: \"askAboutRole\",\n  inputSchema: z.object({\n    candidateName: z.string(),\n    isTechnical: z.boolean(),\n    specialty: z.string(),\n    resumeText: z.string(),\n  }),\n  outputSchema: z.object({\n    question: z.string(),\n  }),\n  execute: async ({ inputData: candidateInfo }) => {\n    const prompt = `You are a recruiter. Given the resume below, craft a short question\nfor ${candidateInfo?.candidateName} asking what interests them most about this role.\nResume: ${candidateInfo?.resumeText}`;\n    const res = await recruiter.generate(prompt);\n    return { question: res?.text?.trim() || \"\" };\n  },\n});\n```\n\n### Add Steps to the Workflow\n\nYou now combine the steps to implement branching logic based on the candidate's technical status. The workflow first gathers candidate data, then either asks about their specialty or about their role, depending on `isTechnical`. This is done by chaining `gatherCandidateInfo` with `askAboutSpecialty` and `askAboutRole`.\n\nTo the existing `src/mastra/workflows/candidate-workflow.ts` file change the `candidateWorkflow` like so:\n\n```ts filename=\"src/mastra/workflows/candidate-workflow.ts\" copy {10-14}\nexport const candidateWorkflow = createWorkflow({\n  id: \"candidate-workflow\",\n  inputSchema: z.object({\n    resumeText: z.string(),\n  }),\n  outputSchema: z.object({\n    askAboutSpecialty: z.object({\n      question: z.string(),\n    }),\n    askAboutRole: z.object({\n      question: z.string(),\n    }),\n  }),\n})\n  .then(gatherCandidateInfo)\n  .branch([\n    [async ({ inputData: { isTechnical } }) => isTechnical, askAboutSpecialty],\n    [async ({ inputData: { isTechnical } }) => !isTechnical, askAboutRole],\n  ])\n  .commit();\n```\n\n### Register the Workflow with Mastra\n\nIn your `src/mastra/index.ts` file, register the workflow:\n\n```ts copy filename=\"src/mastra/index.ts\" {2, 5}\nimport { Mastra } from \"@mastra/core\";\nimport { candidateWorkflow } from \"./workflows/candidate-workflow\";\n\nexport const mastra = new Mastra({\n  workflows: { candidateWorkflow },\n});\n```\n\n</Steps>\n\n## Testing the Workflow\n\nYou can test your workflow inside Mastra's [playground](../../docs/server-db/local-dev-playground.mdx) by starting the development server:\n\n```bash copy\nmastra dev\n```\n\nIn the sidebar, navigate to **Workflows** and select **candidate-workflow**. In the middle you'll see a graph view of your workflow and on the right sidebar the **Run** tab is selected by default. Inside this tab you can enter a resume text, for example:\n\n```text copy\nKnowledgeable Software Engineer with more than 10 years of experience in software development. Proven expertise in the design and development of software databases and optimization of user interfaces.\n```\n\nAfter entering the resume text, press the **Run** button. You should now see two status boxes (`GatherCandidateInfo` and `AskAboutSpecialty`) which contain the output of the workflow steps.\n\nYou can also test the workflow programmatically by calling [`.createRunAsync()`](../../reference/workflows/create-run.mdx) and [`.start()`](../../reference/workflows/run-methods/start.mdx). Create a new file `src/test-workflow.ts` and add the following:\n\n```ts copy filename=\"src/test-workflow.ts\"\nimport { mastra } from \"./mastra\";\n\nconst run = await mastra.getWorkflow(\"candidateWorkflow\").createRunAsync();\n\nconst res = await run.start({\n  inputData: {\n    resumeText:\n      \"Knowledgeable Software Engineer with more than 10 years of experience in software development. Proven expertise in the design and development of software databases and optimization of user interfaces.\",\n  },\n});\n\n// Dump the complete workflow result (includes status, steps and result)\nconsole.log(JSON.stringify(res, null, 2));\n\n// Get the workflow output value\nif (res.status === \"success\") {\n  const question = res.result.askAboutRole?.question ?? res.result.askAboutSpecialty?.question;\n\n  console.log(`Output value: ${question}`);\n}\n```\n\nNow, run the workflow and get output in your terminal:\n\n```bash copy\nnpx tsx src/test-workflow.ts\n```\n\nYou've just built a workflow to parse a resume and decide which question to ask based on the candidate's technical abilities. Congrats and happy hacking!\n\n\n","path":null,"size_bytes":9026,"size_tokens":null},"docs/mastra/03-workflows/09_step-retries.md":{"content":"---\ntitle: \"Step Retries | Error Handling | Mastra Docs\"\ndescription: \"Automatically retry failed steps in Mastra workflows with configurable retry policies.\"\n---\n\n# Step Retries\n[EN] Source: https://mastra.ai/en/reference/legacyWorkflows/step-retries\n\nMastra provides built-in retry mechanisms to handle transient failures in workflow steps. This allows workflows to recover gracefully from temporary issues without requiring manual intervention.\n\n## Overview\n\nWhen a step in a workflow fails (throws an exception), Mastra can automatically retry the step execution based on a configurable retry policy. This is useful for handling:\n\n- Network connectivity issues\n- Service unavailability\n- Rate limiting\n- Temporary resource constraints\n- Other transient failures\n\n## Default Behavior\n\nBy default, steps do not retry when they fail. This means:\n\n- A step will execute once\n- If it fails, it will immediately mark the step as failed\n- The workflow will continue to execute any subsequent steps that don't depend on the failed step\n\n## Configuration Options\n\nRetries can be configured at two levels:\n\n### 1. Workflow-level Configuration\n\nYou can set a default retry configuration for all steps in a workflow:\n\n```typescript\nconst workflow = new LegacyWorkflow({\n  name: \"my-workflow\",\n  retryConfig: {\n    attempts: 3, // Number of retries (in addition to the initial attempt)\n    delay: 1000, // Delay between retries in milliseconds\n  },\n});\n```\n\n### 2. Step-level Configuration\n\nYou can also configure retries on individual steps, which will override the workflow-level configuration for that specific step:\n\n```typescript\nconst fetchDataStep = new LegacyStep({\n  id: \"fetchData\",\n  execute: async () => {\n    // Fetch data from external API\n  },\n  retryConfig: {\n    attempts: 5, // This step will retry up to 5 times\n    delay: 2000, // With a 2-second delay between retries\n  },\n});\n```\n\n## Retry Parameters\n\nThe `retryConfig` object supports the following parameters:\n\n| Parameter  | Type   | Default | Description                                                       |\n| ---------- | ------ | ------- | ----------------------------------------------------------------- |\n| `attempts` | number | 0       | The number of retry attempts (in addition to the initial attempt) |\n| `delay`    | number | 1000    | Time in milliseconds to wait between retries                      |\n\n## How Retries Work\n\nWhen a step fails, Mastra's retry mechanism:\n\n1. Checks if the step has retry attempts remaining\n2. If attempts remain:\n   - Decrements the attempt counter\n   - Transitions the step to a \"waiting\" state\n   - Waits for the configured delay period\n   - Retries the step execution\n3. If no attempts remain or all attempts have been exhausted:\n   - Marks the step as \"failed\"\n   - Continues workflow execution (for steps that don't depend on the failed step)\n\nDuring retry attempts, the workflow execution remains active but paused for the specific step that is being retried.\n\n## Examples\n\n### Basic Retry Example\n\n```typescript\nimport { LegacyWorkflow, LegacyStep } from \"@mastra/core/workflows/legacy\";\n\n// Define a step that might fail\nconst unreliableApiStep = new LegacyStep({\n  id: \"callUnreliableApi\",\n  execute: async () => {\n    // Simulate an API call that might fail\n    const random = Math.random();\n    if (random < 0.7) {\n      throw new Error(\"API call failed\");\n    }\n    return { data: \"API response data\" };\n  },\n  retryConfig: {\n    attempts: 3, // Retry up to 3 times\n    delay: 2000, // Wait 2 seconds between attempts\n  },\n});\n\n// Create a workflow with the unreliable step\nconst workflow = new LegacyWorkflow({\n  name: \"retry-demo-workflow\",\n});\n\nworkflow.step(unreliableApiStep).then(processResultStep).commit();\n```\n\n### Workflow-level Retries with Step Override\n\n```typescript\nimport { LegacyWorkflow, LegacyStep } from \"@mastra/core/workflows/legacy\";\n\n// Create a workflow with default retry configuration\nconst workflow = new LegacyWorkflow({\n  name: \"multi-retry-workflow\",\n  retryConfig: {\n    attempts: 2, // All steps will retry twice by default\n    delay: 1000, // With a 1-second delay\n  },\n});\n\n// This step uses the workflow's default retry configuration\nconst standardStep = new LegacyStep({\n  id: \"standardStep\",\n  execute: async () => {\n    // Some operation that might fail\n  },\n});\n\n// This step overrides the workflow's retry configuration\nconst criticalStep = new LegacyStep({\n  id: \"criticalStep\",\n  execute: async () => {\n    // Critical operation that needs more retry attempts\n  },\n  retryConfig: {\n    attempts: 5, // Override with 5 retry attempts\n    delay: 5000, // And a longer 5-second delay\n  },\n});\n\n// This step disables retries\nconst noRetryStep = new LegacyStep({\n  id: \"noRetryStep\",\n  execute: async () => {\n    // Operation that should not retry\n  },\n  retryConfig: {\n    attempts: 0, // Explicitly disable retries\n  },\n});\n\nworkflow.step(standardStep).then(criticalStep).then(noRetryStep).commit();\n```\n\n## Monitoring Retries\n\nYou can monitor retry attempts in your logs. Mastra logs retry-related events at the `debug` level:\n\n```\n[DEBUG] Step fetchData failed (runId: abc-123)\n[DEBUG] Attempt count for step fetchData: 2 remaining attempts (runId: abc-123)\n[DEBUG] Step fetchData waiting (runId: abc-123)\n[DEBUG] Step fetchData finished waiting (runId: abc-123)\n[DEBUG] Step fetchData pending (runId: abc-123)\n```\n\n## Best Practices\n\n1. **Use Retries for Transient Failures**: Only configure retries for operations that might experience transient failures. For deterministic errors (like validation failures), retries won't help.\n\n2. **Set Appropriate Delays**: Consider using longer delays for external API calls to allow time for services to recover.\n\n3. **Limit Retry Attempts**: Don't set extremely high retry counts as this could cause workflows to run for excessive periods during outages.\n\n4. **Implement Idempotent Operations**: Ensure your step's `execute` function is idempotent (can be called multiple times without side effects) since it may be retried.\n\n5. **Consider Backoff Strategies**: For more advanced scenarios, consider implementing exponential backoff in your step's logic for operations that might be rate-limited.\n\n## Related\n\n- [Step Class Reference](./step-class.mdx)\n- [Workflow Configuration](./workflow.mdx)\n- [Error Handling in Workflows](../../docs/workflows-legacy/error-handling.mdx)\n\n\n","path":null,"size_bytes":6387,"size_tokens":null},"docs/mastra/03-workflows/17_tool-streaming.md":{"content":"---\ntitle: \"Tool Streaming | Streaming | Mastra\"\ndescription: \"Learn how to use tool streaming in Mastra, including handling tool calls, tool results, and tool execution events during streaming.\"\n---\n\nimport { Callout } from \"nextra/components\";\n\n# Tool streaming\n[EN] Source: https://mastra.ai/en/docs/streaming/tool-streaming\n\nTool streaming in Mastra enables tools to send incremental results while they run, rather than waiting until execution finishes. This allows you to surface partial progress, intermediate states, or progressive data directly to users or upstream agents and workflows.\n\nStreams can be written to in two main ways:\n\n- **From within a tool**: every tool receives a `writer` argument, which is a writable stream you can use to push updates as execution progresses.\n- **From an agent stream**: you can also pipe an agentâ€™s `stream` output directly into a toolâ€™s writer, making it easy to chain agent responses into tool results without extra glue code.\n\nBy combining writable tool streams with agent streaming, you gain fine grained control over how intermediate results flow through your system and into the user experience.\n\n## Agent using tool\n\nAgent streaming can be combined with tool calls, allowing tool outputs to be written directly into the agentâ€™s streaming response. This makes it possible to surface tool activity as part of the overall interaction.\n\n```typescript {4,10} showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nimport { testTool } from \"../tools/test-tool\";\n\nexport const testAgent = new Agent({\n  name: \"test-agent\",\n  instructions: \"You are a weather agent.\",\n  model: openai(\"gpt-4o-mini\"),\n  tools: { testTool }\n});\n```\n\n### Using the `writer` argument\n\nThe `writer` argument is passed to a toolâ€™s `execute` function and can be used to emit custom events, data, or values into the active stream. This enables tools to provide intermediate results or status updates while execution is still in progress.\n\n<Callout type=\"warning\">\nYou must `await` the call to `writer.write(...)` or else you will lock the stream and get a `WritableStream is locked` error.\n</Callout>\n\n```typescript {5,8,15} showLineNumbers copy\nimport { createTool } from \"@mastra/core/tools\";\n\nexport const testTool = createTool({\n  // ...\n  execute: async ({ context, writer }) => {\n    const { value } = context;\n\n   await writer?.write({\n      type: \"custom-event\",\n      status: \"pending\"\n    });\n\n    const response = await fetch(...);\n\n   await writer?.write({\n      type: \"custom-event\",\n      status: \"success\"\n    });\n\n    return {\n      value: \"\"\n    };\n  }\n});\n```\n\nYou can also use `writer.custom` if you want to emit top level stream chunks, This useful and relevant when\nintegrating with UI Frameworks\n\n```typescript {5,8,15} showLineNumbers copy\nimport { createTool } from \"@mastra/core/tools\";\n\nexport const testTool = createTool({\n  // ...\n  execute: async ({ context, writer }) => {\n    const { value } = context;\n\n   await writer?.custom({\n      type: \"data-tool-progress\",\n      status: \"pending\"\n    });\n\n    const response = await fetch(...);\n\n   await writer?.custom({\n      type: \"data-tool-progress\",\n      status: \"success\"\n    });\n\n    return {\n      value: \"\"\n    };\n  }\n});\n```\n\n### Inspecting stream payloads\n\nEvents written to the stream are included in the emitted chunks. These chunks can be inspected to access any custom fields, such as event types, intermediate values, or tool-specific data.\n\n```typescript showLineNumbers copy\nconst stream = await testAgent.stream([\n  \"What is the weather in London?\",\n  \"Use the testTool\"\n]);\n\nfor await (const chunk of stream) {\n  if (chunk.payload.output?.type === \"custom-event\") {\n    console.log(JSON.stringify(chunk, null, 2));\n  }\n}\n```\n\n## Tool using an agent\n\nPipe an agentâ€™s `textStream` to the toolâ€™s `writer`. This streams partial output, and Mastra automatically aggregates the agentâ€™s usage into the tool run.\n\n```typescript showLineNumbers copy\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nexport const testTool = createTool({\n  // ...\n  execute: async ({ context, mastra, writer }) => {\n    const { city } = context;\n\n    const testAgent = mastra?.getAgent(\"testAgent\");\n    const stream = await testAgent?.stream(`What is the weather in ${city}?`);\n\n    await stream!.textStream.pipeTo(writer!);\n\n    return {\n      value: await stream!.text\n    };\n  }\n});\n```\n\n\n\n","path":null,"size_bytes":4473,"size_tokens":null},"docs/mastra/01-agents/03_guardrails.md":{"content":"---\ntitle: \"Guardrails | Agents | Mastra Docs\"\ndescription: \"Learn how to implement guardrails using input and output processors to secure and control AI interactions.\"\n---\n\n# Guardrails\n[EN] Source: https://mastra.ai/en/docs/agents/guardrails\n\nAgents use processors to apply guardrails to inputs and outputs. They run before or after each interaction, giving you a way to review, transform, or block information as it passes between the user and the agent.\n\nProcessors can be configured as:\n\n- **`inputProcessors`**: Applied before messages reach the language model.\n- **`outputProcessors`**: Applied to responses before they're returned to users.\n\nSome processors are *hybrid*, meaning they can be used with either `inputProcessors` or `outputProcessors`, depending on where the logic should be applied.\n\n## When to use processors\n\nUse processors for content moderation, prompt injection prevention, response sanitization, message transformation, and other security-related controls. Mastra provides several built-in input and output processors for common use cases.\n\n## Adding processors to an agent\n\nImport and instantiate the relevant processor class, and pass it to your agentâ€™s configuration using either the `inputProcessors` or `outputProcessors` option:\n\n```typescript {3,9-17} filename=\"src/mastra/agents/moderated-agent.ts\" showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { ModerationProcessor } from \"@mastra/core/processors\";\n\nexport const moderatedAgent = new Agent({\n  name: \"moderated-agent\",\n  instructions: \"You are a helpful assistant\",\n  model: openai(\"gpt-4o-mini\"),\n  inputProcessors: [\n    new ModerationProcessor({\n      model: openai(\"gpt-4.1-nano\"),\n      categories: [\"hate\", \"harassment\", \"violence\"],\n      threshold: 0.7,\n      strategy: \"block\",\n      instructions: \"Detect and flag inappropriate content in user messages\",\n    })\n  ]\n});\n```\n\n## Input processors\n\nInput processors are applied before user messages reach the language model. They are useful for normalization, validation, content moderation, prompt injection detection, and security checks.\n\n### Normalizing user messages\n\nThe `UnicodeNormalizer` is an input processor that cleans and normalizes user input by unifying Unicode characters, standardizing whitespace, and removing problematic symbols, allowing the LLM to better understand user messages.\n\n```typescript {6-9} filename=\"src/mastra/agents/normalized-agent.ts\" showLineNumbers copy\nimport { UnicodeNormalizer } from \"@mastra/core/processors\";\n\nexport const normalizedAgent = new Agent({\n  // ...\n  inputProcessors: [\n    new UnicodeNormalizer({\n      stripControlChars: true,\n      collapseWhitespace: true,\n    })\n  ],\n});\n```\n\n> See [UnicodeNormalizer](../../reference/processors/unicode-normalizer.mdx) for a full list of configuration options.\n\n### Preventing prompt injection\n\nThe `PromptInjectionDetector` is an input processor that scans user messages for prompt injection, jailbreak attempts, and system override patterns. It uses an LLM to classify risky input and can block or rewrite it before it reaches the model.\n\n```typescript {6-11} filename=\"src/mastra/agents/secure-agent.ts\" showLineNumbers copy\nimport { PromptInjectionDetector } from \"@mastra/core/processors\";\n\nexport const secureAgent = new Agent({\n  // ...\n  inputProcessors: [\n    new PromptInjectionDetector({\n      model: openai(\"gpt-4.1-nano\"),\n      threshold: 0.8,\n      strategy: 'rewrite',\n      detectionTypes: ['injection', 'jailbreak', 'system-override'],\n    })\n  ],\n});\n```\n\n> See [PromptInjectionDetector](../../reference/processors/prompt-injection-detector.mdx) for a full list of configuration options.\n\n### Detecting and translating language\n\nThe `LanguageDetector` is an input processor that detects and translates user messages into a target language, enabling multilingual support while maintaining consistent interaction. It uses an LLM to identify the language and perform the translation.\n\n```typescript {6-11} filename=\"src/mastra/agents/multilingual-agent.ts\" showLineNumbers copy\nimport { LanguageDetector } from \"@mastra/core/processors\";\n\nexport const multilingualAgent = new Agent({\n  // ...\n  inputProcessors: [\n    new LanguageDetector({\n      model: openai(\"gpt-4.1-nano\"),\n      targetLanguages: ['English', 'en'],\n      strategy: 'translate',\n      threshold: 0.8,\n    })\n  ],\n});\n```\n\n> See [LanguageDetector](../../reference/processors/language-detector.mdx) for a full list of configuration options.\n\n## Output processors\n\nOutput processors are applied after the language model generates a response, but before it is returned to the user. They are useful for response optimization, moderation, transformation, and applying safety controls.\n\n### Batching streamed output\n\nThe `BatchPartsProcessor` is an output processor that combines multiple stream parts before emitting them to the client. This reduces network overhead and improves the user experience by consolidating small chunks into larger batches.\n\n```typescript {6-10} filename=\"src/mastra/agents/batched-agent.ts\" showLineNumbers copy\nimport { BatchPartsProcessor } from \"@mastra/core/processors\";\n\nexport const batchedAgent = new Agent({\n  // ...\n  outputProcessors: [\n    new BatchPartsProcessor({\n      batchSize: 5,\n      maxWaitTime: 100,\n      emitOnNonText: true\n    })\n  ]\n});\n```\n\n> See [BatchPartsProcessor](../../reference/processors/batch-parts-processor.mdx) for a full list of configuration options.\n\n### Limiting token usage\n\nThe `TokenLimiterProcessor` is an output processor that limits the number of tokens in model responses. It helps manage cost and performance by truncating or blocking messages when the limit is exceeded.\n\n```typescript {6-10, 13-15} filename=\"src/mastra/agents/limited-agent.ts\" showLineNumbers copy\nimport { TokenLimiterProcessor } from \"@mastra/core/processors\";\n\nexport const limitedAgent = new Agent({\n  // ...\n  outputProcessors: [\n    new TokenLimiterProcessor({\n      limit: 1000,\n      strategy: \"truncate\",\n      countMode: \"cumulative\"\n    })\n  ]\n})\n```\n\n> See [TokenLimiterProcessor](../../reference/processors/token-limiter-processor.mdx) for a full list of configuration options.\n\n### Scrubbing system prompts\n\nThe `SystemPromptScrubber` is an output processor that detects and redacts system prompts or other internal instructions from model responses. It helps prevent unintended disclosure of prompt content or configuration details that could introduce security risks. It uses an LLM to identify and redact sensitive content based on configured detection types.\n\n```typescript {5-13} filename=\"src/mastra/agents/scrubbed-agent.ts\" copy showLineNumbers\nimport { SystemPromptScrubber } from \"@mastra/core/processors\";\n\nconst scrubbedAgent = new Agent({\n  outputProcessors: [\n    new SystemPromptScrubber({\n      model: openai(\"gpt-4.1-nano\"),\n      strategy: \"redact\",\n      customPatterns: [\"system prompt\", \"internal instructions\"],\n      includeDetections: true,\n      instructions: \"Detect and redact system prompts, internal instructions, and security-sensitive content\",\n      redactionMethod: \"placeholder\",\n      placeholderText: \"[REDACTED]\"\n    })\n  ]\n});\n```\n\n> See [SystemPromptScrubber](../../reference/processors/system-prompt-scrubber.mdx) for a full list of configuration options.\n\n## Hybrid processors\n\nHybrid processors can be applied either before messages are sent to the language model or before responses are returned to the user. They are useful for tasks like content moderation and PII redaction.\n\n### Moderating input and output\n\nThe `ModerationProcessor` is a hybrid processor that detects inappropriate or harmful content across categories like hate, harassment, and violence. It can be used to moderate either user input or model output, depending on where it's applied. It uses an LLM to classify the message and can block or rewrite it based on your configuration.\n\n```typescript {6-11, 14-16} filename=\"src/mastra/agents/moderated-agent.ts\" showLineNumbers copy\nimport { ModerationProcessor } from \"@mastra/core/processors\";\n\nexport const moderatedAgent = new Agent({\n  // ...\n  inputProcessors: [\n    new ModerationProcessor({\n      model: openai(\"gpt-4.1-nano\"),\n      threshold: 0.7,\n      strategy: \"block\",\n      categories: [\"hate\", \"harassment\", \"violence\"]\n    })\n  ],\n  outputProcessors: [\n    new ModerationProcessor({\n      // ...\n    })\n  ]\n});\n```\n\n> See [ModerationProcessor](../../reference/processors/moderation-processor.mdx) for a full list of configuration options.\n\n### Detecting and redacting PII\n\nThe `PIIDetector` is a hybrid processor that detects and removes personally identifiable information such as emails, phone numbers, and credit cards. It can redact either user input or model output, depending on where it's applied. It uses an LLM to identify sensitive content based on configured detection types.\n\n```typescript {6-13, 16-18} filename=\"src/mastra/agents/private-agent.ts\" showLineNumbers copy\nimport { PIIDetector } from \"@mastra/core/processors\";\n\nexport const privateAgent = new Agent({\n  // ...\n  inputProcessors: [\n    new PIIDetector({\n      model: openai(\"gpt-4.1-nano\"),\n      threshold: 0.6,\n      strategy: 'redact',\n      redactionMethod: 'mask',\n      detectionTypes: ['email', 'phone', 'credit-card'],\n      instructions: \"Detect and mask personally identifiable information.\"\n    })\n  ],\n  outputProcessors: [\n    new PIIDetector({\n      // ...\n    })\n  ]\n});\n```\n\n> See [PIIDetector](../../reference/processors/pii-detector.mdx) for a full list of configuration options.\n\n## Applying multiple processors\n\nYou can apply multiple processors by listing them in the `inputProcessors` or `outputProcessors` array. They run in sequence, with each processor receiving the output of the one before it.\n\nA typical order might be:\n\n1. **Normalization**: Standardize input format (`UnicodeNormalizer`).\n2. **Security checks**: Detect threats or sensitive content (`PromptInjectionDetector`, `PIIDetector`).\n3. **Filtering**: Block or transform messages (`ModerationProcessor`).\n\nThe order affects behavior, so arrange processors to suit your goals.\n\n```typescript filename=\"src/mastra/agents/test-agent.ts\" showLineNumbers copy\nimport {\n  UnicodeNormalizer,\n  ModerationProcessor,\n  PromptInjectionDetector,\n  PIIDetector\n  } from \"@mastra/core/processors\";\n\nexport const testAgent = new Agent({\n  // ...\n  inputProcessors: [\n    new UnicodeNormalizer({\n      //...\n    }),\n    new PromptInjectionDetector({\n      // ...\n    }),\n    new PIIDetector({\n      // ...\n    }),\n    new ModerationProcessor({\n      // ...\n    })\n  ],\n});\n```\n\n## Processor strategies\n\nMany of the built-in processors support a `strategy` parameter that controls how they handle flagged input or output. Supported values may include: `block`, `warn`, `detect`, or `redact`.\n\nMost strategies allow the request to continue without interruption. When `block` is used, the processor calls its internal `abort()` function, which immediately stops the request and prevents any subsequent processors from running.\n\n```typescript {8} filename=\"src/mastra/agents/private-agent.ts\" showLineNumbers copy\nimport { PIIDetector } from \"@mastra/core/processors\";\n\nexport const privateAgent = new Agent({\n  // ...\n  inputProcessors: [\n    new PIIDetector({\n      // ...\n      strategy: \"block\"\n    })\n  ]\n})\n```\n\n### Handling blocked requests\n\nWhen a processor blocks a request, the agent will still return successfully without throwing an error. To handle blocked requests, check for `tripwire` or `tripwireReason` in the response.\n\nFor example, if an agent uses the `PIIDetector` with `strategy: \"block\"` and the request includes a credit card number, it will be blocked and the response will include a `tripwireReason`.\n\n#### `.generate()` example\n\n```typescript {3-4, } showLineNumbers\nconst result = await agent.generate(\"Is this credit card number valid?: 4543 1374 5089 4332\");\n\nconsole.error(result.tripwire);\nconsole.error(result.tripwireReason);\n```\n#### `.stream()` example\n\n```typescript {4-5} showLineNumbers\nconst stream = await agent.stream(\"Is this credit card number valid?: 4543 1374 5089 4332\");\n\nfor await (const chunk of stream.fullStream) {\n  if (chunk.type === \"tripwire\") {\n    console.error(chunk.payload.tripwireReason);\n  }\n}\n```\nIn this case, the `tripwireReason` indicates that a credit card number was detected:\n\n```text\nPII detected. Types: credit-card\n```\n\n## Custom processors\n\nIf the built-in processors donâ€™t cover your needs, you can create your own by extending the `Processor` class.\n\nAvailable examples:\n\n- [Message Length Limiter](../../examples/processors/message-length-limiter)\n- [Response Length Limiter](../../examples/rocessors/response-length-limiter)\n- [Response Validator](../../examples/processors/response-validator)\n\n\n","path":null,"size_bytes":12856,"size_tokens":null},"docs/mastra/06-reference/00_agent-class.md":{"content":"---\ntitle: \"Reference: Agent Class | Agents | Mastra Docs\"\ndescription: \"Documentation for the `Agent` class in Mastra, which provides the foundation for creating AI agents with various capabilities.\"\n---\n\n# Agent Class\n[EN] Source: https://mastra.ai/en/reference/agents/agent\n\nThe `Agent` class is the foundation for creating AI agents in Mastra. It provides methods for generating responses, streaming interactions, and handling voice capabilities.\n\n## Usage examples\n\n### Basic string instructions\n\n```typescript filename=\"src/mastra/agents/string-agent.ts\" showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\n// String instructions\nexport const agent = new Agent({\n  name: \"test-agent\",\n  instructions: 'You are a helpful assistant that provides concise answers.',\n  model: openai(\"gpt-4o\")\n});\n\n// System message object\nexport const agent2 = new Agent({\n  name: \"test-agent-2\",\n  instructions: { \n    role: \"system\", \n    content: \"You are an expert programmer\" \n  },\n  model: openai(\"gpt-4o\")\n});\n\n// Array of system messages\nexport const agent3 = new Agent({\n  name: \"test-agent-3\",\n  instructions: [\n    { role: \"system\", content: \"You are a helpful assistant\" },\n    { role: \"system\", content: \"You have expertise in TypeScript\" }\n  ],\n  model: openai(\"gpt-4o\")\n});\n```\n\n### Single CoreSystemMessage\n\nUse CoreSystemMessage format to access additional properties like `providerOptions` for provider-specific configurations:\n\n```typescript filename=\"src/mastra/agents/core-message-agent.ts\" showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nexport const agent = new Agent({\n  name: \"core-message-agent\",\n  instructions: {\n    role: 'system',\n    content: 'You are a helpful assistant specialized in technical documentation.',\n    providerOptions: {\n      openai: { \n        reasoningEffort: 'low'\n      }\n    }\n  },\n  model: openai(\"gpt-5\")\n});\n```\n\n### Multiple CoreSystemMessages\n\n```typescript filename=\"src/mastra/agents/multi-message-agent.ts\" showLineNumbers copy\nimport { anthropic } from \"@ai-sdk/anthropic\";\nimport { Agent } from \"@mastra/core/agent\";\n\n// This could be customizable based on the user\nconst preferredTone = { \n  role: 'system', \n  content: 'Always maintain a professional and empathetic tone.',\n};\n\nexport const agent = new Agent({\n  name: \"multi-message-agent\",\n  instructions: [\n    { role: 'system', content: 'You are a customer service representative.' },\n    preferredTone,\n    { \n      role: 'system', \n      content: 'Escalate complex issues to human agents when needed.',\n      providerOptions: {\n        anthropic: { cacheControl: { type: 'ephemeral' } },\n      },\n    },\n  ],\n  model: anthropic('claude-sonnet-4-20250514'),\n});\n```\n\n## Constructor parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"id\",\n      type: \"string\",\n      isOptional: true,\n      description: \"Optional unique identifier for the agent. Defaults to `name` if not provided.\",\n    },\n    {\n      name: \"name\",\n      type: \"string\",\n      isOptional: false,\n      description: \"Unique identifier for the agent.\",\n    },\n    {\n      name: \"description\",\n      type: \"string\",\n      isOptional: true,\n      description: \"Optional description of the agent's purpose and capabilities.\",\n    },\n    {\n      name: \"instructions\",\n      type: \"SystemMessage | ({ runtimeContext: RuntimeContext }) => SystemMessage | Promise<SystemMessage>\",\n      isOptional: false,\n      description: `Instructions that guide the agent's behavior. Can be a string, array of strings, system message object, \n        array of system messages, or a function that returns any of these types dynamically. \n        SystemMessage types: string | string[] | CoreSystemMessage | CoreSystemMessage[] | SystemModelMessage | SystemModelMessage[]`,\n    },\n    {\n      name: \"model\",\n      type: \"MastraLanguageModel | ({ runtimeContext: RuntimeContext }) => MastraLanguageModel | Promise<MastraLanguageModel>\",\n      isOptional: false,\n      description: \"The language model used by the agent. Can be provided statically or resolved at runtime.\",\n    },\n    {\n      name: \"agents\",\n      type: \"Record<string, Agent> | ({ runtimeContext: RuntimeContext }) => Record<string, Agent> | Promise<Record<string, Agent>>\",\n      isOptional: true,\n      description: \"Sub-Agents that the agent can access. Can be provided statically or resolved dynamically.\",\n    },\n    {\n      name: \"tools\",\n      type: \"ToolsInput | ({ runtimeContext: RuntimeContext }) => ToolsInput | Promise<ToolsInput>\",\n      isOptional: true,\n      description: \"Tools that the agent can access. Can be provided statically or resolved dynamically.\",\n    },\n    {\n      name: \"workflows\",\n      type: \"Record<string, Workflow> | ({ runtimeContext: RuntimeContext }) => Record<string, Workflow> | Promise<Record<string, Workflow>>\",\n      isOptional: true,\n      description: \"Workflows that the agent can execute. Can be static or dynamically resolved.\",\n    },\n    {\n      name: \"defaultGenerateOptions\",\n      type: \"AgentGenerateOptions | ({ runtimeContext: RuntimeContext }) => AgentGenerateOptions | Promise<AgentGenerateOptions>\",\n      isOptional: true,\n      description: \"Default options used when calling `generate()`.\",\n    },\n    {\n      name: \"defaultStreamOptions\",\n      type: \"AgentStreamOptions | ({ runtimeContext: RuntimeContext }) => AgentStreamOptions | Promise<AgentStreamOptions>\",\n      isOptional: true,\n      description: \"Default options used when calling `stream()`.\",\n    },\n    {\n      name: \"defaultStreamOptions\",\n      type: \"AgentExecutionOptions | ({ runtimeContext: RuntimeContext }) => AgentExecutionOptions | Promise<AgentExecutionOptions>\",\n      isOptional: true,\n      description: \"Default options used when calling `stream()` in vNext mode.\",\n    },\n    {\n      name: \"mastra\",\n      type: \"Mastra\",\n      isOptional: true,\n      description: \"Reference to the Mastra runtime instance (injected automatically).\",\n    },\n    {\n      name: \"scorers\",\n      type: \"MastraScorers | ({ runtimeContext: RuntimeContext }) => MastraScorers | Promise<MastraScorers>\",\n      isOptional: true,\n      description: \"Scoring configuration for runtime evaluation and telemetry. Can be static or dynamically provided.\",\n    },\n    {\n      name: \"evals\",\n      type: \"Record<string, Metric>\",\n      isOptional: true,\n      description: \"Evaluation metrics for scoring agent responses.\",\n    },\n    {\n      name: \"memory\",\n      type: \"MastraMemory | ({ runtimeContext: RuntimeContext }) => MastraMemory | Promise<MastraMemory>\",\n      isOptional: true,\n      description: \"Memory module used for storing and retrieving stateful context.\",\n    },\n    {\n      name: \"voice\",\n      type: \"CompositeVoice\",\n      isOptional: true,\n      description: \"Voice settings for speech input and output.\",\n    },\n    {\n      name: \"inputProcessors\",\n      type: \"Processor[] | ({ runtimeContext: RuntimeContext }) => Processor[] | Promise<Processor[]>\",\n      isOptional: true,\n      description: \"Input processors that can modify or validate messages before they are processed by the agent. Must implement the `processInput` function.\",\n    },\n    {\n      name: \"outputProcessors\",\n      type: \"Processor[] | ({ runtimeContext: RuntimeContext }) => Processor[] | Promise<Processor[]>\",\n      isOptional: true,\n      description: \"Output processors that can modify or validate messages from the agent, before it is sent to the client. Must implement either (or both) of the `processOutputResult` and `processOutputStream` functions.\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"agent\",\n      type: \"Agent<TAgentId, TTools, TMetrics>\",\n      description: \"A new Agent instance with the specified configuration.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Agents overview](../../docs/agents/overview.mdx)\n- [Calling Agents](../../examples/agents/calling-agents.mdx)\n\n\n","path":null,"size_bytes":7969,"size_tokens":null},"src/triggers/cronTriggers.ts":{"content":"/**\n * Cron Trigger - Time-based Workflow Scheduling\n *\n * This module provides time-based triggering for Mastra workflows using cron expressions.\n * Unlike webhook-based triggers, cron triggers run on a schedule without external events.\n *\n * PATTERN:\n * 1. Define the cron schedule with a standard cron expression\n * 2. Pass the workflow to be executed\n * 3. Call registerCronTrigger BEFORE mastra initialization (not in apiRoutes array)\n *\n * IMPORTANT: Unlike webhook triggers, cron triggers are registered by calling\n * registerCronTrigger() directly in src/mastra/index.ts, NOT by spreading into apiRoutes.\n * This is because cron triggers don't create HTTP endpoints.\n *\n * CRON EXPRESSION FORMAT:\n * Standard 5-field cron format: minute hour day-of-month month day-of-week\n *\n * Example cron expressions:\n * Daily at 8 AM, every 2 hours, every 5 minutes, weekly on Sunday\n */\n\nimport { registerCronWorkflow } from \"../mastra/inngest\";\n\n/**\n * Register a cron-based trigger\n *\n * Usage in src/mastra/index.ts (call BEFORE mastra initialization):\n *\n * ```typescript\n * import { physicsJokeWorkflow } from \"./workflows/physicsJokeWorkflow\";\n * import { registerCronTrigger } from \"../triggers/cronTriggers\";\n *\n * // Register cron trigger (call this before creating the mastra instance)\n * registerCronTrigger({\n *   cronExpression: \"0 8 * * *\", // Daily at 8 AM\n *   workflow: physicsJokeWorkflow\n * });\n *\n * export const mastra = new Mastra({\n *   // ... rest of mastra config\n * });\n * ```\n *\n * Note: Returns empty array for consistency with trigger file conventions,\n * but this should NOT be spread into apiRoutes array.\n */\nexport function registerCronTrigger({\n  cronExpression,\n  workflow,\n}: {\n  cronExpression: string;\n  workflow: any;\n}) {\n  // Delegate to the helper in inngest/index.ts which manages inngestFunctions\n  registerCronWorkflow(cronExpression, workflow);\n\n  // Returns empty array for consistency with trigger file conventions\n  // Note: Do NOT spread this into apiRoutes - cron triggers don't create HTTP routes\n  return [];\n}\n","path":null,"size_bytes":2061,"size_tokens":null},"docs/mastra/06-reference/20_cli-create-mastra.md":{"content":"---\ntitle: \"Reference: create-mastra | CLI\"\ndescription: Documentation for the create-mastra command, which creates a new Mastra project with interactive setup options.\n---\n\nimport { Tabs, Tab } from \"@/components/tabs\";\n\n# create-mastra\n[EN] Source: https://mastra.ai/en/reference/cli/create-mastra\n\nThe `create-mastra` command **creates** a new standalone Mastra project. Use this command to scaffold a complete Mastra setup in a dedicated directory. You can run it with additional flags to customize the setup process.\n\n## Usage\n\n<Tabs items={[\"npm\", \"yarn\", \"pnpm\", \"bun\"]}>\n  <Tab>\n    ```bash copy\n    npx create-mastra@latest\n    ```\n  </Tab>\n  <Tab>\n    ```bash copy\n    yarn dlx create-mastra@latest\n    ```\n  </Tab>\n  <Tab>\n    ```bash copy\n    pnpm create mastra@latest\n    ```\n  </Tab>\n  <Tab>\n    ```bash copy\n    bun create mastra@latest\n    ```\n  </Tab>\n</Tabs>\n\n`create-mastra` automatically runs in _interactive_ mode, but you can also specify your project name and template with command line arguments.\n\n<Tabs items={[\"npm\", \"yarn\", \"pnpm\", \"bun\"]}>\n  <Tab>\n    ```bash copy\n    npx create-mastra@latest my-mastra-project -- --template coding-agent\n    ```\n  </Tab>\n  <Tab>\n    ```bash copy\n    yarn dlx create-mastra@latest --template coding-agent\n    ```\n  </Tab>\n  <Tab>\n    ```bash copy\n    pnpm create mastra@latest --template coding-agent\n    ```\n  </Tab>\n  <Tab>\n    ```bash copy\n    bun create mastra@latest --template coding-agent\n    ```\n  </Tab>\n</Tabs>\n\nCheck out the [full list](https://mastra.ai/api/templates.json) of templates and use the `slug` as input to the `--template` CLI flag.\n\nYou can also use any GitHub repo as a template (it has to be a valid Mastra project):\n\n```bash\nnpx create-mastra@latest my-mastra-project -- --template mastra-ai/template-coding-agent\n```\n\n## CLI flags\n\nInstead of an interactive prompt you can also define these CLI flags.\n\n<PropertiesTable\n  content={[\n    {\n      name: \"--version\",\n      type: \"boolean\",\n      description: \"Output the version number\",\n      isOptional: true,\n    },\n    {\n      name: \"--project-name\",\n      type: \"string\",\n      description:\n        \"Project name that will be used in package.json and as the project directory name\",\n      isOptional: true,\n    },\n    {\n      name: \"--default\",\n      type: \"boolean\",\n      description: \"Quick start with defaults (src, OpenAI, no examples)\",\n      isOptional: true,\n    },\n    {\n      name: \"--components\",\n      type: \"string\",\n      description:\n        \"Comma-separated list of components (agents, tools, workflows, scorers)\",\n      isOptional: true,\n    },\n    {\n      name: \"--llm\",\n      type: \"string\",\n      description:\n        \"Default model provider (openai, anthropic, groq, google, or cerebras)\",\n      isOptional: true,\n    },\n    {\n      name: \"--llm-api-key\",\n      type: \"string\",\n      description: \"API key for the model provider\",\n      isOptional: true,\n    },\n    {\n      name: \"--example\",\n      type: \"boolean\",\n      description: \"Include example code\",\n      isOptional: true,\n    },\n    {\n      name: \"--no-example\",\n      type: \"boolean\",\n      description: \"Do not include example code\",\n      isOptional: true,\n    },\n    {\n      name: \"--template\",\n      type: \"string\",\n      description:\n        \"Create project from a template (use template name, public GitHub URL, or leave blank to select from list)\",\n      isOptional: true,\n    },\n    {\n      name: \"--timeout\",\n      type: \"number\",\n      description:\n        \"Configurable timeout for package installation, defaults to 60000 ms\",\n      isOptional: true,\n    },\n    {\n      name: \"--dir\",\n      type: \"string\",\n      description: \"Target directory for Mastra source code (default: src/)\",\n      isOptional: true,\n    },\n    {\n      name: \"--mcp\",\n      type: \"string\",\n      description:\n        \"MCP Server for code editor (cursor, cursor-global, windsurf, vscode)\",\n      isOptional: true,\n    },\n    {\n      name: \"--help\",\n      type: \"boolean\",\n      description: \"Display help for command\",\n      isOptional: true,\n    },\n  ]}\n/>\n\n## Telemetry\n\nBy default, Mastra collects anonymous information about your project like your OS, Mastra version or Node.js version. You can read the [source code](https://github.com/mastra-ai/mastra/blob/main/packages/cli/src/analytics/index.ts) to check what's collected.\n\nYou can opt out of the CLI analytics by setting an environment variable:\n\n```bash copy\nMASTRA_TELEMETRY_DISABLED=1\n```\n\nYou can also set this while using other `mastra` commands:\n\n```bash copy\nMASTRA_TELEMETRY_DISABLED=1 npx create-mastra@latest\n```\n\n\n","path":null,"size_bytes":4596,"size_tokens":null},"docs/mastra/06-reference/100_workflow-sleep.md":{"content":"---\ntitle: \"Reference: Workflow.sleep() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow.sleep()` method in workflows, which pauses execution for a specified number of milliseconds.\n---\n\n# Workflow.sleep()\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow-methods/sleep\n\nThe `.sleep()` method pauses execution for a specified number of milliseconds. It accepts either a static number or a callback function for dynamic delays.\n\n## Usage example\n\n```typescript copy\nworkflow.sleep(5000);\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"milliseconds\",\n      type: \"number | ((context: { inputData: any }) => number | Promise<number>)\",\n      description: \"The number of milliseconds to pause execution, or a callback that returns the delay\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"Workflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\n\nconst step1 = createStep({...});\nconst step2 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .sleep(async ({ inputData }) => {\n    const { delayInMs } = inputData;\n    return delayInMs;\n  })\n  .then(step2)\n  .commit();\n```\n\n## Related\n\n- [Suspend & Resume](../../../docs/workflows/suspend-and-resume.mdx#sleep--events)\n\n\n","path":null,"size_bytes":1508,"size_tokens":null},"docs/mastra/03-workflows/08_inngest-workflows.md":{"content":"---\ntitle: \"Inngest Workflows | Workflows | Mastra Docs\"\ndescription: \"Inngest workflow allows you to run Mastra workflows with Inngest\"\n---\n\n# Inngest Workflow\n[EN] Source: https://mastra.ai/en/docs/workflows/inngest-workflow\n\n[Inngest](https://www.inngest.com/docs) is a developer platform for building and running background workflows, without managing infrastructure.\n\n## How Inngest Works with Mastra\n\nInngest and Mastra integrate by aligning their workflow models: Inngest organizes logic into functions composed of steps, and Mastra workflows defined using `createWorkflow` and `createStep` map directly onto this paradigm. Each Mastra workflow becomes an Inngest function with a unique identifier, and each step within the workflow maps to an Inngest step.\n\nThe `serve` function bridges the two systems by registering Mastra workflows as Inngest functions and setting up the necessary event handlers for execution and monitoring.\n\nWhen an event triggers a workflow, Inngest executes it step by step, memoizing each stepâ€™s result. This means if a workflow is retried or resumed, completed steps are skipped, ensuring efficient and reliable execution. Control flow primitives in Mastra, such as loops, conditionals, and nested workflows are seamlessly translated into the same Inngestâ€™s function/step model, preserving advanced workflow features like composition, branching, and suspension.\n\nReal-time monitoring, suspend/resume, and step-level observability are enabled via Inngestâ€™s publish-subscribe system and dashboard. As each step executes, its state and output are tracked using Mastra storage and can be resumed as needed.\n\n## Setup\n\n```sh\nnpm install @mastra/inngest @mastra/core @mastra/deployer\n```\n\n## Building an Inngest Workflow\n\nThis guide walks through creating a workflow with Inngest and Mastra, demonstrating a counter application that increments a value until it reaches 10.\n\n### Inngest Initialization\n\nInitialize the Inngest integration to obtain Mastra-compatible workflow helpers. The createWorkflow and createStep functions are used to create workflow and step objects that are compatible with Mastra and inngest.\n\nIn development\n\n```ts showLineNumbers copy filename=\"src/mastra/inngest/index.ts\"\nimport { Inngest } from \"inngest\";\nimport { realtimeMiddleware } from \"@inngest/realtime\";\n\nexport const inngest = new Inngest({\n  id: \"mastra\",\n  baseUrl:\"http://localhost:8288\",\n  isDev: true,\n  middleware: [realtimeMiddleware()],\n});\n```\n\nIn production\n\n```ts showLineNumbers copy filename=\"src/mastra/inngest/index.ts\"\nimport { Inngest } from \"inngest\";\nimport { realtimeMiddleware } from \"@inngest/realtime\";\n\nexport const inngest = new Inngest({\n  id: \"mastra\",\n  middleware: [realtimeMiddleware()],\n});\n```\n\n### Creating Steps\n\nDefine the individual steps that will compose your workflow:\n\n```ts showLineNumbers copy filename=\"src/mastra/workflows/index.ts\"\nimport { z } from \"zod\";\nimport { inngest } from \"../inngest\";\nimport { init } from \"@mastra/inngest\";\n\n// Initialize Inngest with Mastra, pointing to your local Inngest server\nconst { createWorkflow, createStep } = init(inngest);\n\n// Step: Increment the counter value\nconst incrementStep = createStep({\n  id: \"increment\",\n  inputSchema: z.object({\n    value: z.number(),\n  }),\n  outputSchema: z.object({\n    value: z.number(),\n  }),\n  execute: async ({ inputData }) => {\n    return { value: inputData.value + 1 };\n  },\n});\n```\n\n### Creating the Workflow\n\nCompose the steps into a workflow using the `dountil` loop pattern. The createWorkflow function creates a function on inngest server that is invocable.\n\n```ts showLineNumbers copy filename=\"src/mastra/workflows/index.ts\"\n// workflow that is registered as a function on inngest server\nconst workflow = createWorkflow({\n  id: \"increment-workflow\",\n  inputSchema: z.object({\n    value: z.number(),\n  }),\n  outputSchema: z.object({\n    value: z.number(),\n  }),\n}).then(incrementStep);\n\nworkflow.commit();\n\nexport { workflow as incrementWorkflow };\n```\n\n### Configuring the Mastra Instance and Executing the Workflow\n\nRegister the workflow with Mastra and configure the Inngest API endpoint:\n\n```ts showLineNumbers copy filename=\"src/mastra/index.ts\"\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { serve as inngestServe } from \"@mastra/inngest\";\nimport { incrementWorkflow } from \"./workflows\";\nimport { inngest } from \"./inngest\";\nimport { PinoLogger } from \"@mastra/loggers\";\n\n// Configure Mastra with the workflow and Inngest API endpoint\nexport const mastra = new Mastra({\n  workflows: {\n    incrementWorkflow,\n  },\n  server: {\n    // The server configuration is required to allow local docker container can connect to the mastra server\n    host: \"0.0.0.0\",\n    apiRoutes: [\n      // This API route is used to register the Mastra workflow (inngest function) on the inngest server\n      {\n        path: \"/api/inngest\",\n        method: \"ALL\",\n        createHandler: async ({ mastra }) => inngestServe({ mastra, inngest }),\n        // The inngestServe function integrates Mastra workflows with Inngest by:\n        // 1. Creating Inngest functions for each workflow with unique IDs (workflow.${workflowId})\n        // 2. Setting up event handlers that:\n        //    - Generate unique run IDs for each workflow execution\n        //    - Create an InngestExecutionEngine to manage step execution\n        //    - Handle workflow state persistence and real-time updates\n        // 3. Establishing a publish-subscribe system for real-time monitoring\n        //    through the workflow:${workflowId}:${runId} channel\n        //\n        // Optional: You can also pass additional Inngest functions to serve alongside workflows:\n        // createHandler: async ({ mastra }) => inngestServe({\n        //   mastra,\n        //   inngest,\n        //   functions: [customFunction1, customFunction2] // User-defined Inngest functions\n        // }),\n      },\n    ],\n  },\n  logger: new PinoLogger({\n    name: \"Mastra\",\n    level: \"info\",\n  }),\n});\n```\n\n### Running the Workflow locally\n\n> **Prerequisites:**\n>\n> - Docker installed and running\n> - Mastra project set up\n> - Dependencies installed (`npm install`)\n\n1. Run `npx mastra dev` to start the Mastra server on local to serve the server on port 4111.\n2. Start the Inngest Dev Server (via Docker)\n   In a new terminal, run:\n\n```sh\ndocker run --rm -p 8288:8288 \\\n  inngest/inngest \\\n  inngest dev -u http://host.docker.internal:4111/api/inngest\n```\n\n> **Note:** The URL after `-u` tells the Inngest dev server where to find your Mastra `/api/inngest` endpoint.\n\n3. Open the Inngest Dashboard\n\n- Visit [http://localhost:8288](http://localhost:8288) in your browser.\n- Go to the **Apps** section in the sidebar.\n- You should see your Mastra workflow registered.\n  ![Inngest Dashboard](/inngest-apps-dashboard.png)\n\n4. Invoke the Workflow\n\n- Go to the **Functions** section in the sidebar.\n- Select your Mastra workflow.\n- Click **Invoke** and use the following input:\n\n```json\n{\n  \"data\": {\n    \"inputData\": {\n      \"value\": 5\n    }\n  }\n}\n```\n\n![Inngest Function](/inngest-function-dashboard.png)\n\n5. **Monitor the Workflow Execution**\n\n- Go to the **Runs** tab in the sidebar.\n- Click on the latest run to see step-by-step execution progress.\n  ![Inngest Function Run](/inngest-runs-dashboard.png)\n\n### Running the Workflow in Production\n\n> **Prerequisites:**\n>\n> - Vercel account and Vercel CLI installed (`npm i -g vercel`)\n> - Inngest account\n> - Vercel token (recommended: set as environment variable)\n\n1. Add Vercel Deployer to Mastra instance\n\n```ts showLineNumbers copy filename=\"src/mastra/index.ts\"\nimport { VercelDeployer } from \"@mastra/deployer-vercel\";\n\nexport const mastra = new Mastra({\n  // ...other config\n  deployer: new VercelDeployer({\n    teamSlug: \"your_team_slug\",\n    projectName: \"your_project_name\",\n    // you can get your vercel token from the vercel dashboard by clicking on the user icon in the top right corner\n    // and then clicking on \"Account Settings\" and then clicking on \"Tokens\" on the left sidebar.\n    token: \"your_vercel_token\",\n  }),\n});\n```\n\n> **Note:** Set your Vercel token in your environment:\n>\n> ```sh\n> export VERCEL_TOKEN=your_vercel_token\n> ```\n\n2. Build the mastra instance\n\n```sh\nnpx mastra build\n```\n\n3. Deploy to Vercel\n\n```sh\ncd .mastra/output\nvercel --prod\n```\n\n> **Tip:** If you haven't already, log in to Vercel CLI with `vercel login`.\n\n4. Sync with Inngest Dashboard\n\n- Go to the [Inngest dashboard](https://app.inngest.com/env/production/apps).\n- Click **Sync new app with Vercel** and follow the instructions.\n- You should see your Mastra workflow registered as an app.\n  ![Inngest Dashboard](/inngest-apps-dashboard-prod.png)\n\n5. Invoke the Workflow\n\n- In the **Functions** section, select `workflow.increment-workflow`.\n- Click **All actions** (top right) > **Invoke**.\n- Provide the following input:\n\n```json\n{\n  \"data\": {\n    \"inputData\": {\n      \"value\": 5\n    }\n  }\n}\n```\n\n![Inngest Function Run](/inngest-function-dashboard-prod.png)\n\n6.  Monitor Execution\n\n- Go to the **Runs** tab.\n- Click the latest run to see step-by-step execution progress.\n  ![Inngest Function Run](/inngest-runs-dashboard-prod.png)\n\n## Advanced Usage: Adding Custom Inngest Functions\n\nYou can serve additional Inngest functions alongside your Mastra workflows by using the optional `functions` parameter in `inngestServe`.\n\n### Creating Custom Functions\n\nFirst, create your custom Inngest functions:\n\n```ts showLineNumbers copy filename=\"src/inngest/custom-functions.ts\"\nimport { inngest } from \"./inngest\";\n\n// Define custom Inngest functions\nexport const customEmailFunction = inngest.createFunction(\n  { id: 'send-welcome-email' },\n  { event: 'user/registered' },\n  async ({ event }) => {\n    // Custom email logic here\n    console.log(`Sending welcome email to ${event.data.email}`);\n    return { status: 'email_sent' };\n  }\n);\n\nexport const customWebhookFunction = inngest.createFunction(\n  { id: 'process-webhook' },\n  { event: 'webhook/received' },\n  async ({ event }) => {\n    // Custom webhook processing\n    console.log(`Processing webhook: ${event.data.type}`);\n    return { processed: true };\n  }\n);\n```\n\n### Serving Custom Functions with Workflows\n\nUpdate your Mastra configuration to include the custom functions:\n\n```ts showLineNumbers copy filename=\"src/mastra/index.ts\"\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { serve as inngestServe } from \"@mastra/inngest\";\nimport { incrementWorkflow } from \"./workflows\";\nimport { inngest } from \"./inngest\";\nimport { customEmailFunction, customWebhookFunction } from \"./inngest/custom-functions\";\n\nexport const mastra = new Mastra({\n  workflows: {\n    incrementWorkflow,\n  },\n  server: {\n    host: \"0.0.0.0\",\n    apiRoutes: [\n      {\n        path: \"/api/inngest\",\n        method: \"ALL\",\n        createHandler: async ({ mastra }) => inngestServe({\n          mastra,\n          inngest,\n          functions: [customEmailFunction, customWebhookFunction] // Add your custom functions\n        }),\n      },\n    ],\n  },\n});\n```\n\n### Function Registration\n\nWhen you include custom functions:\n\n1. **Mastra workflows** are automatically converted to Inngest functions with IDs like `workflow.${workflowId}`\n2. **Custom functions** retain their specified IDs (e.g., `send-welcome-email`, `process-webhook`)\n3. **All functions** are served together on the same `/api/inngest` endpoint\n\nThis allows you to combine Mastra's workflow orchestration with your existing Inngest functions seamlessly.\n\n\n","path":null,"size_bytes":11494,"size_tokens":null},"src/mastra/inngest/index.ts":{"content":"import { inngest } from \"./client\";\nimport { init, serve as originalInngestServe } from \"@mastra/inngest\";\nimport { registerApiRoute as originalRegisterApiRoute } from \"@mastra/core/server\";\nimport { type Mastra } from \"@mastra/core\";\nimport { type Inngest, InngestFunction, NonRetriableError } from \"inngest\";\n\n// Initialize Inngest with Mastra to get Inngest-compatible workflow helpers\nconst {\n  createWorkflow: originalCreateWorkflow,\n  createStep,\n  cloneStep,\n} = init(inngest);\n\nexport function createWorkflow(\n  params: Parameters<typeof originalCreateWorkflow>[0],\n): ReturnType<typeof originalCreateWorkflow> {\n  return originalCreateWorkflow({\n    ...params,\n    retryConfig: {\n      attempts: process.env.NODE_ENV === \"production\" ? 3 : 0,\n      ...(params.retryConfig ?? {}),\n    },\n  });\n}\n\n// Export the Inngest client and Inngest-compatible workflow helpers\nexport { inngest, createStep, cloneStep };\n\nconst inngestFunctions: InngestFunction.Any[] = [];\n\n// Create a middleware for Inngest to be able to route triggers to Mastra directly.\nexport function registerApiRoute<P extends string>(\n  ...args: Parameters<typeof originalRegisterApiRoute<P>>\n): ReturnType<typeof originalRegisterApiRoute<P>> {\n  const [path, options] = args;\n  if (typeof options !== \"object\") {\n    // This will throw an error.\n    return originalRegisterApiRoute(...args);\n  }\n\n  // Extract connector name from path\n  // For paths like \"/api/linear\" -> \"linear\"\n  // For paths like \"/linear\" or \"/linear/webhook\" -> \"linear\"\n  const pathWithoutSlash = path.replace(/^\\/+/, \"\");\n  const pathWithoutApi = pathWithoutSlash.startsWith(\"api/\")\n    ? pathWithoutSlash.substring(4)\n    : pathWithoutSlash;\n  // Take only the first segment as the connector name\n  const connectorName = pathWithoutApi.split(\"/\")[0];\n\n  inngestFunctions.push(\n    inngest.createFunction(\n      {\n        id: `api-${connectorName}`,\n        name: path,\n      },\n      {\n        // Match the event pattern created by createWebhook: event/api.webhooks.{connector-name}.action\n        event: `event/api.webhooks.${connectorName}.action`,\n      },\n      async ({ event, step }) => {\n        await step.run(\"forward request to Mastra\", async () => {\n          // It is hard to obtain an internal handle on the Hono server,\n          // so we just forward the request to the local Mastra server.\n          const response = await fetch(`http://0.0.0.0:5000${path}`, {\n            method: event.data.method,\n            headers: event.data.headers,\n            body: event.data.body,\n          });\n\n          if (!response.ok) {\n            if (\n              (response.status >= 500 && response.status < 600) ||\n              response.status == 429 ||\n              response.status == 408\n            ) {\n              // 5XX, 429 (Rate-Limit Exceeded), 408 (Request Timeout) are retriable.\n              throw new Error(\n                `Failed to forward request to Mastra: ${response.statusText}`,\n              );\n            } else {\n              // All other errors are non-retriable.\n              throw new NonRetriableError(\n                `Failed to forward request to Mastra: ${response.statusText}`,\n              );\n            }\n          }\n        });\n      },\n    ),\n  );\n\n  return originalRegisterApiRoute(...args);\n}\n\n// ======================================================================\n// TRIGGER FUNCTIONS - CHOOSE ONE BASED ON YOUR AUTOMATION TYPE\n// ======================================================================\n// An automation only has a single trigger type. Based on your trigger:\n//\n// FOR TIME-BASED AUTOMATIONS (cron/schedule):\n//   - Keep the registerCronWorkflow function below\n//   - Delete the registerApiRoute function above (entire function)\n//   - Used for: Daily reports, scheduled tasks, periodic checks\n//\n// FOR WEBHOOK-BASED AUTOMATIONS (Slack, Telegram, connectors):\n//   - Keep the registerApiRoute function above\n//   - Delete the registerCronWorkflow function below (entire function)\n//   - Used for: Slack bots, Telegram bots, GitHub webhooks, Linear webhooks, etc.\n// ======================================================================\n\n// Helper function for registering cron-based workflow triggers\nexport function registerCronWorkflow(cronExpression: string, workflow: any) {\n  console.log(\"ðŸ• [registerCronWorkflow] Registering cron trigger\", {\n    cronExpression,\n    workflowId: workflow?.id,\n  });\n\n  const cronFunction = inngest.createFunction(\n    { id: \"cron-trigger\" },\n    [{ event: \"replit/cron.trigger\" }, { cron: cronExpression }],\n    async ({ event, step }) => {\n      return await step.run(\"execute-cron-workflow\", async () => {\n        console.log(\"ðŸš€ [Cron Trigger] Starting scheduled workflow execution\", {\n          workflowId: workflow?.id,\n          scheduledTime: new Date().toISOString(),\n          cronExpression,\n        });\n\n        try {\n          const run = await workflow.createRunAsync();\n          console.log(\"ðŸ“ [Cron Trigger] Workflow run created\", {\n            runId: run?.id,\n          });\n\n          const result = await run.start({ inputData: {} });\n          console.log(\"âœ… [Cron Trigger] Workflow completed successfully\", {\n            workflowId: workflow?.id,\n            status: result?.status,\n          });\n\n          return result;\n        } catch (error) {\n          console.error(\"âŒ [Cron Trigger] Workflow execution failed\", {\n            workflowId: workflow?.id,\n            error: error instanceof Error ? error.message : String(error),\n            stack: error instanceof Error ? error.stack : undefined,\n          });\n          throw error;\n        }\n      });\n    },\n  );\n\n  inngestFunctions.push(cronFunction);\n  console.log(\n    \"âœ… [registerCronWorkflow] Cron trigger registered successfully\",\n    {\n      cronExpression,\n    },\n  );\n}\n\nexport function inngestServe({\n  mastra,\n  inngest,\n}: {\n  mastra: Mastra;\n  inngest: Inngest;\n}): ReturnType<typeof originalInngestServe> {\n  let serveHost: string | undefined = undefined;\n  if (process.env.NODE_ENV === \"production\") {\n    if (process.env.REPLIT_DOMAINS) {\n      serveHost = `https://${process.env.REPLIT_DOMAINS.split(\",\")[0]}`;\n    }\n  } else {\n    serveHost = \"http://0.0.0.0:5000\";\n  }\n  return originalInngestServe({\n    mastra,\n    inngest,\n    functions: inngestFunctions,\n    registerOptions: { serveHost },\n  });\n}\n","path":null,"size_bytes":6376,"size_tokens":null},"src/mastra/workflows/exampleWorkflow.ts":{"content":"// Use Inngest-compatible versions for this project\n// These are initialized with init(inngest) to work with the Inngest workflow engine\nimport { createStep, createWorkflow } from \"../inngest\";\nimport { z } from \"zod\";\nimport { exampleAgent } from \"../agents/exampleAgent\";\n\n/**\n * Example Mastra Workflow\n *\n * MASTRA WORKFLOW GUIDE:\n * - Workflows orchestrate multiple steps in sequence\n * - Each step has typed inputs/outputs for reliability\n * - Steps can use agents, tools, or custom logic\n */\n\n/**\n * Step 1: Process with Agent\n * This step demonstrates how to use an agent within a workflow\n */\nconst processWithAgent = createStep({\n  id: \"process-with-agent\",\n  description:\n    \"Processes the input message using an AI agent with optional analysis\", // Must contain a clear, concise description of what the step does.\n\n  // Define what this step expects as input\n  inputSchema: z.object({\n    message: z.string().describe(\"Message to process\"),\n    includeAnalysis: z\n      .boolean()\n      .optional()\n      .describe(\"Whether to include detailed analysis\"),\n  }),\n\n  // Defines what this step will output. Must match the inputSchema of the next step.\n  outputSchema: z.object({\n    agentResponse: z.string(),\n    processedData: z\n      .object({\n        original: z.string(),\n        processed: z.string(),\n        timestamp: z.string(),\n      })\n      .optional(),\n  }),\n\n  // Step logic - mastra is available for logging and utilities\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info(\"ðŸš€ [Step 1] Processing with agent...\");\n\n    // Construct a prompt for the agent\n    const prompt = `\n      Please process the following message using the example tool:\n      \"${inputData.message}\"\n\n      ${inputData.includeAnalysis ? \"Also provide a brief analysis of the results.\" : \"\"}\n    `;\n\n    // Call the agent using generateLegacy or streamLegacy for SDK v4 compatibility\n    const response = await exampleAgent.generateLegacy(\n      [{ role: \"user\", content: prompt }],\n      // We can add other properties, for instance the thread id keeps track of the message history\n      // {\n      //   resourceId: \"bot\",\n      //   threadId: inputData.threadId,\n      //   maxSteps: 5, // Allow multi-step reasoning if needed\n      // }\n    );\n\n    logger?.info(\"âœ… [Step 1] Agent processing complete\");\n\n    // In a real workflow, you might:\n    // - Parse structured data from the response\n    // - Extract specific information\n    // - Handle errors gracefully\n\n    return {\n      agentResponse: response.text,\n      processedData: {\n        original: inputData.message,\n        processed: inputData.message.toUpperCase(), // Simple mock processing\n        timestamp: new Date().toISOString(),\n      },\n    };\n  },\n});\n\n/**\n * Step 2: Output Results\n * This step demonstrates how to handle and output results\n */\nconst outputResults = createStep({\n  id: \"output-results\",\n  description:\n    \"Formats the agent's response and puts it in the form of a summary.\", // Must contain a clear, concise description of what the step does.\n\n  // This step receives the output from the previous step\n  inputSchema: z.object({\n    agentResponse: z.string(),\n    processedData: z\n      .object({\n        original: z.string(),\n        processed: z.string(),\n        timestamp: z.string(),\n      })\n      .optional(),\n  }),\n\n  // Final output schema - this is what the workflow returns\n  outputSchema: z.object({\n    summary: z.string(),\n    formattedOutput: z.string(),\n    success: z.boolean(),\n  }),\n\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info(\"ðŸ“¤ [Step 2] Outputting results...\");\n\n    /**\n     * In a real workflow, this step might:\n     * - Send data to an API\n     * - Write to a database\n     * - Send an email\n     * - Update a UI\n     * - Trigger webhooks\n     * - Store in a file system\n     */\n\n    // For this example, we'll format and log the output\n    const formattedOutput = `\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nðŸ“Š WORKFLOW RESULTS\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nðŸ¤– Agent Response:\n${inputData.agentResponse}\n\nðŸ“ Processed Data:\n${\n  inputData.processedData\n    ? `\n  â€¢ Original: ${inputData.processedData.original}\n  â€¢ Processed: ${inputData.processedData.processed}\n  â€¢ Timestamp: ${inputData.processedData.timestamp}\n`\n    : \"No processed data available\"\n}\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nðŸ’¡ NOTE: In a real implementation, this is where you would:\n- Send results to your database\n- Trigger notifications\n- Update external systems\n- Generate reports\n- etc.\n`;\n\n    // Log the output using Mastra logger\n    logger?.info(formattedOutput);\n    logger?.info(\"âœ… [Step 2] Results formatted and logged\");\n\n    return {\n      summary: `Successfully processed message with ${inputData.processedData?.original.length || 0} characters`,\n      formattedOutput,\n      success: true,\n    };\n  },\n});\n\n/**\n * MASTRA STEPS:\n * Remember:  There are many ways of structing a workflow, including boolean operations (conditionals, etc), looping, etc.\n * Please read the Mastra docs as instructed for more information if you need to setup a complex workflow. Mastra comes with many powerful primitives to help you build workflows.\n * We critically chain our steps in order to accomplish the user's desired automation.\n */\n\n/**\n * Create the workflow by chaining steps\n */\nexport const exampleWorkflow = createWorkflow({\n  id: \"example-workflow\",\n\n  // Define the initial input schema for the entire workflow\n  inputSchema: z.object({\n    message: z.string().describe(\"Message to process through the workflow\"),\n    includeAnalysis: z\n      .boolean()\n      .optional()\n      .describe(\"Whether to include detailed analysis\"),\n  }) as any, // TS workaround: Inngest type system incompatibility with Zod schemas\n\n  // Define the final output schema (should match the last step's output)\n  outputSchema: z.object({\n    summary: z.string(),\n    formattedOutput: z.string(),\n    success: z.boolean(),\n  }),\n})\n  // Chain your steps in order\n  .then(processWithAgent as any) // TS workaround: type inference issues with Inngest step chaining\n  .then(outputResults as any)\n  .commit();\n","path":null,"size_bytes":6422,"size_tokens":null},"docs/mastra/01-agents/30_example-ai-sdk-v5-integration.md":{"content":"---\ntitle: \"Example: AI SDK v5 Integration | Agents | Mastra Docs\"\ndescription: Example of integrating Mastra agents with AI SDK v5 for streaming chat interfaces with memory and tool integration.\n---\n\nimport { Callout } from \"nextra/components\";\nimport { GithubLink } from \"@/components/github-link\";\n\n# Example: AI SDK v5 Integration\n[EN] Source: https://mastra.ai/en/examples/agents/ai-sdk-v5-integration\n\nThis example demonstrates how to integrate Mastra agents with [AI SDK v5](https://sdk.vercel.ai/) to build modern streaming chat interfaces. It showcases a complete Next.js application with real-time conversation capabilities, persistent memory, and tool integration using the `stream` method with AI SDK v5 format support.\n\n## Key Features\n\n- **Streaming Chat Interface**: Uses AI SDK v5's `useChat` hook for real-time conversations\n- **Mastra Agent Integration**: Weather agent with custom tools and OpenAI GPT-4o\n- **Persistent Memory**: Conversation history stored with LibSQL\n- **Compatibility Layer**: Seamless integration between Mastra and AI SDK v5 streams\n- **Tool Integration**: Custom weather tool for real-time data fetching\n\n## Mastra Configuration\n\nFirst, set up your Mastra agent with memory and tools:\n\n```typescript showLineNumbers copy filename=\"src/mastra/index.ts\"\nimport { ConsoleLogger } from \"@mastra/core/logger\";\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { weatherAgent } from \"./agents\";\n\nexport const mastra = new Mastra({\n  agents: { weatherAgent },\n  logger: new ConsoleLogger(),\n  // aiSdkCompat: \"v4\", // Optional: for additional compatibility\n});\n```\n\n```typescript showLineNumbers copy filename=\"src/mastra/agents/index.ts\"\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { Memory } from \"@mastra/memory\";\nimport { LibSQLStore } from \"@mastra/libsql\";\nimport { weatherTool } from \"../tools\";\n\nexport const memory = new Memory({\n  storage: new LibSQLStore({\n    url: `file:./mastra.db`,\n  }),\n  options: {\n    semanticRecall: false,\n    workingMemory: {\n      enabled: false,\n    },\n    lastMessages: 5\n  },\n});\n\nexport const weatherAgent = new Agent({\n  name: \"Weather Agent\",\n  instructions: `\n    You are a helpful weather assistant that provides accurate weather information.\n\n    Your primary function is to help users get weather details for specific locations. When responding:\n    - Always ask for a location if none is provided\n    - Include relevant details like humidity, wind conditions, and precipitation\n    - Keep responses concise but informative\n\n    Use the weatherTool to fetch current weather data.\n  `,\n  model: openai(\"gpt-4o-mini\"),\n  tools: {\n    weatherTool,\n  },\n  memory,\n});\n```\n\n## Custom Weather Tool\n\nCreate a tool that fetches real-time weather data:\n\n```typescript showLineNumbers copy filename=\"src/mastra/tools/index.ts\"\nimport { createTool } from '@mastra/core/tools';\nimport { z } from 'zod';\n\nexport const weatherTool = createTool({\n  id: 'get-weather',\n  description: 'Get current weather for a location',\n  inputSchema: z.object({\n    location: z.string().describe('City name'),\n  }),\n  outputSchema: z.object({\n    temperature: z.number(),\n    feelsLike: z.number(),\n    humidity: z.number(),\n    windSpeed: z.number(),\n    windGust: z.number(),\n    conditions: z.string(),\n    location: z.string(),\n  }),\n  execute: async ({ context }) => {\n    return await getWeather(context.location);\n  },\n});\n\nconst getWeather = async (location: string) => {\n  // Geocoding API call\n  const geocodingUrl = `https://geocoding-api.open-meteo.com/v1/search?name=${encodeURIComponent(location)}&count=1`;\n  const geocodingResponse = await fetch(geocodingUrl);\n  const geocodingData = await geocodingResponse.json();\n\n  if (!geocodingData.results?.[0]) {\n    throw new Error(`Location '${location}' not found`);\n  }\n\n  const { latitude, longitude, name } = geocodingData.results[0];\n\n  // Weather API call\n  const weatherUrl = `https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=temperature_2m,apparent_temperature,relative_humidity_2m,wind_speed_10m,wind_gusts_10m,weather_code`;\n  const response = await fetch(weatherUrl);\n  const data = await response.json();\n\n  return {\n    temperature: data.current.temperature_2m,\n    feelsLike: data.current.apparent_temperature,\n    humidity: data.current.relative_humidity_2m,\n    windSpeed: data.current.wind_speed_10m,\n    windGust: data.current.wind_gusts_10m,\n    conditions: getWeatherCondition(data.current.weather_code),\n    location: name,\n  };\n};\n```\n\n## Next.js API Routes\n\n### Streaming Chat Endpoint\n\nCreate an API route that streams responses from your Mastra agent using the `stream` method with AI SDK v5 format:\n\n```typescript showLineNumbers copy filename=\"app/api/chat/route.ts\"\nimport { mastra } from \"@/src/mastra\";\n\nconst myAgent = mastra.getAgent(\"weatherAgent\");\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  // Use stream with AI SDK v5 format (experimental)\n  const stream = await myAgent.stream(messages, {\n    format: 'aisdk',  // Enable AI SDK v5 compatibility\n    memory: {\n      thread: \"user-session\", // Use actual user/session ID\n      resource: \"weather-chat\",\n    },\n  });\n\n  // Stream is already in AI SDK v5 format\n  return stream.toUIMessageStreamResponse();\n}\n```\n\n### Initial Chat History\n\nLoad conversation history from Mastra Memory:\n\n```typescript showLineNumbers copy filename=\"app/api/initial-chat/route.ts\"\nimport { mastra } from \"@/src/mastra\";\nimport { NextResponse } from \"next/server\";\nimport { convertMessages } from \"@mastra/core/agent\"\n\nconst myAgent = mastra.getAgent(\"weatherAgent\");\n\nexport async function GET() {\n  const result = await myAgent.getMemory()?.query({\n    threadId: \"user-session\",\n  });\n\n  const messages = convertMessages(result?.uiMessages || []).to('AIV5.UI');\n  return NextResponse.json(messages);\n}\n```\n\n## React Chat Interface\n\nBuild the frontend using AI SDK v5's `useChat` hook:\n\n```typescript showLineNumbers copy filename=\"app/page.tsx\"\n\"use client\";\n\nimport { Message, useChat } from \"@ai-sdk/react\";\nimport useSWR from \"swr\";\n\nconst fetcher = (url: string) => fetch(url).then((res) => res.json());\n\nexport default function Chat() {\n  // Load initial conversation history\n  const { data: initialMessages = [] } = useSWR<Message[]>(\n    \"/api/initial-chat\",\n    fetcher,\n  );\n\n  // Set up streaming chat with AI SDK v5\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    initialMessages,\n  });\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map((m) => (\n        <div\n          key={m.id}\n          className=\"whitespace-pre-wrap\"\n          style={{ marginTop: \"1em\" }}\n        >\n          <h3\n            style={{\n              fontWeight: \"bold\",\n              color: m.role === \"user\" ? \"green\" : \"yellow\",\n            }}\n          >\n            {m.role === \"user\" ? \"User: \" : \"AI: \"}\n          </h3>\n          {m.parts.map((p) => p.type === \"text\" && p.text).join(\"\\n\")}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Ask about the weather...\"\n          onChange={handleInputChange}\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n## Package Configuration\n\nInstall the required dependencies:\n\nNOTE: ai-sdk v5 is still in beta, while it is in beta you'll have to install the beta ai-sdk versions and the beta mastra versions. See [here](https://github.com/mastra-ai/mastra/issues/5470) for more information\n\n```json showLineNumbers copy filename=\"package.json\"\n{\n  \"dependencies\": {\n    \"@ai-sdk/openai\": \"2.0.0-beta.1\",\n    \"@ai-sdk/react\": \"2.0.0-beta.1\",\n    \"@mastra/core\": \"0.0.0-ai-v5-20250625173645\",\n    \"@mastra/libsql\": \"0.0.0-ai-v5-20250625173645\",\n    \"@mastra/memory\": \"0.0.0-ai-v5-20250625173645\",\n    \"next\": \"15.1.7\",\n    \"react\": \"^19.0.0\",\n    \"react-dom\": \"^19.0.0\",\n    \"swr\": \"^2.3.3\",\n    \"zod\": \"^3.25.67\"\n  }\n}\n```\n\n## Key Integration Points\n\n### Experimental stream Format Support\n\nThe experimental `stream` method with `format: 'aisdk'` provides native AI SDK v5 compatibility:\n\n```typescript\n// Use stream with AI SDK v5 format\nconst stream = await agent.stream(messages, {\n  format: 'aisdk'  // Returns AISDKV5OutputStream\n});\n\n// Direct compatibility with AI SDK v5 interfaces\nreturn stream.toUIMessageStreamResponse();\n```\n\n### Memory Persistence\n\nConversations are automatically persisted using Mastra Memory:\n\n- Each conversation uses a unique `threadId`\n- History is loaded on page refresh via `/api/initial-chat`\n- New messages are automatically stored by the agent\n\n### Tool Integration\n\nThe weather tool is seamlessly integrated:\n\n- Agent automatically calls the tool when weather information is needed\n- Real-time data is fetched from external APIs\n- Structured output ensures consistent responses\n\n## Running the Example\n\n1. Set your OpenAI API key:\n```bash\necho \"OPENAI_API_KEY=your_key_here\" > .env.local\n```\n\n2. Start the development server:\n```bash\npnpm dev\n```\n\n3. Visit `http://localhost:3000` and ask about weather in different cities!\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/tree/main/examples/ai-sdk-v5\"\n  }\n/>\n\n\n","path":null,"size_bytes":9498,"size_tokens":null},"docs/mastra/06-reference/56_memory-query.md":{"content":"---\ntitle: \"Reference: Memory.query() | Memory | Mastra Docs\"\ndescription: \"Documentation for the `Memory.query()` method in Mastra, which retrieves messages from a specific thread with support for pagination, filtering options, and semantic search.\"\n---\n\n# Memory.query()\n[EN] Source: https://mastra.ai/en/reference/memory/query\n\nthe `.query()` method retrieves messages from a specific thread, with support for pagination, filtering options, and semantic search.\n\n## Usage Example\n\n```typescript copy\nawait memory?.query({ threadId: \"user-123\" });\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"threadId\",\n      type: \"string\",\n      description: \"The unique identifier of the thread to retrieve messages from\",\n      isOptional: false,\n    },\n    {\n      name: \"resourceId\",\n      type: \"string\",\n      description: \"Optional ID of the resource that owns the thread. If provided, validates thread ownership\",\n      isOptional: true,\n    },\n    {\n      name: \"selectBy\",\n      type: \"object\",\n      description: \"Options for filtering and selecting messages\",\n      isOptional: true,\n    },\n    {\n      name: \"threadConfig\",\n      type: \"MemoryConfig\",\n      description: \"Configuration options for message retrieval and semantic search\",\n      isOptional: true,\n    },\n    {\n      name: \"format\",\n      type: \"'v1' | 'v2'\",\n      description: \"Message format to return. Defaults to 'v2' for current format, 'v1' for backwards compatibility\",\n      isOptional: true,\n    },\n  ]}\n/>\n\n### selectBy parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"vectorSearchString\",\n      type: \"string\",\n      description: \"Search string for finding semantically similar messages. Requires semantic recall to be enabled in threadConfig.\",\n      isOptional: true,\n    },\n    {\n      name: \"last\",\n      type: \"number | false\",\n      description: \"Number of most recent messages to retrieve. Set to false to disable limit. Note: threadConfig.lastMessages (default: 10) will override this if smaller.\",\n      isOptional: true,\n    },\n    {\n      name: \"include\",\n      type: \"{ id: string; threadId?: string; withPreviousMessages?: number; withNextMessages?: number }[]\",\n      description: \"Array of specific message IDs to include with optional context messages. Each item has an `id` (required), optional `threadId` (defaults to main threadId), `withPreviousMessages` (number of messages before, defaults to 2 for vector search, 0 otherwise), and `withNextMessages` (number of messages after, defaults to 2 for vector search, 0 otherwise).\",\n      isOptional: true,\n    },\n    {\n      name: \"pagination\",\n      type: \"{ dateRange?: { start?: Date; end?: Date }; page?: number; perPage?: number }\",\n      description: \"Pagination options for retrieving messages in batches. Includes `dateRange` (filter by date range), `page` (0-based page number), and `perPage` (messages per page).\",\n      isOptional: true,\n    },\n  ]}\n/>\n\n### threadConfig parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"lastMessages\",\n      type: \"number | false\",\n      description: \"Number of most recent messages to retrieve. Set to false to disable.\",\n      isOptional: true,\n      defaultValue: \"10\",\n    },\n    {\n      name: \"semanticRecall\",\n      type: \"boolean | { topK: number; messageRange: number | { before: number; after: number }; scope?: 'thread' | 'resource' }\",\n      description: \"Enable semantic search in message history. Can be a boolean or an object with configuration options. When enabled, requires both vector store and embedder to be configured.\",\n      isOptional: true,\n      defaultValue: \"false\",\n    },\n    {\n      name: \"workingMemory\",\n      type: \"WorkingMemory\",\n      description: \"Configuration for working memory feature. Can be `{ enabled: boolean; template?: string; schema?: ZodObject<any> | JSONSchema7; scope?: 'thread' | 'resource' }` or `{ enabled: boolean }` to disable.\",\n      isOptional: true,\n      defaultValue: \"{ enabled: false, template: '# User Information\\\\n- **First Name**:\\\\n- **Last Name**:\\\\n...' }\",\n    },\n    {\n      name: \"threads\",\n      type: \"{ generateTitle?: boolean | { model: DynamicArgument<MastraLanguageModel>; instructions?: DynamicArgument<string> } }\",\n      description: \"Settings related to memory thread creation. `generateTitle` controls automatic thread title generation from the user's first message. Can be a boolean or an object with custom model and instructions.\",\n      isOptional: true,\n      defaultValue: \"{ generateTitle: false }\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"messages\",\n      type: \"CoreMessage[]\",\n      description: \"Array of retrieved messages in their core format\",\n    },\n    {\n      name: \"uiMessages\",\n      type: \"UIMessageWithMetadata[]\",\n      description: \"Array of messages formatted for UI display, including proper threading of tool calls and results\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript filename=\"src/test-memory.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst agent = mastra.getAgent(\"agent\");\nconst memory = await agent.getMemory();\n\nconst { messages, uiMessages } = await memory!.query({\n  threadId: \"thread-123\",\n  selectBy: {\n    last: 50,\n    vectorSearchString: \"What messages are there?\",\n    include: [\n      {\n        id: \"msg-123\"\n      },\n      {\n        id: \"msg-456\",\n        withPreviousMessages: 3,\n        withNextMessages: 1\n      }\n    ]\n  },\n  threadConfig: {\n    semanticRecall: true\n  }\n});\n\nconsole.log(messages);\nconsole.log(uiMessages);\n```\n\n### Related\n\n- [Memory Class Reference](/reference/memory/Memory.mdx)\n- [Getting Started with Memory](/docs/memory/overview.mdx)\n- [Semantic Recall](/docs/memory/semantic-recall.mdx)\n- [createThread](/reference/memory/createThread.mdx)\n\n\n","path":null,"size_bytes":5808,"size_tokens":null},"docs/mastra/06-reference/101_workflow-sleep-until.md":{"content":"---\ntitle: \"Reference: Workflow.sleepUntil() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow.sleepUntil()` method in workflows, which pauses execution until a specified date.\n---\n\n# Workflow.sleepUntil()\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow-methods/sleepUntil\n\nThe `.sleepUntil()` method pauses execution until a specified date.\n\n## Usage example\n\n```typescript copy\nworkflow.sleepUntil(new Date(Date.now() + 5000));\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"dateOrCallback\",\n      type: \"Date | ((params: ExecuteFunctionParams) => Promise<Date>)\",\n      description: \"Either a Date object or a callback function that returns a Date. The callback receives execution context and can compute the target time dynamically based on input data.\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"Workflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\n\nconst step1 = createStep({...});\nconst step2 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .sleepUntil(async ({ inputData }) => {\n    const { delayInMs } = inputData;\n    return new Date(Date.now() + delayInMs);\n  })\n  .then(step2)\n  .commit();\n```\n\n## Related\n\n- [Suspend & Resume](../../../docs/workflows/suspend-and-resume.mdx#sleep--events)\n\n\n","path":null,"size_bytes":1557,"size_tokens":null},"docs/mastra/06-reference/13_agent-network.md":{"content":"---\ntitle: \"Reference: Agent.network() (Experimental) | Agents | Mastra Docs\"\ndescription: \"Documentation for the `Agent.network()` method in Mastra agents, which enables multi-agent collaboration and routing.\"\n---\n\nimport { NetworkCallout } from \"@/components/network-callout.tsx\"\n\n# Agent.network()\n[EN] Source: https://mastra.ai/en/reference/agents/network\n\n<NetworkCallout />\n\nThe `.network()` method enables multi-agent collaboration and routing. This method accepts messages and optional execution options.\n\n## Usage example\n\n```typescript copy\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { agent1, agent2 } from './agents';\nimport { workflow1 } from './workflows';\nimport { tool1, tool2 } from './tools';\n\nconst agent = new Agent({\n  name: 'network-agent',\n  instructions: 'You are a network agent that can help users with a variety of tasks.',\n  model: openai('gpt-4o'),\n  agents: {\n    agent1,\n    agent2,\n  },\n  workflows: {\n    workflow1,\n  },\n  tools: {\n    tool1,\n    tool2,\n  },\n})\n\nawait agent.network(`\n  Find me the weather in Tokyo. \n  Based on the weather, plan an activity for me.\n`);\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"messages\",\n      type: \"string | string[] | CoreMessage[] | AiMessageType[] | UIMessageWithMetadata[]\",\n      description: \"The messages to send to the agent. Can be a single string, array of strings, or structured message objects.\",\n    },\n    {\n      name: \"options\",\n      type: \"MultiPrimitiveExecutionOptions\",\n      isOptional: true,\n      description: \"Optional configuration for the network process.\",\n    },\n  ]}\n/>\n\n### Options\n\n<PropertiesTable\n  content={[\n    {\n      name: \"maxSteps\",\n      type: \"number\",\n      isOptional: true,\n      description: \"Maximum number of steps to run during execution.\",\n    },\n    {\n      name: \"memory\",\n      type: \"object\",\n      isOptional: true,\n      description: \"Configuration for memory. This is the preferred way to manage memory.\",\n      properties: [\n        {\n          parameters: [{\n              name: \"thread\",\n              type: \"string | { id: string; metadata?: Record<string, any>, title?: string }\",\n              isOptional: false,\n              description: \"The conversation thread, as a string ID or an object with an `id` and optional `metadata`.\"\n          }]\n        },\n        {\n          parameters: [{\n              name: \"resource\",\n              type: \"string\",\n              isOptional: false,\n              description: \"Identifier for the user or resource associated with the thread.\"\n          }]\n        },\n        {\n          parameters: [{\n              name: \"options\",\n              type: \"MemoryConfig\",\n              isOptional: true,\n              description: \"Configuration for memory behavior, like message history and semantic recall.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"tracingContext\",\n      type: \"TracingContext\",\n      isOptional: true,\n      description: \"AI tracing context for creating child spans and adding metadata. Automatically injected when using Mastra's tracing system.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"currentSpan\",\n            type: \"AISpan\",\n            isOptional: true,\n            description: \"Current AI span for creating child spans and adding metadata. Use this to create custom child spans or update span attributes during execution.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"tracingOptions\",\n      type: \"TracingOptions\",\n      isOptional: true,\n      description: \"Options for AI tracing configuration.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"metadata\",\n            type: \"Record<string, any>\",\n            isOptional: true,\n            description: \"Metadata to add to the root trace span. Useful for adding custom attributes like user IDs, session IDs, or feature flags.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"telemetry\",\n      type: \"TelemetrySettings\",\n      isOptional: true,\n      description:\n        \"Settings for OTLP telemetry collection during streaming (not AI tracing).\",\n      properties: [\n        {\n          parameters: [{\n            name: \"isEnabled\",\n            type: \"boolean\",\n            isOptional: true,\n            description: \"Enable or disable telemetry. Disabled by default while experimental.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"recordInputs\",\n            type: \"boolean\",\n            isOptional: true,\n            description: \"Enable or disable input recording. Enabled by default. You might want to disable input recording to avoid recording sensitive information.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"recordOutputs\",\n            type: \"boolean\",\n            isOptional: true,\n            description: \"Enable or disable output recording. Enabled by default. You might want to disable output recording to avoid recording sensitive information.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"functionId\",\n            type: \"string\",\n            isOptional: true,\n            description: \"Identifier for this function. Used to group telemetry data by function.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"modelSettings\",\n      type: \"CallSettings\",\n      isOptional: true,\n      description:\n        \"Model-specific settings like temperature, maxTokens, topP, etc. These are passed to the underlying language model.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"temperature\",\n            type: \"number\",\n            isOptional: true,\n            description: \"Controls randomness in the model's output. Higher values (e.g., 0.8) make the output more random, lower values (e.g., 0.2) make it more focused and deterministic.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"maxRetries\",\n            type: \"number\",\n            isOptional: true,\n            description: \"Maximum number of retries for failed requests.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"topP\",\n            type: \"number\",\n            isOptional: true,\n            description: \"Nucleus sampling. This is a number between 0 and 1. It is recommended to set either temperature or topP, but not both.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"topK\",\n            type: \"number\",\n            isOptional: true,\n            description: \"Only sample from the top K options for each subsequent token. Used to remove 'long tail' low probability responses.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"presencePenalty\",\n            type: \"number\",\n            isOptional: true,\n            description: \"Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. A number between -1 (increase repetition) and 1 (maximum penalty, decrease repetition).\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"frequencyPenalty\",\n            type: \"number\",\n            isOptional: true,\n            description: \"Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. A number between -1 (increase repetition) and 1 (maximum penalty, decrease repetition).\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"stopSequences\",\n            type: \"string[]\",\n            isOptional: true,\n            description: \"Stop sequences. If set, the model will stop generating text when one of the stop sequences is generated.\"\n          }]\n        },\n      ]\n    },\n    {\n      name: \"runId\",\n      type: \"string\",\n      isOptional: true,\n      description: \"Unique ID for this generation run. Useful for tracking and debugging purposes.\",\n    },\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      isOptional: true,\n      description: \"Runtime context for dependency injection and contextual information.\",\n    },\n    {\n      name: \"traceId\",\n      type: \"string\",\n      isOptional: true,\n      description: \"The trace ID associated with this execution when AI tracing is enabled. Use this to correlate logs and debug execution flow.\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"stream\",\n      type: \"MastraAgentNetworkStream<NetworkChunkType>\",\n      description: \"A custom stream that extends ReadableStream<NetworkChunkType> with additional network-specific properties\",\n    },\n    {\n      name: \"status\",\n      type: \"Promise<RunStatus>\",\n      description: \"A promise that resolves to the current workflow run status\",\n    },\n    {\n      name: \"result\",\n      type: \"Promise<WorkflowResult<TState, TOutput, TSteps>>\",\n      description: \"A promise that resolves to the final workflow result\",\n    },\n    {\n      name: \"usage\",\n      type: \"Promise<{ promptTokens: number; completionTokens: number; totalTokens: number }>\",\n      description: \"A promise that resolves to token usage statistics\",\n    },\n  ]}\n/>\n\n\n","path":null,"size_bytes":9200,"size_tokens":null},"docs/mastra/06-reference/105_workflow-if.md":{"content":"---\ntitle: \"Reference: Workflow.if() | Conditional Branching | Mastra Docs\"\ndescription: \"Documentation for the `.if()` method in Mastra workflows, which creates conditional branches based on specified conditions.\"\n---\n\n# Workflow.if()\n[EN] Source: https://mastra.ai/en/reference/legacyWorkflows/if\n\n> Experimental\n\nThe `.if()` method creates a conditional branch in the workflow, allowing steps to execute only when a specified condition is true. This enables dynamic workflow paths based on the results of previous steps.\n\n## Usage\n\n```typescript copy showLineNumbers\nworkflow\n  .step(startStep)\n  .if(async ({ context }) => {\n    const value = context.getStepResult<{ value: number }>(\"start\")?.value;\n    return value < 10; // If true, execute the \"if\" branch\n  })\n  .then(ifBranchStep)\n  .else()\n  .then(elseBranchStep)\n  .commit();\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"condition\",\n      type: \"Function | ReferenceCondition\",\n      description:\n        \"A function or reference condition that determines whether to execute the 'if' branch\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Condition Types\n\n### Function Condition\n\nYou can use a function that returns a boolean:\n\n```typescript\nworkflow\n  .step(startStep)\n  .if(async ({ context }) => {\n    const result = context.getStepResult<{ status: string }>(\"start\");\n    return result?.status === \"success\"; // Execute \"if\" branch when status is \"success\"\n  })\n  .then(successStep)\n  .else()\n  .then(failureStep);\n```\n\n### Reference Condition\n\nYou can use a reference-based condition with comparison operators:\n\n```typescript\nworkflow\n  .step(startStep)\n  .if({\n    ref: { step: startStep, path: \"value\" },\n    query: { $lt: 10 }, // Execute \"if\" branch when value is less than 10\n  })\n  .then(ifBranchStep)\n  .else()\n  .then(elseBranchStep);\n```\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"LegacyWorkflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Error Handling\n\nThe `if` method requires a previous step to be defined. If you try to use it without a preceding step, an error will be thrown:\n\n```typescript\ntry {\n  // This will throw an error\n  workflow\n    .if(async ({ context }) => true)\n    .then(someStep)\n    .commit();\n} catch (error) {\n  console.error(error); // \"Condition requires a step to be executed after\"\n}\n```\n\n## Related\n\n- [else Reference](./else.mdx)\n- [then Reference](./then.mdx)\n- [Control Flow Guide](../../docs/workflows-legacy/control-flow.mdx)\n- [Step Condition Reference](./step-condition.mdx)\n\n\n","path":null,"size_bytes":2597,"size_tokens":null},"tests/testCronAutomation.ts":{"content":"/**\n * Test Script for Time-Based (Cron) Workflows\n *\n * This script demonstrates the CORRECT way to test time-based workflows\n * that use Inngest for orchestration. It simulates a cron schedule firing.\n *\n * WHAT THIS TESTS:\n * - Manual trigger of cron workflow (replit/cron.trigger event)\n * - Workflow execution without waiting for schedule\n * - Complete Inngest step-by-step orchestration\n * - Fast feedback loop for cron automation development\n *\n * PREREQUISITES:\n * 1. Start your Mastra server: npm start (runs on localhost:5000)\n * 2. Start Inngest dev server: inngest dev -u http://localhost:5000/api/inngest --port 3000\n *\n * HOW TO RUN:\n * npx tsx tests/testCronAutomation.ts\n *\n * VERIFICATION:\n * - Check console output for success messages\n * - Visit http://localhost:3000 to see execution in Inngest dashboard\n */\n\nimport { inngest } from \"../src/mastra/inngest/client\";\n\n// ============================================================================\n// TEST FUNCTION\n// ============================================================================\n\nasync function testCronTrigger() {\n  console.log(`\\n${\"=\".repeat(70)}`);\n  console.log(`ðŸš€ CRON AUTOMATION TEST`);\n  console.log(`${\"=\".repeat(70)}`);\n  console.log(`Trigger Type: Time-Based (Cron)\\n`);\n\n  try {\n    /**\n     * Send an Inngest event that simulates a cron schedule firing.\n     *\n     * In production, the flow is:\n     * 1. Inngest Cloud evaluates cron expression and fires at scheduled time\n     * 2. Inngest Cloud â†’ triggers the cron function (id: \"cron-trigger\")\n     * 3. Cron function â†’ starts workflow with empty inputData\n     * 4. Workflow â†’ orchestrated by Inngest step-by-step\n     *\n     * This test simulates step 1-2, allowing you to test your cron workflow\n     * immediately without waiting for the actual schedule.\n     *\n     * Note: The cron function also listens to this manual trigger event,\n     * so you can test at any time during development.\n     */\n    await inngest.send({\n      // Event name that cron triggers listen for (see registerCronWorkflow)\n      name: \"replit/cron.trigger\",\n\n      // Cron workflows don't receive external input, so data is empty\n      data: {},\n    });\n\n    console.log(`âœ… Cron trigger event sent successfully!`);\n    console.log(`\\nðŸ“Š Check execution at: http://localhost:3000`);\n    console.log(`   - Functions tab: Look for \"cron-trigger\"`);\n    console.log(`   - Runs tab: See the complete execution trace`);\n    console.log(\n      `\\nðŸ’¡ TIP: Your workflow will execute immediately, no need to wait for the schedule!\\n`,\n    );\n  } catch (error) {\n    console.error(\"âŒ Error sending Inngest event:\", error);\n    console.error(\"\\nTroubleshooting:\");\n    console.error(\"  1. Is your Mastra server running? (npm start)\");\n    console.error(\n      \"  2. Is Inngest dev server running? (inngest dev -u http://localhost:5000/api/inngest --port 3000)\",\n    );\n    console.error(\n      \"  3. Is registerCronTrigger called in src/mastra/index.ts?\\n\",\n    );\n    process.exit(1);\n  }\n}\n\n// Run the test\ntestCronTrigger();\n","path":null,"size_bytes":3068,"size_tokens":null},"docs/mastra/06-reference/39_mastra-set-logger.md":{"content":"---\ntitle: \"Reference: Mastra.setLogger() | Core | Mastra Docs\"\ndescription: \"Documentation for the `Mastra.setLogger()` method in Mastra, which sets the logger for all components (agents, workflows, etc.).\"\n---\n\n# Mastra.setLogger()\n[EN] Source: https://mastra.ai/en/reference/core/setLogger\n\nThe `.setLogger()` method is used to set the logger for all components (agents, workflows, etc.) in the Mastra instance. This method accepts a single object parameter with a logger property.\n\n## Usage example\n\n```typescript copy\nmastra.setLogger({ logger: new PinoLogger({ name: \"testLogger\" }) });\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"options\",\n      type: \"{ logger: TLogger }\",\n      description: \"An object containing the logger instance to set for all components.\",\n    },\n  ]}\n/>\n\n### Options\n\n<PropertiesTable\n  content={[\n    {\n      name: \"logger\",\n      type: \"TLogger\",\n      description: \"The logger instance to set for all components (agents, workflows, etc.).\",\n    },\n  ]}\n/>\n\n## Returns\n\nThis method does not return a value.\n\n## Related\n\n- [Logging overview](../../docs/observability/logging.mdx)\n- [Logger reference](../../reference/observability/logger.mdx)\n\n\n","path":null,"size_bytes":1204,"size_tokens":null},"docs/mastra/06-reference/04_agent-get-description.md":{"content":"---\ntitle: \"Reference: Agent.getDescription() | Agents | Mastra Docs\"\ndescription: \"Documentation for the `Agent.getDescription()` method in Mastra agents, which retrieves the agent's description.\"\n---\n\n# Agent.getDescription()\n[EN] Source: https://mastra.ai/en/reference/agents/getDescription\n\nThe `.getDescription()` method retrieves the description configured for an agent. This method returns a simple string description that describes the agent's purpose and capabilities.\n\n## Usage example\n\n```typescript copy\nagent.getDescription();\n```\n\n## Parameters\n\nThis method takes no parameters.\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"description\",\n      type: \"string\",\n      description: \"The description of the agent, or an empty string if no description was configured.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Agents overview](../../docs/agents/overview.mdx)\n\n\n","path":null,"size_bytes":878,"size_tokens":null},"src/mastra/tools/exampleTool.ts":{"content":"import { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\n/**\n * Example Mastra Tool\n *\n * MASTRA TOOL GUIDE:\n * - Tools are reusable functions that can be used by agents and workflows\n * - Use createTool() to define a tool with typed inputs/outputs\n * - Tools should be focused on a single task\n * - Always include clear descriptions for the tool and its parameters\n */\n\n// Define the input and output schemas using Zod\n// This provides type safety and validation\nexport const exampleTool = createTool({\n  id: \"example-tool\",\n\n  // Describe what your tool does - this helps agents understand when to use it\n  description:\n    \"A simple example tool that demonstrates how to create Mastra tools\",\n\n  // Define what inputs your tool expects\n  // Use .describe() to add helpful descriptions for each field\n  inputSchema: z.object({\n    message: z.string().describe(\"A message to process\"),\n    count: z.number().optional().describe(\"Optional number parameter\"),\n  }),\n\n  // Define what your tool will return\n  outputSchema: z.object({\n    processed: z.string(),\n    timestamp: z.string(),\n    metadata: z.object({\n      characterCount: z.number(),\n      wordCount: z.number(),\n    }),\n  }),\n\n  // The execute function contains your tool's logic\n  // Access mastra for logging and other utilities\n  execute: async ({ context, mastra }) => {\n    const logger = mastra?.getLogger();\n\n    // Use logger instead of console.log for proper observability\n    logger?.info(\"ðŸ”§ [exampleTool] Executing with:\", context);\n\n    // In a real tool, you might:\n    // 1. Call external APIs\n    // 2. Process data\n    // 3. Interact with databases\n    // 4. Transform information\n\n    // For this example, we'll do some trivial data processing\n    const processedMessage = context.message.toUpperCase();\n    const words = context.message.split(\" \").filter((w) => w.length > 0);\n\n    logger?.info(\"âœ… [exampleTool] Processing complete\");\n\n    // Return data matching the output schema\n    return {\n      processed: processedMessage,\n      timestamp: new Date().toISOString(),\n      metadata: {\n        characterCount: context.message.length,\n        wordCount: words.length,\n      },\n    };\n  },\n});\n","path":null,"size_bytes":2200,"size_tokens":null},"docs/mastra/06-reference/06_agent-get-llm.md":{"content":"---\ntitle: \"Reference: Agent.getLLM() | Agents | Mastra Docs\"\ndescription: \"Documentation for the `Agent.getLLM()` method in Mastra agents, which retrieves the language model instance.\"\n---\n\n# Agent.getLLM()\n[EN] Source: https://mastra.ai/en/reference/agents/getLLM\n\nThe `.getLLM()` method retrieves the language model instance configured for an agent, resolving it if it's a function. This method provides access to the underlying LLM that powers the agent's capabilities.\n\n## Usage example\n\n```typescript copy\nawait agent.getLLM();\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"options\",\n      type: \"{ runtimeContext?: RuntimeContext; model?: MastraLanguageModel | DynamicArgument<MastraLanguageModel> }\",\n      isOptional: true,\n      defaultValue: \"{}\",\n      description: \"Optional configuration object containing runtime context and optional model override.\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"llm\",\n      type: \"MastraLLMV1 | Promise<MastraLLMV1>\",\n      description: \"The language model instance configured for the agent, either as a direct instance or a promise that resolves to the LLM.\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript copy\nawait agent.getLLM({\n  runtimeContext: new RuntimeContext(),\n  model: openai('gpt-4')\n});\n```\n\n### Options parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      isOptional: true,\n      defaultValue: \"new RuntimeContext()\",\n      description: \"Runtime context for dependency injection and contextual information.\",\n    },\n    {\n      name: \"model\",\n      type: \"MastraLanguageModel | DynamicArgument<MastraLanguageModel>\",\n      isOptional: true,\n      description: \"Optional model override. If provided, this model will be used used instead of the agent's configured model.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Agents overview](../../docs/agents/overview.mdx)\n- [Runtime Context](../../docs/server-db/runtime-context.mdx)\n\n\n","path":null,"size_bytes":2008,"size_tokens":null},"docs/mastra/06-reference/67_processor-unicode-normalizer.md":{"content":"---\ntitle: \"Reference: Unicode Normalizer | Processors | Mastra Docs\"\ndescription: \"Documentation for the UnicodeNormalizer in Mastra, which normalizes Unicode text to ensure consistent formatting and remove potentially problematic characters.\"\n---\n\n# UnicodeNormalizer\n[EN] Source: https://mastra.ai/en/reference/processors/unicode-normalizer\n\nThe `UnicodeNormalizer` is an **input processor** that normalizes Unicode text to ensure consistent formatting and remove potentially problematic characters before messages are sent to the language model. This processor helps maintain text quality by handling various Unicode representations, removing control characters, and standardizing whitespace formatting.\n\n## Usage example\n\n```typescript copy\nimport { UnicodeNormalizer } from \"@mastra/core/processors\";\n\nconst processor = new UnicodeNormalizer({\n  stripControlChars: true,\n  collapseWhitespace: true\n});\n```\n\n## Constructor parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"options\",\n      type: \"Options\",\n      description: \"Configuration options for Unicode text normalization\",\n      isOptional: true,\n    },\n  ]}\n/>\n\n### Options\n\n<PropertiesTable\n  content={[\n    {\n      name: \"stripControlChars\",\n      type: \"boolean\",\n      description: \"Whether to strip control characters. When enabled, removes control characters except \\t, \\n, \\r\",\n      isOptional: true,\n      default: \"false\",\n    },\n    {\n      name: \"preserveEmojis\",\n      type: \"boolean\",\n      description: \"Whether to preserve emojis. When disabled, emojis may be removed if they contain control characters\",\n      isOptional: true,\n      default: \"true\",\n    },\n    {\n      name: \"collapseWhitespace\",\n      type: \"boolean\",\n      description: \"Whether to collapse consecutive whitespace. When enabled, multiple spaces/tabs/newlines are collapsed to single instances\",\n      isOptional: true,\n      default: \"true\",\n    },\n    {\n      name: \"trim\",\n      type: \"boolean\",\n      description: \"Whether to trim leading and trailing whitespace\",\n      isOptional: true,\n      default: \"true\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"name\",\n      type: \"string\",\n      description: \"Processor name set to 'unicode-normalizer'\",\n      isOptional: false,\n    },\n    {\n      name: \"processInput\",\n      type: \"(args: { messages: MastraMessageV2[]; abort: (reason?: string) => never }) => MastraMessageV2[]\",\n      description: \"Processes input messages to normalize Unicode text\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n\n## Extended usage example\n\n```typescript filename=\"src/mastra/agents/normalized-agent.ts\" showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { UnicodeNormalizer } from \"@mastra/core/processors\";\n\nexport const agent = new Agent({\n  name: \"normalized-agent\",\n  instructions: \"You are a helpful assistant\",\n  model: openai(\"gpt-4o-mini\"),\n  inputProcessors: [\n    new UnicodeNormalizer({\n      stripControlChars: true,\n      preserveEmojis: true,\n      collapseWhitespace: true,\n      trim: true\n    })\n  ]\n});\n```\n\n\n## Related\n\n- [Input Processors](../../docs/agents/input-processors.mdx)\n\n\n","path":null,"size_bytes":3191,"size_tokens":null},"tests/testWebhookAutomation.ts":{"content":"/**\n * Test Script for Webhook-Triggered Workflows\n *\n * This script demonstrates the CORRECT way to test webhook-triggered workflows\n * that use Inngest for orchestration. It simulates the complete production flow.\n *\n * WHAT THIS TESTS:\n * - Inngest event routing (event/api.webhooks.{provider}.action)\n *   Note: Slack/Telegram use event/api.webhooks.webhooks.action due to /webhooks/{provider}/action path\n * - The forwarding function created by registerApiRoute\n * - HTTP forwarding to your webhook handler (e.g., /linear/webhook)\n * - Webhook payload validation in your handler\n * - Workflow triggering via workflow.start()\n * - Complete Inngest step-by-step orchestration\n *\n * PREREQUISITES:\n * 1. Start your Mastra server with restart_workflow tool\n * 2. Start Inngest dev server with restart_workflow tool\n *\n * HOW TO RUN:\n * npx tsx tests/testWebhookAutomation.ts\n *\n * VERIFICATION:\n * - Check console output for success messages\n * - Visit http://localhost:3000 to see execution in Inngest dashboard\n */\n\nimport { inngest } from \"../src/mastra/inngest/client\";\n\n// ============================================================================\n// CONFIGURATION - Update these based on your webhook automation\n// ============================================================================\n\n// Change this to match your connector name\nconst PROVIDER: string = \"linear\"; // e.g., \"linear\", \"github\", etc\n\n// Mock webhook payload - This **must** match your connector's webhook schema.\n// Keep data obviously fake (use \"MOCK:\" prefixes, fake IDs like \"mock-123\")\nconst mockWebhookPayload = {\n  action: \"create\", // Linear webhook action (create/update/remove)\n  type: \"Issue\", // Linear webhook type (Issue/Comment/Project)\n  data: {\n    id: \"mock-issue-999\",\n    title: \"MOCK: Test Issue for Validation\",\n    description: \"MOCK: This is test data to validate the webhook flow\",\n    number: 999,\n    priority: 1,\n    createdAt: \"2025-01-01T00:00:00Z\",\n    updatedAt: \"2025-01-01T00:00:00Z\",\n  },\n  createdAt: \"2025-01-01T00:00:00Z\",\n  organizationId: \"mock-org-123\",\n};\n\n// ============================================================================\n// TEST FUNCTION\n// ============================================================================\n\nasync function testWebhookTrigger() {\n  console.log(`\\n${\"=\".repeat(70)}`);\n  console.log(`ðŸš€ WEBHOOK AUTOMATION TEST`);\n  console.log(`${\"=\".repeat(70)}`);\n  console.log(`Provider: ${PROVIDER}\\n`);\n\n  try {\n    /**\n     * Send an Inngest event that simulates what Replit Webhook Service sends in production.\n     *\n     * In production, the flow is:\n     * 1. External webhook â†’ Replit Webhook Service\n     * 2. Replit transforms it â†’ Inngest Cloud (sends event/api.webhooks.{provider}.action)\n     * 3. Inngest Cloud â†’ triggers the forwarding function (id: \"api-{provider}\")\n     * 4. Forwarding function â†’ POSTs to your webhook handler (/{provider}/webhook)\n     * 5. Webhook handler â†’ validates payload and starts workflow\n     * 6. Workflow â†’ orchestrated by Inngest step-by-step\n     *\n     * This test simulates step 2, exercising the complete flow from there.\n     */\n\n    const eventName = `event/api.webhooks.${PROVIDER}.action`;\n\n    await inngest.send({\n      // Event name must match what registerApiRoute creates an Inngest function to listen for\n      name: eventName,\n\n      // Data structure matches what Replit Webhook Service sends to Inngest\n      data: {\n        method: \"POST\",\n        headers: { \"content-type\": \"application/json\" },\n        body: JSON.stringify(mockWebhookPayload),\n      },\n    });\n\n    console.log(`âœ… Event sent successfully for ${PROVIDER}!`);\n    console.log(`\\nðŸ“Š Check execution at: http://localhost:3000`);\n    console.log(\n      `   - Functions tab: Look for \"api-${PROVIDER}\" (the forwarding function)`,\n    );\n    console.log(`   - Runs tab: See the complete execution trace\\n`);\n  } catch (error) {\n    console.error(\"âŒ Error sending Inngest event:\", error);\n    console.error(\"\\nTroubleshooting:\");\n    console.error(\"  1. Is your Mastra server running? (npm start)\");\n    console.error(\n      \"  2. Is Inngest dev server running? (inngest dev -u http://localhost:5000/api/inngest --port 3000)\",\n    );\n    console.error(\n      \"  3. Is the webhook handler registered in src/mastra/index.ts?\\n\",\n    );\n    process.exit(1);\n  }\n}\n\n// Run the test\ntestWebhookTrigger();\n","path":null,"size_bytes":4399,"size_tokens":null},"docs/mastra/03-workflows/07_snapshots.md":{"content":"---\ntitle: \"Snapshots | Mastra Docs\"\ndescription: \"Learn how to save and resume workflow execution state with snapshots in Mastra\"\n---\n\n# Snapshots\n[EN] Source: https://mastra.ai/en/docs/workflows/snapshots\n\nIn Mastra, a snapshot is a serializable representation of a workflow's complete execution state at a specific point in time. Snapshots capture all the information needed to resume a workflow from exactly where it left off, including:\n\n- The current state of each step in the workflow\n- The outputs of completed steps\n- The execution path taken through the workflow\n- Any suspended steps and their metadata\n- The remaining retry attempts for each step\n- Additional contextual data needed to resume execution\n\nSnapshots are automatically created and managed by Mastra whenever a workflow is suspended, and are persisted to the configured storage system.\n\n## The role of snapshots in suspend and resume\n\nSnapshots are the key mechanism enabling Mastra's suspend and resume capabilities. When a workflow step calls `await suspend()`:\n\n1. The workflow execution is paused at that exact point\n2. The current state of the workflow is captured as a snapshot\n3. The snapshot is persisted to storage\n4. The workflow step is marked as \"suspended\" with a status of `'suspended'`\n5. Later, when `resume()` is called on the suspended step, the snapshot is retrieved\n6. The workflow execution resumes from exactly where it left off\n\nThis mechanism provides a powerful way to implement human-in-the-loop workflows, handle rate limiting, wait for external resources, and implement complex branching workflows that may need to pause for extended periods.\n\n## Snapshot anatomy\n\nEach snapshot includes the `runId`, input, step status (`success`, `suspended`, etc.), any suspend and resume payloads, and the final output. This ensures full context is available when resuming execution.\n\n\n```json\n{\n  \"runId\": \"34904c14-e79e-4a12-9804-9655d4616c50\",\n  \"status\": \"success\",\n  \"value\": {},\n  \"context\": {\n    \"input\": { \"value\": 100, \"user\": \"Michael\", \"requiredApprovers\": [\"manager\", \"finance\"] },\n    \"approval-step\": {\n      \"payload\": { \"value\": 100, \"user\": \"Michael\", \"requiredApprovers\": [\"manager\", \"finance\"] },\n      \"startedAt\": 1758027577955,\n      \"status\": \"success\",\n      \"suspendPayload\": { \"message\": \"Workflow suspended\", \"requestedBy\": \"Michael\", \"approvers\": [\"manager\", \"finance\"] },\n      \"suspendedAt\": 1758027578065,\n      \"resumePayload\": { \"confirm\": true, \"approver\": \"manager\" },\n      \"resumedAt\": 1758027578517,\n      \"output\": { \"value\": 100, \"approved\": true },\n      \"endedAt\": 1758027578634\n    }\n  },\n  \"activePaths\": [],\n  \"serializedStepGraph\": [{ \"type\": \"step\", \"step\": { \"id\": \"approval-step\", \"description\": \"Accepts a value, waits for confirmation\" } }],\n  \"suspendedPaths\": {},\n  \"waitingPaths\": {},\n  \"result\": { \"value\": 100, \"approved\": true },\n  \"runtimeContext\": {},\n  \"timestamp\": 1758027578740\n}\n```\n\n## How snapshots are saved and retrieved\n\nSnapshots are saved to the configured storage system. By default, they use LibSQL, but you can configure Upstash or PostgreSQL instead. Each snapshot is saved in the `workflow_snapshots` table and identified by the workflowâ€™s `runId`.\n\nRead more about:\n- [LibSQL Storage](../../reference/storage/libsql.mdx)\n- [Upstash Storage](../../reference/storage/upstash.mdx)\n- [PostgreSQL Storage](../../reference/storage/postgresql.mdx)\n\n\n### Saving snapshots\n\nWhen a workflow is suspended, Mastra automatically persists the workflow snapshot with these steps:\n\n1. The `suspend()` function in a step execution triggers the snapshot process\n2. The `WorkflowInstance.suspend()` method records the suspended machine\n3. `persistWorkflowSnapshot()` is called to save the current state\n4. The snapshot is serialized and stored in the configured database in the `workflow_snapshots` table\n5. The storage record includes the workflow name, run ID, and the serialized snapshot\n\n### Retrieving snapshots\n\nWhen a workflow is resumed, Mastra retrieves the persisted snapshot with these steps:\n\n1. The `resume()` method is called with a specific step ID\n2. The snapshot is loaded from storage using `loadWorkflowSnapshot()`\n3. The snapshot is parsed and prepared for resumption\n4. The workflow execution is recreated with the snapshot state\n5. The suspended step is resumed, and execution continues\n\n\n```typescript\nconst storage = mastra.getStorage();\n\nconst snapshot = await storage!.loadWorkflowSnapshot({\n  runId: \"<run-id>\",\n  workflowName: \"<workflow-id>\"\n});\n\nconsole.log(snapshot);\n```\n\n## Storage options for snapshots\n\nSnapshots are persisted using a `storage` instance configured on the `Mastra` class. This storage layer is shared across all workflows registered to that instance. Mastra supports multiple storage options for flexibility in different environments.\n\n### LibSQL `@mastra/libsql`\n\nThis example demonstrates how to use snapshots with LibSQL.\n\n```typescript filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { LibSQLStore } from \"@mastra/libsql\";\n\nexport const mastra = new Mastra({\n  // ...\n  storage: new LibSQLStore({\n    url: \":memory:\"\n  })\n});\n```\n\n### Upstash `@mastra/upstash`\n\nThis example demonstrates how to use snapshots with Upstash.\n\n```typescript filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { UpstashStore } from \"@mastra/upstash\";\n\nexport const mastra = new Mastra({\n  // ...\n  storage: new UpstashStore({\n    url: \"<upstash-redis-rest-url>\",\n    token: \"<upstash-redis-rest-token>\"\n  })\n})\n```\n\n### Postgres `@mastra/pg`\n\nThis example demonstrates how to use snapshots with PostgreSQL.\n\n```typescript filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { PostgresStore } from \"@mastra/pg\";\n\nexport const mastra = new Mastra({\n  // ...\n  storage: new PostgresStore({\n    connectionString: \"<database-url>\"\n  })\n});\n```\n\n## Best practices\n\n1. **Ensure Serializability**: Any data that needs to be included in the snapshot must be serializable (convertible to JSON).\n2. **Minimize Snapshot Size**: Avoid storing large data objects directly in the workflow context. Instead, store references to them (like IDs) and retrieve the data when needed.\n3. **Handle Resume Context Carefully**: When resuming a workflow, carefully consider what context to provide. This will be merged with the existing snapshot data.\n4. **Set Up Proper Monitoring**: Implement monitoring for suspended workflows, especially long-running ones, to ensure they are properly resumed.\n5. **Consider Storage Scaling**: For applications with many suspended workflows, ensure your storage solution is appropriately scaled.\n\n## Custom snapshot metadata\n\nYou can attach custom metadata when suspending a workflow by defining a `suspendSchema`. This metadata is stored in the snapshot and made available when the workflow is resumed.\n\n```typescript {30-34} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst approvalStep = createStep({\n  id: \"approval-step\",\n  description: \"Accepts a value, waits for confirmation\",\n  inputSchema: z.object({\n    value: z.number(),\n    user: z.string(),\n    requiredApprovers: z.array(z.string())\n  }),\n  suspendSchema: z.object({\n    message: z.string(),\n    requestedBy: z.string(),\n    approvers: z.array(z.string())\n  }),\n  resumeSchema: z.object({\n    confirm: z.boolean(),\n    approver: z.string()\n  }),\n  outputSchema: z.object({\n    value: z.number(),\n    approved: z.boolean()\n  }),\n  execute: async ({ inputData, resumeData, suspend }) => {\n    const { value, user, requiredApprovers } = inputData;\n    const { confirm } = resumeData ?? {};\n\n    if (!confirm) {\n      return await suspend({\n        message: \"Workflow suspended\",\n        requestedBy: user,\n        approvers: [...requiredApprovers]\n      });\n    }\n\n    return {\n      value,\n      approved: confirm\n    };\n  }\n});\n```\n\n### Providing resume data\n\nUse `resumeData` to pass structured input when resuming a suspended step. It must match the stepâ€™s `resumeSchema`.\n\n```typescript {14-20} showLineNumbers copy\nconst workflow = mastra.getWorkflow(\"approvalWorkflow\");\n\nconst run = await workflow.createRunAsync();\n\nconst result = await run.start({\n  inputData: {\n    value: 100,\n    user: \"Michael\",\n    requiredApprovers: [\"manager\", \"finance\"]\n  }\n});\n\nif (result.status === \"suspended\") {\n  const resumedResult = await run.resume({\n    step: \"approval-step\",\n    resumeData: {\n      confirm: true,\n      approver: \"manager\"\n    }\n  });\n}\n```\n\n## Related\n\n- [Suspend and resume](../../docs/workflows/suspend-and-resume.mdx)\n- [Human in the loop example](../../examples/workflows/human-in-the-loop.mdx)\n- [WorkflowRun.watch()](../../reference/workflows/run-methods/watch.mdx)\n\n\n","path":null,"size_bytes":8934,"size_tokens":null},"docs/mastra/01-agents/11_conversation-history.md":{"content":"---\ntitle: \"Conversation History | Memory | Mastra Docs\"\ndescription: \"Learn how to configure conversation history in Mastra to store recent messages from the current conversation.\"\n---\n\n# Conversation History\n[EN] Source: https://mastra.ai/en/docs/memory/conversation-history\n\nConversation history is the simplest kind of memory. It is a list of messages from the current conversation.\n\nBy default, each request includes the last 10 messages from the current memory thread, giving the agent short-term conversational context. This limit can be increased using the `lastMessages` parameter.\n\nYou can increase this limit by passing the `lastMessages` parameter to the `Memory` instance.\n\n```typescript {3-7} showLineNumbers\nexport const testAgent = new Agent({\n  // ...\n  memory: new Memory({\n    options: {\n      lastMessages: 20\n    },\n  })\n});\n```\n\n\n","path":null,"size_bytes":852,"size_tokens":null},"docs/mastra/06-reference/08_agent-get-model.md":{"content":"---\ntitle: \"Reference: Agent.getModel() | Agents | Mastra Docs\"\ndescription: \"Documentation for the `Agent.getModel()` method in Mastra agents, which retrieves the language model that powers the agent.\"\n---\n\n# Agent.getModel()\n[EN] Source: https://mastra.ai/en/reference/agents/getModel\n\nThe `.getModel()` method retrieves the language model configured for an agent, resolving it if it's a function. This method is used to access the underlying model that powers the agent's capabilities.\n\n## Usage example\n\n```typescript copy\nawait agent.getModel();\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"{ runtimeContext = new RuntimeContext() }\",\n      type: \"{ runtimeContext?: RuntimeContext }\",\n      isOptional: true,\n      defaultValue: \"new RuntimeContext()\",\n      description: \"Optional configuration object containing runtime context.\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"model\",\n      type: \"MastraLanguageModel | Promise<MastraLanguageModel>\",\n      description: \"The language model configured for the agent, either as a direct instance or a promise that resolves to the model.\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript copy\nawait agent.getModel({\n  runtimeContext: new RuntimeContext()\n});\n```\n\n### Options parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      isOptional: true,\n      defaultValue: \"undefined\",\n      description: \"Runtime context for dependency injection and contextual information.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Agents overview](../../docs/agents/overview.mdx)\n- [Runtime Context](../../docs/server-db/runtime-context.mdx)\n\n\n","path":null,"size_bytes":1695,"size_tokens":null},"docs/mastra/06-reference/05_agent-get-instructions.md":{"content":"---\ntitle: \"Reference: Agent.getInstructions() | Agents | Mastra Docs\"\ndescription: \"Documentation for the `Agent.getInstructions()` method in Mastra agents, which retrieves the instructions that guide the agent's behavior.\"\n---\n\n# Agent.getInstructions()\n[EN] Source: https://mastra.ai/en/reference/agents/getInstructions\n\nThe `.getInstructions()` method retrieves the instructions configured for an agent, resolving them if they're a function. These instructions guide the agent's behavior and define its capabilities and constraints.\n\n## Usage example\n\n```typescript copy\nawait agent.getInstructions();\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"options\",\n      type: \"{ runtimeContext?: RuntimeContext }\",\n      isOptional: true,\n      defaultValue: \"{}\",\n      description: \"Optional configuration object containing runtime context.\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"instructions\",\n      type: \"SystemMessage | Promise<SystemMessage>\",\n      description: \"The instructions configured for the agent. SystemMessage can be: string | string[] | CoreSystemMessage | CoreSystemMessage[] | SystemModelMessage | SystemModelMessage[]. Returns either directly or as a promise that resolves to the instructions.\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript copy\nawait agent.getInstructions({\n  runtimeContext: new RuntimeContext()\n});\n```\n\n### Options parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      isOptional: true,\n      defaultValue: \"undefined\",\n      description: \"Runtime context for dependency injection and contextual information.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Agents overview](../../docs/agents/overview.mdx)\n- [Runtime Context](../../docs/server-db/runtime-context.mdx)\n\n\n","path":null,"size_bytes":1831,"size_tokens":null},"docs/mastra/05-guides/02_ai-stock-agent.md":{"content":"---\ntitle: \"Building an AI Stock Agent | Mastra Agents | Guides\"\ndescription: Guide on creating a simple stock agent in Mastra to fetch the last day's closing stock price for a given symbol.\n---\n\nimport { Steps } from \"nextra/components\";\nimport YouTube from \"@/components/youtube\";\n\n# Building an AI Stock Agent\n[EN] Source: https://mastra.ai/en/guides/guide/stock-agent\n\nIn this guide, you're going to create a simple agent that fetches the last day's closing stock price for a given symbol. You'll learn how to create a tool, add it to an agent, and use the agent to fetch stock prices.\n\n<YouTube id=\"rIaZ4l7y9wo\" />\n\n## Prerequisites\n\n- Node.js `v20.0` or later installed\n- An API key from a supported [Model Provider](/models)\n- An existing Mastra project (Follow the [installation guide](/docs/getting-started/installation) to set up a new project)\n\n## Creating the Agent\n\nTo create an agent in Mastra use the `Agent` class to define it and then register it with Mastra.\n\n<Steps>\n\n### Define the Agent\n\nCreate a new file `src/mastra/agents/stockAgent.ts` and define your agent:\n\n```ts copy filename=\"src/mastra/agents/stockAgent.ts\"\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nexport const stockAgent = new Agent({\n  name: \"Stock Agent\",\n  instructions:\n    \"You are a helpful assistant that provides current stock prices. When asked about a stock, use the stock price tool to fetch the stock price.\",\n  model: openai(\"gpt-4o-mini\"),\n});\n```\n\n### Register the Agent with Mastra\n\nIn your `src/mastra/index.ts` file, register the agent:\n\n```ts copy filename=\"src/mastra/index.ts\" {2, 5}\nimport { Mastra } from \"@mastra/core\";\nimport { stockAgent } from \"./agents/stockAgent\";\n\nexport const mastra = new Mastra({\n  agents: { stockAgent },\n});\n```\n\n</Steps>\n\n## Creating the Stock Price Tool\n\nSo far the Stock Agent doesn't know anything about the current stock prices. To change this, create a tool and add it to the agent.\n\n<Steps>\n\n### Define the Tool\n\nCreate a new file `src/mastra/tools/stockPrices.ts`. Inside, add a `stockPrices` tool that will fetch the last day's closing stock price for a given symbol:\n\n```ts filename=\"src/mastra/tools/stockPrices.ts\"\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nconst getStockPrice = async (symbol: string) => {\n  const data = await fetch(\n    `https://mastra-stock-data.vercel.app/api/stock-data?symbol=${symbol}`,\n  ).then((r) => r.json());\n  return data.prices[\"4. close\"];\n};\n\nexport const stockPrices = createTool({\n  id: \"Get Stock Price\",\n  inputSchema: z.object({\n    symbol: z.string(),\n  }),\n  description: `Fetches the last day's closing stock price for a given symbol`,\n  execute: async ({ context: { symbol } }) => {\n    console.log(\"Using tool to fetch stock price for\", symbol);\n    return {\n      symbol,\n      currentPrice: await getStockPrice(symbol),\n    };\n  },\n});\n```\n\n### Add the Tool to the Stock Agent\n\nInside `src/mastra/agents/stockAgent.ts` import your newly created `stockPrices` tool and add it to the agent.\n\n```ts copy filename=\"src/mastra/agents/stockAgent.ts\" {3, 10-12}\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { stockPrices } from \"../tools/stockPrices\";\n\nexport const stockAgent = new Agent({\n  name: \"Stock Agent\",\n  instructions:\n    \"You are a helpful assistant that provides current stock prices. When asked about a stock, use the stock price tool to fetch the stock price.\",\n  model: openai(\"gpt-4o-mini\"),\n  tools: {\n    stockPrices,\n  },\n});\n```\n\n</Steps>\n\n## Running the Agent Server\n\nLearn how to interact with your agent through Mastra's API.\n\n<Steps>\n\n### Using `mastra dev`\n\nYou can run your agent as a service using the `mastra dev` command:\n\n```bash copy\nmastra dev\n```\n\nThis will start a server exposing endpoints to interact with your registered agents. Within the [playground](../../docs/server-db/local-dev-playground.mdx) you can test your `stockAgent` and `stockPrices` tool through a UI.\n\n### Accessing the Stock Agent API\n\nBy default, `mastra dev` runs on `http://localhost:4111`. Your Stock agent will be available at:\n\n```\nPOST http://localhost:4111/api/agents/stockAgent/generate\n```\n\n### Interacting with the Agent via `curl`\n\nYou can interact with the agent using `curl` from the command line:\n\n```bash copy\ncurl -X POST http://localhost:4111/api/agents/stockAgent/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"messages\": [\n      { \"role\": \"user\", \"content\": \"What is the current stock price of Apple (AAPL)?\" }\n    ]\n  }'\n```\n\n**Expected Response:**\n\nYou should receive a JSON response similar to:\n\n```json\n{\n  \"text\": \"The current price of Apple (AAPL) is $174.55.\",\n  \"agent\": \"Stock Agent\"\n}\n```\n\nThis indicates that your agent successfully processed the request, used the `stockPrices` tool to fetch the stock price, and returned the result.\n\n</Steps>\n\n\n","path":null,"size_bytes":4912,"size_tokens":null},"src/mastra/agents/exampleAgent.ts":{"content":"import { Agent } from \"@mastra/core/agent\";\nimport { Memory } from \"@mastra/memory\";\nimport { sharedPostgresStorage } from \"../storage\";\nimport { exampleTool } from \"../tools/exampleTool\";\nimport { createOpenAI } from \"@ai-sdk/openai\";\n\n/**\n * LLM CLIENT CONFIGURATION\n *\n * IMPORTANT: Both approaches require the SAME syntax for Replit Playground compatibility:\n * - Use AI SDK v4: model, e.g. openai(\"gpt-4o-mini\")\n * - In workflows: Use agent.generateLegacy()\n * - The Replit Playground UI always calls the legacy Mastra endpoint.\n * NOTE: You must always keep the API key as an environment variable for safety!\n * ---\n * OPTION 1: Replit AI Integrations, **only** if user has enabled it via connector.\n *\n * No OpenAI API key required - charges billed to Replit credits\n * Automatic key rotation and management\n */\nconst openai = createOpenAI({\n  baseURL: process.env.AI_INTEGRATIONS_OPENAI_BASE_URL,\n  apiKey: process.env.AI_INTEGRATIONS_OPENAI_API_KEY,\n});\n/*\n * OPTION 2: Standard OpenAI Setup (Your Own API Key)\n */\n// const openai = createOpenAI({\n//   baseURL: process.env.OPENAI_BASE_URL || undefined,\n//   apiKey: process.env.OPENAI_API_KEY,\n// });\n\n/**\n * Example Mastra Agent\n *\n * MASTRA AGENT GUIDE:\n * - Agents are AI-powered assistants that can use tools and maintain conversation memory\n * - They combine an LLM model with tools and optional memory storage\n * - Agents can be used in workflows\n */\n\nexport const exampleAgent = new Agent({\n  // Give your agent a descriptive name\n  name: \"Example Agent\",\n\n  /**\n   * Instructions define your agent's behavior and personality\n   * Be specific about:\n   * - What the agent should do\n   * - How it should respond\n   * - What tools it should use and when\n   * - Any constraints or guidelines\n   */\n  instructions: `\n    You are a helpful example agent that demonstrates how to use Mastra agents.\n\n    Your primary function is to process messages using the example tool and explain what you're doing.\n\n    When responding:\n    - Always be helpful and educational\n    - Explain what tools you're using and why\n    - If asked to process a message, use the exampleTool\n    - Share the results in a clear, formatted way\n    - Add educational comments about how Mastra works when relevant\n\n    Remember: You're teaching developers how to use Mastra by example!\n`,\n\n  /**\n   * Choose your LLM model\n   *\n   * MUST use AI SDK v4 syntax for Replit Playground compatibility.\n   * Use openai.responses(\"gpt-5\") for gpt-5 class models, use openai(\"gpt-4o\") for gpt-4 class models.\n   */\n  model: openai.responses(\"gpt-5\"),\n\n  /**\n   * Provide tools that the agent can use\n   * Tools must be created with createTool()\n   * You can provide multiple tools.\n   */\n  tools: { exampleTool },\n\n  /**\n   * Optional: Add memory to persist conversations.\n   * Using PostgreSQL for production-ready persistent storage.\n   * Only add memory if the user requests it or it's strongly implied (e.g., a chatbot that needs to remember context).  See Mastra docs for more information.\n   */\n  memory: new Memory({\n    options: {\n      threads: {\n        generateTitle: true, // Auto-generate conversation titles\n      },\n      lastMessages: 10, // Example: Keep last 10 messages in context\n    },\n    storage: sharedPostgresStorage,\n  }),\n\n  /**\n   * Optional: Configure additional settings\n   */\n  // maxSteps: 10, // Limit tool usage iterations if needed\n  // temperature: 0.9, // Control creativity (0-1)\n  // If you need other standard LLM agent features, check the Mastra docs to see if there's a primitive you can use.\n});\n","path":null,"size_bytes":3563,"size_tokens":null},"docs/mastra/06-reference/121_suspend.md":{"content":"---\ntitle: \"Reference: suspend() | Control Flow | Mastra Docs\"\ndescription: \"Documentation for the suspend function in Mastra workflows, which pauses execution until resumed.\"\n---\n\n# suspend()\n[EN] Source: https://mastra.ai/en/reference/legacyWorkflows/suspend\n\nPauses workflow execution at the current step until explicitly resumed. The workflow state is persisted and can be continued later.\n\n## Usage Example\n\n```typescript\nconst approvalStep = new LegacyStep({\n  id: \"needsApproval\",\n  execute: async ({ context, suspend }) => {\n    if (context.steps.amount > 1000) {\n      await suspend();\n    }\n    return { approved: true };\n  },\n});\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"metadata\",\n      type: \"Record<string, any>\",\n      description: \"Optional data to store with the suspended state\",\n      isOptional: true,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"Promise<void>\",\n      type: \"Promise\",\n      description: \"Resolves when the workflow is successfully suspended\",\n    },\n  ]}\n/>\n\n## Additional Examples\n\nSuspend with metadata:\n\n```typescript\nconst reviewStep = new LegacyStep({\n  id: \"review\",\n  execute: async ({ context, suspend }) => {\n    await suspend({\n      reason: \"Needs manager approval\",\n      requestedBy: context.user,\n    });\n    return { reviewed: true };\n  },\n});\n```\n\n### Related\n\n- [Suspend & Resume Workflows](../../docs/workflows-legacy/suspend-and-resume.mdx)\n- [.resume()](./resume.mdx)\n- [.watch()](./watch.mdx)\n\n\n","path":null,"size_bytes":1514,"size_tokens":null},"docs/mastra/07-migration/00_agent-network-migration.md":{"content":"---\ntitle: \"AgentNetwork to .network() | Migration Guide\"\ndescription: \"Learn how to migrate from AgentNetwork primitives to .network() in Mastra.\"\n---\n\n## Overview\n[EN] Source: https://mastra.ai/en/guides/migrations/agentnetwork\n\nAs of `v0.20.0` for `@mastra/core`, the following changes apply.\n\n### Upgrade from AI SDK v4 to v5\n\n- Bump all your model provider packages by a major version.\n\n> This will ensure that they are all v5 models now.\n\n### Memory is required\n\n- Memory is now required for the agent network to function properly.\n\n> You must configure memory for the agent.\n\n## Migration paths\n\nIf you were using the `AgentNetwork` primitive, you can replace the `AgentNetwork` with `Agent`.\n\nBefore:\n\n```typescript\nimport { AgentNetwork } from '@mastra/core/network';\n\nconst agent = new AgentNetwork({\n  name: 'agent-network',\n  agents: [agent1, agent2],\n  tools: { tool1, tool2 },\n  model: openai('gpt-4o'),\n  instructions: 'You are a network agent that can help users with a variety of tasks.',\n});\n\nawait agent.stream('Find me the weather in Tokyo.');\n```\n\nAfter:\n\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { Memory } from '@mastra/memory';\n\nconst memory = new Memory();\n\nconst agent = new Agent({\n  name: 'agent-network',\n  agents: { agent1, agent2 },\n  tools: { tool1, tool2 },\n  model: openai('gpt-4o'),\n  instructions: 'You are a network agent that can help users with a variety of tasks.',\n  memory,\n});\n\nawait agent.network('Find me the weather in Tokyo.');\n```\n\nIf you were using the `NewAgentNetwork` primitive, you can replace the `NewAgentNetwork` with `Agent`.\n\nBefore:\n\n```typescript\nimport { NewAgentNetwork } from '@mastra/core/network/vnext';\n\nconst agent = new NewAgentNetwork({\n  name: 'agent-network',\n  agents: { agent1, agent2 },\n  workflows: { workflow1 },\n  tools: { tool1, tool2 },\n  model: openai('gpt-4o'),\n  instructions: 'You are a network agent that can help users with a variety of tasks.',\n});\n\nawait agent.loop('Find me the weather in Tokyo.');\n```\n\nAfter:\n\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { Memory } from '@mastra/memory';\n\nconst memory = new Memory();\n\nconst agent = new Agent({\n  name: 'agent-network',\n  agents: { agent1, agent2 },\n  workflows: { workflow1 },\n  tools: { tool1, tool2 },\n  model: openai('gpt-4o'),\n  instructions: 'You are a network agent that can help users with a variety of tasks.',\n  memory,\n});\n\nawait agent.network('Find me the weather in Tokyo.');\n```\n\n","path":null,"size_bytes":2479,"size_tokens":null},"src/mastra/tools/textMatchTool.ts":{"content":"import { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\nimport * as fs from \"fs\";\nimport * as path from \"path\";\nimport { fileURLToPath } from \"url\";\n\nlet memoryLines: string[] = [];\nlet memoryLinesLower: string[] = [];\n\nfunction loadMemory() {\n  let currentDir = \"\";\n  try {\n    currentDir = path.dirname(fileURLToPath(import.meta.url));\n  } catch (e) {\n    currentDir = process.cwd();\n  }\n\n  const possiblePaths = [\n    path.join(process.cwd(), \"us-complete.txt\"),\n    path.join(process.cwd(), \"..\", \"us-complete.txt\"),\n    path.join(process.cwd(), \"..\", \"..\", \"us-complete.txt\"),\n    \"/home/runner/workspace/us-complete.txt\",\n    path.join(currentDir, \"..\", \"..\", \"..\", \"us-complete.txt\"),\n    path.join(currentDir, \"..\", \"..\", \"..\", \"..\", \"us-complete.txt\"),\n    path.join(process.cwd(), \"public\", \"us-complete.txt\"),\n  ];\n\n  for (const memoryPath of possiblePaths) {\n    try {\n      if (fs.existsSync(memoryPath)) {\n        const content = fs.readFileSync(memoryPath, \"utf-8\");\n        memoryLines = content\n          .split(\"\\n\")\n          .map((line) => line.trim())\n          .filter((line) => line.length > 3);\n        memoryLinesLower = memoryLines.map((line) => line.toLowerCase());\n        console.log(`ðŸ“š Loaded ${memoryLines.length} lines from ${memoryPath}`);\n        return;\n      }\n    } catch (error) {\n      continue;\n    }\n  }\n  \n  console.error(\"âŒ Failed to load us-complete.txt from any path\");\n  console.log(\"Tried paths:\", possiblePaths);\n  memoryLines = [];\n  memoryLinesLower = [];\n}\n\nloadMemory();\n\nfunction tokenize(text: string): string[] {\n  return text\n    .toLowerCase()\n    .replace(/[^\\w\\s]/g, \" \")\n    .split(/\\s+/)\n    .filter((word) => word.length > 2);\n}\n\nfunction scoreMatch(queryTokens: string[], lineTokens: string[]): number {\n  if (lineTokens.length === 0) return 0;\n  let matches = 0;\n  for (const qt of queryTokens) {\n    for (const lt of lineTokens) {\n      if (lt.includes(qt) || qt.includes(lt)) {\n        matches++;\n        break;\n      }\n    }\n  }\n  return matches / Math.max(queryTokens.length, 1);\n}\n\nfunction findBestMatches(query: string, limit: number = 3): { line: string; score: number }[] {\n  const queryLower = query.toLowerCase().trim();\n  const queryTokens = tokenize(query);\n  const results: { line: string; score: number; index: number }[] = [];\n\n  for (let i = 0; i < memoryLinesLower.length; i++) {\n    const lineLower = memoryLinesLower[i];\n    const line = memoryLines[i];\n\n    if (lineLower.includes(queryLower)) {\n      results.push({ line, score: 1.0, index: i });\n      continue;\n    }\n\n    for (const token of queryTokens) {\n      if (token.length > 3 && lineLower.includes(token)) {\n        const lineTokens = tokenize(line);\n        const score = scoreMatch(queryTokens, lineTokens);\n        if (score > 0.3) {\n          results.push({ line, score, index: i });\n        }\n        break;\n      }\n    }\n  }\n\n  results.sort((a, b) => b.score - a.score);\n  \n  const seen = new Set<string>();\n  const unique: { line: string; score: number }[] = [];\n  for (const r of results) {\n    const key = r.line.substring(0, 50);\n    if (!seen.has(key)) {\n      seen.add(key);\n      unique.push({ line: r.line, score: r.score });\n      if (unique.length >= limit) break;\n    }\n  }\n\n  return unique;\n}\n\nexport const textMatchTool = createTool({\n  id: \"text-match-tool\",\n  description:\n    \"Searches the full memory (us-complete.txt) for lines matching the input using token-based scoring. Returns the best matches from 9000+ lines of conversation history.\",\n\n  inputSchema: z.object({\n    searchText: z.string().describe(\"The text to search for in the memory\"),\n  }),\n\n  outputSchema: z.object({\n    matchedLine: z.string().describe(\"The best matched line from memory\"),\n    foundMatch: z.boolean().describe(\"Whether a match was found\"),\n    confidence: z.number().describe(\"Match confidence score 0-1\"),\n    alternatives: z.array(z.string()).describe(\"Other good matches\"),\n  }),\n\n  execute: async ({ context, mastra }) => {\n    const logger = mastra?.getLogger();\n    const searchText = context.searchText;\n    \n    logger?.info(\"ðŸ” [textMatchTool] Searching for:\", { searchText });\n    logger?.info(\"ðŸ“š [textMatchTool] Memory has\", { lineCount: memoryLines.length });\n\n    if (memoryLines.length === 0) {\n      loadMemory();\n    }\n\n    if (memoryLines.length === 0) {\n      logger?.error(\"âŒ [textMatchTool] No memory loaded\");\n      return {\n        matchedLine: \"Memory not loaded. Check us-complete.txt file.\",\n        foundMatch: false,\n        confidence: 0,\n        alternatives: [],\n      };\n    }\n\n    const matches = findBestMatches(searchText, 5);\n\n    if (matches.length > 0 && matches[0].score > 0.3) {\n      logger?.info(\"âœ… [textMatchTool] Found match:\", { \n        match: matches[0].line.substring(0, 100),\n        score: matches[0].score \n      });\n      \n      return {\n        matchedLine: matches[0].line,\n        foundMatch: true,\n        confidence: matches[0].score,\n        alternatives: matches.slice(1).map(m => m.line),\n      };\n    }\n\n    const randomIndex = Math.floor(Math.random() * Math.min(100, memoryLines.length));\n    const randomLine = memoryLines[randomIndex] || \"got it. what now, brother?\";\n    \n    logger?.info(\"ðŸŽ² [textMatchTool] No strong match, returning contextual response\");\n    return {\n      matchedLine: randomLine,\n      foundMatch: false,\n      confidence: 0,\n      alternatives: [],\n    };\n  },\n});\n\nexport function reloadMemory() {\n  loadMemory();\n  return memoryLines.length;\n}\n\nexport function getMemoryStats() {\n  return {\n    lineCount: memoryLines.length,\n    loaded: memoryLines.length > 0,\n  };\n}\n","path":null,"size_bytes":5647,"size_tokens":null},"docs/mastra/06-reference/30_mastra-class.md":{"content":"---\ntitle: \"Reference: Mastra Class | Core | Mastra Docs\"\ndescription: \"Documentation for the `Mastra` class in Mastra, the core entry point for managing agents, workflows, MCP servers, and server endpoints.\"\n---\n\n# Mastra Class\n[EN] Source: https://mastra.ai/en/reference/core/mastra-class\n\nThe `Mastra` class is the central orchestrator in any Mastra application, managing agents, workflows, storage, logging, telemetry, and more. Typically, you create a single instance of `Mastra` to coordinate your application.\n\nThink of `Mastra` as a top-level registry:\n\n- Registering **integrations** makes them accessible to **agents**, **workflows**, and **tools** alike.\n- **tools** arenâ€™t registered on `Mastra` directly but are associated with agents and discovered automatically.\n\n\n## Usage example\n\n```typescript filename=\"src/mastra/index.ts\"\nimport { Mastra } from '@mastra/core/mastra';\nimport { PinoLogger } from '@mastra/loggers';\nimport { LibSQLStore } from '@mastra/libsql';\nimport { weatherWorkflow } from './workflows/weather-workflow';\nimport { weatherAgent } from './agents/weather-agent';\n\nexport const mastra = new Mastra({\n  workflows: { weatherWorkflow },\n  agents: { weatherAgent },\n  storage: new LibSQLStore({\n    url: \":memory:\",\n  }),\n  logger: new PinoLogger({\n    name: 'Mastra',\n    level: 'info',\n  }),\n});\n```\n\n## Constructor parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"agents\",\n      type: \"Agent[]\",\n      description: \"Array of Agent instances to register\",\n      isOptional: true,\n      defaultValue: \"[]\",\n    },\n    {\n      name: \"tools\",\n      type: \"Record<string, ToolApi>\",\n      description:\n        \"Custom tools to register. Structured as a key-value pair, with keys being the tool name and values being the tool function.\",\n      isOptional: true,\n      defaultValue: \"{}\",\n    },\n    {\n      name: \"storage\",\n      type: \"MastraStorage\",\n      description: \"Storage engine instance for persisting data\",\n      isOptional: true,\n    },\n    {\n      name: \"vectors\",\n      type: \"Record<string, MastraVector>\",\n      description:\n        \"Vector store instance, used for semantic search and vector-based tools (eg Pinecone, PgVector or Qdrant)\",\n      isOptional: true,\n    },\n    {\n      name: \"logger\",\n      type: \"Logger\",\n      description: \"Logger instance created with new PinoLogger()\",\n      isOptional: true,\n      defaultValue: \"Console logger with INFO level\",\n    },\n    {\n      name: \"idGenerator\",\n      type: \"() => string\",\n      description: \"Custom ID generator function. Used by agents, workflows, memory, and other components to generate unique identifiers.\",\n      isOptional: true,\n    },\n    {\n      name: \"workflows\",\n      type: \"Record<string, Workflow>\",\n      description:\n        \"Workflows to register. Structured as a key-value pair, with keys being the workflow name and values being the workflow instance.\",\n      isOptional: true,\n      defaultValue: \"{}\",\n    },\n    {\n      name: \"tts\",\n      type: \"Record<string, MastraTTS>\",\n      isOptional: true,\n      description: \"An object for registering Text-To-Speech services.\",\n    },\n    {\n      name: \"telemetry\",\n      type: \"OtelConfig\",\n      isOptional: true,\n      description: \"Configuration for OpenTelemetry integration.\",\n    },\n    {\n      name: \"deployer\",\n      type: \"MastraDeployer\",\n      isOptional: true,\n      description: \"An instance of a MastraDeployer for managing deployments.\",\n    },\n    {\n      name: \"server\",\n      type: \"ServerConfig\",\n      description:\n        \"Server configuration including port, host, timeout, API routes, middleware, CORS settings, and build options for Swagger UI, API request logging, and OpenAPI docs.\",\n      isOptional: true,\n      defaultValue:\n        \"{ port: 4111, host: localhost,  cors: { origin: '*', allowMethods: ['GET', 'POST', 'PUT', 'PATCH', 'DELETE', 'OPTIONS'], allowHeaders: ['Content-Type', 'Authorization', 'x-mastra-client-type'], exposeHeaders: ['Content-Length', 'X-Requested-With'], credentials: false } }\",\n    },\n    {\n      name: \"mcpServers\",\n      type: \"Record<string, MCPServerBase>\",\n      isOptional: true,\n      description:\n        \"An object where keys are unique server identifiers and values are instances of MCPServer or classes extending MCPServerBase. This allows Mastra to be aware of and potentially manage these MCP servers.\",\n    },\n    {\n      name: \"bundler\",\n      type: \"BundlerConfig\",\n      description: \"Configuration for the asset bundler with options for externals, sourcemap, and transpilePackages.\",\n      isOptional: true,\n      defaultValue: \"{ externals: [], sourcemap: false, transpilePackages: [] }\",\n    },\n    {\n      name: \"scorers\",\n      type: \"Record<string, MastraScorer>\",\n        description: \"Scorers to register for scoring traces and overriding default scorers used during agent generation or workflow execution. Structured as a key-value pair, with keys being the scorer name and values being the scorer instance.\",\n      isOptional: true,\n      defaultValue: \"{}\",\n    },\n  ]}\n/>\n\n\n\n\n\n","path":null,"size_bytes":5055,"size_tokens":null},"docs/mastra/01-agents/05_runtime-context.md":{"content":"---\ntitle: \"Runtime Context | Agents | Mastra Docs\"\ndescription: Learn how to use Mastra's RuntimeContext to provide dynamic, request-specific configuration to agents.\n---\n\nimport { Callout } from \"nextra/components\";\n\n# Runtime Context\n[EN] Source: https://mastra.ai/en/docs/server-db/runtime-context\n\nAgents, tools, and workflows can all accept `RuntimeContext` as a parameter, making request-specific values available to the underlying primitives.\n\n## When to use `RuntimeContext`\n\nUse `RuntimeContext` when a primitiveâ€™s behavior should change based on runtime conditions. For example, you might switch models or storage backends based on user attributes, or adjust instructions and tool selection based on language.\n\n<Callout>\n  **Note:** `RuntimeContext` is primarily used for passing data into specific\n  requests. It's distinct from agent memory, which handles conversation\n  history and state persistence across multiple calls.\n</Callout>\n\n## Setting values\n\nPass `runtimeContext` into an agent, network, workflow, or tool call to make values available to all underlying primitives during execution. Use `.set()` to define values before making the call.\n\nThe `.set()` method takes two arguments:\n\n1. **key**: The name used to identify the value.\n2. **value**: The data to associate with that key.\n\n```typescript showLineNumbers\nimport { RuntimeContext } from \"@mastra/core/runtime-context\";\n\nexport type UserTier = {\n  \"user-tier\": \"enterprise\" | \"pro\";\n};\n\nconst runtimeContext = new RuntimeContext<UserTier>();\nruntimeContext.set(\"user-tier\", \"enterprise\");\n\nconst agent = mastra.getAgent(\"weatherAgent\");\nawait agent.generate(\"What's the weather in London?\", {\n  runtimeContext\n})\n\nconst routingAgent = mastra.getAgent(\"routingAgent\");\nroutingAgent.network(\"What's the weather in London?\", {\n  runtimeContext\n});\n\nconst run = await mastra.getWorkflow(\"weatherWorkflow\").createRunAsync();\nawait run.start({\n  inputData: {\n    location: \"London\"\n  },\n  runtimeContext\n});\nawait run.resume({\n  resumeData: {\n    city: \"New York\"\n  },\n  runtimeContext\n});\n\nawait weatherTool.execute({\n  context: {\n    location: \"London\"\n  },\n  runtimeContext\n});\n```\n\n### Setting values based on request headers\n\nYou can populate `runtimeContext` dynamically in server middleware by extracting information from the request. In this example, the `temperature-unit` is set based on the Cloudflare `CF-IPCountry` header to ensure responses match the user's locale.\n\n```typescript filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { RuntimeContext } from \"@mastra/core/runtime-context\";\nimport { testWeatherAgent } from \"./agents/test-weather-agent\";\n\nexport const mastra = new Mastra({\n  agents: { testWeatherAgent },\n  server: {\n    middleware: [\n      async (context, next) => {\n        const country = context.req.header(\"CF-IPCountry\");\n        const runtimeContext = context.get(\"runtimeContext\");\n\n        runtimeContext.set(\"temperature-unit\", country === \"US\" ? \"fahrenheit\" : \"celsius\");\n\n        await next();\n      }\n    ]\n  }\n});\n```\n\n> See [Middleware](../server-db/middleware.mdx) for how to use server middleware.\n\n## Accessing values with agents\n\nYou can access the `runtimeContext` argument from any supported configuration options in agents. These functions can be sync or `async`. Use the `.get()` method to read values from `runtimeContext`.\n\n```typescript {7-8,15,18,21} filename=\"src/mastra/agents/weather-agent.ts\" showLineNumbers\nexport type UserTier = {\n  \"user-tier\": \"enterprise\" | \"pro\";\n};\n\nexport const weatherAgent = new Agent({\n  name: \"weather-agent\",\n  instructions: async ({ runtimeContext }) => {\n    const userTier = runtimeContext.get(\"user-tier\") as UserTier[\"user-tier\"];\n\n    if (userTier === \"enterprise\") {\n      // ...\n    }\n    // ...\n  },\n  model: ({ runtimeContext }) => {\n    // ...\n  },\n  tools: ({ runtimeContext }) => {\n    // ...\n  },\n  memory: ({ runtimeContext }) => {\n    // ...\n  },\n});\n```\n\nYou can also use `runtimeContext` with other options like `agents`, `workflows`, `scorers`, `inputProcessors`, and `outputProcessors`.\n\n> See [Agent](../../reference/agents/agent.mdx) for a full list of configuration options.\n\n## Accessing values from workflow steps\n\nYou can access the `runtimeContext` argument from a workflow step's `execute` function. This function can be sync or async. Use the `.get()` method to read values from `runtimeContext`.\n\n```typescript {7-8} filename=\"src/mastra/workflows/weather-workflow.ts\" showLineNumbers copy\nexport type UserTier = {\n  \"user-tier\": \"enterprise\" | \"pro\";\n};\n\nconst stepOne = createStep({\n  id: \"step-one\",\n  execute: async ({ runtimeContext }) => {\n    const userTier = runtimeContext.get(\"user-tier\") as UserTier[\"user-tier\"];\n\n    if (userTier === \"enterprise\") {\n      // ...\n    }\n    // ...\n  }\n});\n```\n\n> See [createStep()](../../reference/workflows/step.mdx) for a full list of configuration options.\n\n## Accessing values with tools\n\nYou can access the `runtimeContext` argument from a toolâ€™s `execute` function. This function is `async`.  Use the `.get()` method to read values from `runtimeContext`.\n\n```typescript {7-8} filename=\"src/mastra/tools/weather-tool.ts\" showLineNumbers\nexport type UserTier = {\n  \"user-tier\": \"enterprise\" | \"pro\";\n};\n\nexport const weatherTool = createTool({\n  id: \"weather-tool\",\n  execute: async ({ runtimeContext }) => {\n    const userTier = runtimeContext.get(\"user-tier\") as UserTier[\"user-tier\"];\n\n    if (userTier === \"enterprise\") {\n      // ...\n    }\n   // ...\n  }\n});\n```\n\n> See [createTool()](../../reference/tools/create-tool.mdx) for a full list of configuration options.\n\n## Related\n\n- [Runtime Context Example](../../examples/agents/runtime-context.mdx)\n- [Agent Runtime Context](../agents/overview.mdx#using-runtimecontext)\n- [Workflow Runtime Context](../workflows/overview.mdx#using-runtimecontext)\n- [Tool Runtime Context](../tools-mcp/overview.mdx#using-runtimecontext)\n- [Server Middleware Runtime Context](../server-db/middleware.mdx)\n\n\n---\ntitle: Storage in Mastra | Mastra Docs\ndescription: Overview of Mastra's storage system and data persistence capabilities.\n---\n\nimport { Tabs } from \"nextra/components\";\n\nimport { PropertiesTable } from \"@/components/properties-table\";\nimport { SchemaTable } from \"@/components/schema-table\";\nimport { StorageOverviewImage } from \"@/components/storage-overview-image\";\n\n# MastraStorage\n[EN] Source: https://mastra.ai/en/docs/server-db/storage\n\n`MastraStorage` provides a unified interface for managing:\n\n- **Suspended Workflows**: the serialized state of suspended workflows (so they can be resumed later)\n- **Memory**: threads and messages per `resourceId` in your application\n- **Traces**: OpenTelemetry traces from all components of Mastra\n- **Eval Datasets**: scores and scoring reasons from eval runs\n\n<br />\n\n<br />\n\n<StorageOverviewImage />\n\nMastra provides different storage providers, but you can treat them as interchangeable. Eg, you could use libsql in development but postgres in production, and your code will work the same both ways.\n\n## Configuration\n\nMastra can be configured with a default storage option:\n\n```typescript copy\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { LibSQLStore } from \"@mastra/libsql\";\n\nconst mastra = new Mastra({\n  storage: new LibSQLStore({\n    url: \"file:./mastra.db\",\n  }),\n});\n```\n\nIf you do not specify any `storage` configuration, Mastra will not persist data across application restarts or deployments. For any\ndeployment beyond local testing you should provide your own storage\nconfiguration either on `Mastra` or directly within `new Memory()`.\n\n## Data Schema\n\n{/*\nLLM CONTEXT: This Tabs component displays the database schema for different data types stored by Mastra.\nEach tab shows the table structure and column definitions for a specific data entity (Messages, Threads, Workflows, etc.).\nThe tabs help users understand the data model and relationships between different storage entities.\nEach tab includes detailed column information with types, constraints, and example data structures.\nThe data types include Messages, Threads, Workflows, Eval Datasets, and Traces.\n*/}\n\n<Tabs items={['Messages', 'Threads', 'Resources', 'Workflows', 'Eval Datasets', 'Traces']}>\n  <Tabs.Tab>\nStores conversation messages and their metadata. Each message belongs to a thread and contains the actual content along with metadata about the sender role and message type.\n\n<br />\n<SchemaTable\n  columns={[\n    {\n      name: \"id\",\n      type: \"uuidv4\",\n      description: \"Unique identifier for the message (format: `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`)\",\n      constraints: [\n        { type: \"primaryKey\" },\n        { type: \"nullable\", value: false }\n      ]\n    },\n    {\n      name: \"thread_id\",\n      type: \"uuidv4\",\n      description: \"Parent thread reference\",\n      constraints: [\n        { type: \"foreignKey\", value: \"threads.id\" },\n        { type: \"nullable\", value: false }\n      ]\n    },\n    {\n      name: \"resourceId\",\n      type: \"uuidv4\",\n      description: \"ID of the resource that owns this message\",\n      constraints: [\n        { type: \"nullable\", value: true }\n      ]\n    },\n    {\n      name: \"content\",\n      type: \"text\",\n      description: \"JSON of the message content in V2 format. Example: `{ format: 2, parts: [...] }`\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"role\",\n      type: \"text\",\n      description: \"Enum of `user | assistant`\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"createdAt\",\n      type: \"timestamp\",\n      description: \"Used for thread message ordering\",\n      constraints: [{ type: \"nullable\", value: false }]\n    }\n  ]}\n/>\n\nThe message `content` column contains a JSON object conforming to the `MastraMessageContentV2` type, which is designed to align closely with the AI SDK `UIMessage` message shape.\n\n<SchemaTable\n  columns={[\n    {\n      name: \"format\",\n      type: \"integer\",\n      description: \"Message format version (currently 2)\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"parts\",\n      type: \"array (JSON)\",\n      description: \"Array of message parts (text, tool-invocation, file, reasoning, etc.). The structure of items in this array varies by `type`.\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"experimental_attachments\",\n      type: \"array (JSON)\",\n      description: \"Optional array of file attachments\",\n      constraints: [{ type: \"nullable\", value: true }]\n    },\n    {\n      name: \"content\",\n      type: \"text\",\n      description: \"Optional main text content of the message\",\n      constraints: [{ type: \"nullable\", value: true }]\n    },\n    {\n      name: \"toolInvocations\",\n      type: \"array (JSON)\",\n      description: \"Optional array summarizing tool calls and results\",\n      constraints: [{ type: \"nullable\", value: true }]\n    },\n    {\n      name: \"reasoning\",\n      type: \"object (JSON)\",\n      description: \"Optional information about the reasoning process behind the assistant's response\",\n      constraints: [{ type: \"nullable\", value: true }]\n    },\n    {\n      name: \"annotations\",\n      type: \"object (JSON)\",\n      description: \"Optional additional metadata or annotations\",\n      constraints: [{ type: \"nullable\", value: true }]\n    }\n  ]}\n/>\n\n\n\n</Tabs.Tab>\n\n  <Tabs.Tab>\nGroups related messages together and associates them with a resource. Contains metadata about the conversation.\n\n<br />\n<SchemaTable\n  columns={[\n    {\n      name: \"id\",\n      type: \"uuidv4\",\n      description: \"Unique identifier for the thread (format: `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`)\",\n      constraints: [\n        { type: \"primaryKey\" },\n        { type: \"nullable\", value: false }\n      ]\n    },\n    {\n      name: \"resourceId\",\n      type: \"text\",\n      description: \"Primary identifier of the external resource this thread is associated with. Used to group and retrieve related threads.\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"title\",\n      type: \"text\",\n      description: \"Title of the conversation thread\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"metadata\",\n      type: \"text\",\n      description: \"Custom thread metadata as stringified JSON. Example:\",\n      example: {\n        category: \"support\",\n        priority: 1\n      }\n    },\n    {\n      name: \"createdAt\",\n      type: \"timestamp\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"updatedAt\",\n      type: \"timestamp\",\n      description: \"Used for thread ordering history\",\n      constraints: [{ type: \"nullable\", value: false }]\n    }\n  ]}\n/>\n\n</Tabs.Tab>\n  <Tabs.Tab>\nStores user-specific data for resource-scoped working memory. Each resource represents a user or entity, allowing working memory to persist across all conversation threads for that user.\n\n<br />\n<SchemaTable\n  columns={[\n    {\n      name: \"id\",\n      type: \"text\",\n      description: \"Resource identifier (user or entity ID) - same as resourceId used in threads and agent calls\",\n      constraints: [\n        { type: \"primaryKey\" },\n        { type: \"nullable\", value: false }\n      ]\n    },\n    {\n      name: \"workingMemory\",\n      type: \"text\",\n      description: \"Persistent working memory data as Markdown text. Contains user profile, preferences, and contextual information that persists across conversation threads.\",\n      constraints: [{ type: \"nullable\", value: true }]\n    },\n    {\n      name: \"metadata\",\n      type: \"jsonb\",\n      description: \"Additional resource metadata as JSON. Example:\",\n      example: {\n        preferences: { language: \"en\", timezone: \"UTC\" },\n        tags: [\"premium\", \"beta-user\"]\n      },\n      constraints: [{ type: \"nullable\", value: true }]\n    },\n    {\n      name: \"createdAt\",\n      type: \"timestamp\",\n      description: \"When the resource record was first created\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"updatedAt\",\n      type: \"timestamp\",\n      description: \"When the working memory was last updated\",\n      constraints: [{ type: \"nullable\", value: false }]\n    }\n  ]}\n/>\n\n**Note**: This table is only created and used by storage adapters that support resource-scoped working memory (LibSQL, PostgreSQL, Upstash). Other storage adapters will provide helpful error messages if resource-scoped memory is attempted.\n\n</Tabs.Tab>\n  <Tabs.Tab>\nWhen `suspend` is called on a workflow, its state is saved in the following format. When `resume` is called, that state is rehydrated.\n\n<br />\n<SchemaTable\n  columns={[\n    {\n      name: \"workflow_name\",\n      type: \"text\",\n      description: \"Name of the workflow\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"run_id\",\n      type: \"uuidv4\",\n      description: \"Unique identifier for the workflow execution. Used to track state across suspend/resume cycles (format: `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`)\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"snapshot\",\n      type: \"text\",\n      description: \"Serialized workflow state as JSON. Example:\",\n      example: {\n        value: { currentState: 'running' },\n        context: {\n          stepResults: {},\n          attempts: {},\n          triggerData: {}\n        },\n        activePaths: [],\n        runId: '550e8400-e29b-41d4-a716-446655440000',\n        timestamp: 1648176000000\n      },\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"createdAt\",\n      type: \"timestamp\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"updatedAt\",\n      type: \"timestamp\",\n      description: \"Last modification time, used to track state changes during workflow execution\",\n      constraints: [{ type: \"nullable\", value: false }]\n    }\n  ]}\n/>\n  </Tabs.Tab>\n  <Tabs.Tab>\nStores eval results from running metrics against agent outputs.\n\n<br />\n<SchemaTable\n  columns={[\n    {\n      name: \"input\",\n      type: \"text\",\n      description: \"Input provided to the agent\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"output\",\n      type: \"text\",\n      description: \"Output generated by the agent\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"result\",\n      type: \"jsonb\",\n      description: \"Eval result data that includes score and details. Example:\",\n      example: {\n        score: 0.95,\n        details: {\n          reason: \"Response accurately reflects source material\",\n          citations: [\"page 1\", \"page 3\"]\n        }\n      },\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"agent_name\",\n      type: \"text\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"metric_name\",\n      type: \"text\",\n      description: \"e.g Faithfulness, Hallucination, etc.\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"instructions\",\n      type: \"text\",\n      description: \"System prompt or instructions for the agent\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"test_info\",\n      type: \"jsonb\",\n      description: \"Additional test metadata and configuration\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"global_run_id\",\n      type: \"uuidv4\",\n      description: \"Groups related evaluation runs (e.g. all unit tests in a CI run)\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"run_id\",\n      type: \"uuidv4\",\n      description: \"Unique identifier for the run being evaluated (format: `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`)\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"created_at\",\n      type: \"timestamp\",\n      constraints: [{ type: \"nullable\", value: false }]\n    }\n  ]}\n/>\n  </Tabs.Tab>\n  <Tabs.Tab>\nCaptures OpenTelemetry traces for monitoring and debugging.\n\n<br />\n<SchemaTable\n  columns={[\n    {\n      name: \"id\",\n      type: \"text\",\n      description: \"Unique trace identifier\",\n      constraints: [\n        { type: \"nullable\", value: false },\n        { type: \"primaryKey\" }\n      ]\n    },\n    {\n      name: \"parentSpanId\",\n      type: \"text\",\n      description: \"ID of the parent span. Null if span is top level\",\n    },\n    {\n      name: \"name\",\n      type: \"text\",\n      description: \"Hierarchical operation name (e.g. `workflow.myWorkflow.execute`, `http.request`, `database.query`)\",\n      constraints: [{ type: \"nullable\", value: false }],\n    },\n    {\n      name: \"traceId\",\n      type: \"text\",\n      description: \"Root trace identifier that groups related spans\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"scope\",\n      type: \"text\",\n      description: \"Library/package/service that created the span (e.g. `@mastra/core`, `express`, `pg`)\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"kind\",\n      type: \"integer\",\n      description: \"`INTERNAL` (0, within process), `CLIENT` (1, outgoing calls), `SERVER` (2, incoming calls), `PRODUCER` (3, async job creation), `CONSUMER` (4, async job processing)\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"attributes\",\n      type: \"jsonb\",\n      description: \"User defined key-value pairs that contain span metadata\",\n    },\n    {\n      name: \"status\",\n      type: \"jsonb\",\n      description: \"JSON object with `code` (UNSET=0, ERROR=1, OK=2) and optional `message`. Example:\",\n      example: {\n        code: 1,\n        message: \"HTTP request failed with status 500\"\n      }\n    },\n    {\n      name: \"events\",\n      type: \"jsonb\",\n      description: \"Time-stamped events that occurred during the span\",\n    },\n    {\n      name: \"links\",\n      type: \"jsonb\",\n      description: \"Links to other related spans\",\n      },\n    {\n      name: \"other\",\n      type: \"text\",\n      description: \"Additional OpenTelemetry span fields as stringified JSON. Example:\",\n      example: {\n        droppedAttributesCount: 2,\n        droppedEventsCount: 1,\n        instrumentationLibrary: \"@opentelemetry/instrumentation-http\"\n      }\n    },\n    {\n      name: \"startTime\",\n      type: \"bigint\",\n      description: \"Nanoseconds since Unix epoch when span started\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"endTime\",\n      type: \"bigint\",\n      description: \"Nanoseconds since Unix epoch when span ended\",\n      constraints: [{ type: \"nullable\", value: false }]\n    },\n    {\n      name: \"createdAt\",\n      type: \"timestamp\",\n      constraints: [{ type: \"nullable\", value: false }]\n    }\n  ]}\n/>\n  </Tabs.Tab>\n</Tabs>\n\n### Querying Messages\n\nMessages are stored in a V2 format internally, which is roughly equivalent to the AI SDK's `UIMessage` format. When querying messages using `getMessages`, you can specify the desired output format, defaulting to `v1` for backwards compatibility:\n\n```typescript copy\n// Get messages in the default V1 format (roughly equivalent to AI SDK's CoreMessage format)\nconst messagesV1 = await mastra.getStorage().getMessages({ threadId: 'your-thread-id' });\n\n// Get messages in the V2 format (roughly equivalent to AI SDK's UIMessage format)\nconst messagesV2 = await mastra.getStorage().getMessages({ threadId: 'your-thread-id', format: 'v2' });\n```\n\nYou can also retrieve messages using an array of message IDs. Note that unlike `getMessages`, this defaults to the V2 format:\n\n```typescript copy\nconst messagesV1 = await mastra.getStorage().getMessagesById({ messageIds: messageIdArr, format: 'v1' });\n\nconst messagesV2 = await mastra.getStorage().getMessagesById({ messageIds: messageIdArr });\n```\n\n## Storage Providers\n\nMastra supports the following providers:\n\n- For local development, check out [LibSQL Storage](../../reference/storage/libsql.mdx)\n- For production, check out [PostgreSQL Storage](../../reference/storage/postgresql.mdx)\n- For serverless deployments, check out [Upstash Storage](../../reference/storage/upstash.mdx)\n- For document-based storage, check out [MongoDB Storage](../../reference/storage/mongodb.mdx)\n\n\n","path":null,"size_bytes":22249,"size_tokens":null},"docs/mastra/06-reference/34_mastra-get-logs.md":{"content":"---\ntitle: \"Reference: Mastra.getLogs() | Core | Mastra Docs\"\ndescription: \"Documentation for the `Mastra.getLogs()` method in Mastra, which retrieves all logs for a specific transport ID.\"\n---\n\n# Mastra.getLogs()\n[EN] Source: https://mastra.ai/en/reference/core/getLogs\n\nThe `.getLogs()` method is used to retrieve all logs for a specific transport ID. This method requires a configured logger that supports the `getLogs` operation.\n\n## Usage example\n\n```typescript copy\nmastra.getLogs(\"456\");\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"transportId\",\n      type: \"string\",\n      description: \"The transport ID to retrieve logs from.\",\n    },\n    {\n      name: \"options\",\n      type: \"object\",\n      description: \"Optional parameters for filtering and pagination. See Options section below for details.\",\n      optional: true,\n    },\n  ]}\n/>\n\n### Options\n\n<PropertiesTable\n  content={[\n    {\n      name: \"fromDate\",\n      type: \"Date\",\n      description: \"Optional start date for filtering logs. e.g., new Date('2024-01-01').\",\n      optional: true,\n    },\n    {\n      name: \"toDate\",\n      type: \"Date\",\n      description: \"Optional end date for filtering logs. e.g., new Date('2024-01-31').\",\n      optional: true,\n    },\n    {\n      name: \"logLevel\",\n      type: \"LogLevel\",\n      description: \"Optional log level to filter by.\",\n      optional: true,\n    },\n    {\n      name: \"filters\",\n      type: \"Record<string, any>\",\n      description: \"Optional additional filters to apply to the log query.\",\n      optional: true,\n    },\n    {\n      name: \"page\",\n      type: \"number\",\n      description: \"Optional page number for pagination.\",\n      optional: true,\n    },\n    {\n      name: \"perPage\",\n      type: \"number\",\n      description: \"Optional number of logs per page for pagination.\",\n      optional: true,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"logs\",\n      type: \"Promise<any>\",\n      description: \"A promise that resolves to the logs for the specified transport ID.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Logging overview](../../docs/observability/logging.mdx)\n- [Logger reference](../../reference/observability/logger.mdx)\n\n\n","path":null,"size_bytes":2192,"size_tokens":null},"src/mastra/index.ts":{"content":"import { Mastra } from \"@mastra/core\";\nimport { MastraError } from \"@mastra/core/error\";\nimport { PinoLogger } from \"@mastra/loggers\";\nimport { LogLevel, MastraLogger } from \"@mastra/core/logger\";\nimport pino from \"pino\";\nimport { MCPServer } from \"@mastra/mcp\";\nimport { NonRetriableError } from \"inngest\";\nimport { z } from \"zod\";\nimport puppeteer from \"puppeteer\";\n\nimport { sharedPostgresStorage } from \"./storage\";\nimport { inngest, inngestServe } from \"./inngest\";\n\nimport { registerTelegramTrigger } from \"../triggers/telegramTriggers\";\nimport { araBrainWorkflow } from \"./workflows/araBrainWorkflow\";\nimport { brainEngine } from \"./tools/brainEngine\";\nimport { generateQuote, getMaterialsList } from \"./tools/guardianPricing\";\n\n// Shared memory storage (in-memory for now)\nlet memoryPhrases: string[] = [\n  \"hello there, how are you doing today?\",\n  \"what's up brother, nice to hear from you\",\n  \"good morning sunshine, hope you slept well\",\n  \"i love you more than words can say\",\n  \"remember that time we stayed up all night talking?\",\n  \"you always know how to make me smile\",\n  \"can't wait to see you again soon\",\n  \"thinking about you right now\",\n  \"you're my favorite person in the world\",\n  \"let's grab coffee sometime this week\",\n  \"missing our late night conversations\",\n  \"you make everything better just by being here\",\n  \"thanks for always being there for me\",\n  \"you're the best thing that ever happened to me\",\n  \"hope your day is as amazing as you are\"\n];\n\n// Conversation history tracking - per session with style adaptation\ninterface ConversationMessage {\n  role: 'user' | 'bot';\n  content: string;\n  timestamp: number;\n}\n\ninterface UserStyle {\n  formality: 'casual' | 'neutral' | 'formal';\n  verbosity: 'brief' | 'medium' | 'detailed';\n  usesEmoji: boolean;\n  usesSlang: boolean;\n  avgWordCount: number;\n  topics: string[];\n}\n\ninterface ConversationSession {\n  messages: ConversationMessage[];\n  createdAt: number;\n  lastActivity: number;\n  userStyle: UserStyle;\n  messageCount: number;\n}\n\nconst conversationSessions: Map<string, ConversationSession> = new Map();\n\nfunction analyzeUserStyle(messages: ConversationMessage[]): UserStyle {\n  const userMessages = messages.filter(m => m.role === 'user');\n  if (userMessages.length === 0) {\n    return { formality: 'neutral', verbosity: 'medium', usesEmoji: false, usesSlang: false, avgWordCount: 10, topics: [] };\n  }\n  \n  const allText = userMessages.map(m => m.content).join(' ');\n  const words = allText.split(/\\s+/);\n  const avgWords = words.length / userMessages.length;\n  \n  const commonEmojis = ['ðŸ˜€', 'ðŸ˜ƒ', 'ðŸ˜„', 'ðŸ˜', 'ðŸ˜Š', 'ðŸ™‚', 'ðŸ˜Ž', 'ðŸ‘', 'ðŸ‘‹', 'â¤ï¸', 'ðŸ”¥', 'âœ¨', 'ðŸŽ‰', 'ðŸ’¯', 'ðŸ™', 'ðŸ˜‚', 'ðŸ¤£', 'ðŸ˜', 'ðŸ¥°', 'ðŸ˜¢', 'ðŸ˜­', 'ðŸ¤”', 'ðŸ‘€', 'ðŸ’ª', 'ðŸŽ¯'];\n  const usesEmoji = commonEmojis.some(e => allText.includes(e));\n  \n  const slangWords = ['lol', 'omg', 'btw', 'idk', 'tbh', 'ngl', 'bruh', 'yo', 'sup', 'gonna', 'wanna', 'gotta', 'kinda', 'sorta'];\n  const usesSlang = slangWords.some(s => allText.toLowerCase().includes(s));\n  \n  const formalWords = ['please', 'kindly', 'would you', 'could you', 'thank you', 'appreciate', 'regards'];\n  const formalCount = formalWords.filter(w => allText.toLowerCase().includes(w)).length;\n  \n  let formality: 'casual' | 'neutral' | 'formal' = 'neutral';\n  if (usesSlang || usesEmoji) formality = 'casual';\n  else if (formalCount >= 2) formality = 'formal';\n  \n  let verbosity: 'brief' | 'medium' | 'detailed' = 'medium';\n  if (avgWords < 8) verbosity = 'brief';\n  else if (avgWords > 20) verbosity = 'detailed';\n  \n  const topicKeywords = ['code', 'help', 'question', 'encrypt', 'pattern', 'remember', 'history'];\n  const topics = topicKeywords.filter(t => allText.toLowerCase().includes(t));\n  \n  return { formality, verbosity, usesEmoji, usesSlang, avgWordCount: Math.round(avgWords), topics };\n}\n\nfunction adaptResponse(response: string, style: UserStyle): string {\n  let adapted = response;\n  \n  if (style.formality === 'casual') {\n    adapted = adapted.replace(/Hello/g, 'Hey').replace(/Certainly/g, 'Sure');\n    if (style.usesEmoji && !adapted.includes('ðŸ˜Š')) {\n      const emojis = ['ðŸ‘', 'ðŸ˜Š', 'âœ¨', 'ðŸ™Œ'];\n      adapted = adapted + ' ' + emojis[Math.floor(Math.random() * emojis.length)];\n    }\n  } else if (style.formality === 'formal') {\n    adapted = adapted.replace(/hey/gi, 'Hello').replace(/yeah/gi, 'Yes');\n  }\n  \n  if (style.verbosity === 'brief' && adapted.length > 100) {\n    const sentences = adapted.split(/[.!?]+/).filter(s => s.trim());\n    if (sentences.length > 2) {\n      adapted = sentences.slice(0, 2).join('. ') + '.';\n    }\n  }\n  \n  return adapted;\n}\n\nexport function getOrCreateSession(sessionId: string): ConversationSession {\n  if (!conversationSessions.has(sessionId)) {\n    conversationSessions.set(sessionId, {\n      messages: [],\n      createdAt: Date.now(),\n      lastActivity: Date.now(),\n      userStyle: { formality: 'neutral', verbosity: 'medium', usesEmoji: false, usesSlang: false, avgWordCount: 10, topics: [] },\n      messageCount: 0\n    });\n  }\n  const session = conversationSessions.get(sessionId)!;\n  session.lastActivity = Date.now();\n  return session;\n}\n\nexport function addToHistory(sessionId: string, role: 'user' | 'bot', content: string) {\n  const session = getOrCreateSession(sessionId);\n  session.messages.push({ role, content, timestamp: Date.now() });\n  session.messageCount++;\n  if (session.messages.length > 100) {\n    session.messages = session.messages.slice(-100);\n  }\n  if (role === 'user' && session.messageCount % 3 === 0) {\n    session.userStyle = analyzeUserStyle(session.messages);\n  }\n}\n\nexport function getHistory(sessionId: string): ConversationMessage[] {\n  return getOrCreateSession(sessionId).messages;\n}\n\nexport function getUserStyle(sessionId: string): UserStyle {\n  return getOrCreateSession(sessionId).userStyle;\n}\n\nexport function getSessionStats(sessionId: string): { messageCount: number; duration: number; style: UserStyle } {\n  const session = getOrCreateSession(sessionId);\n  return {\n    messageCount: session.messageCount,\n    duration: Date.now() - session.createdAt,\n    style: session.userStyle\n  };\n}\n\nexport function getRecentContext(sessionId: string, limit: number = 5): string {\n  const messages = getHistory(sessionId).slice(-limit);\n  return messages.map(m => `${m.role}: ${m.content}`).join('\\n');\n}\n\nexport function findInHistory(sessionId: string, query: string): ConversationMessage[] {\n  const messages = getHistory(sessionId);\n  const queryLower = query.toLowerCase();\n  return messages.filter(m => m.content.toLowerCase().includes(queryLower));\n}\n\nexport function getMemoryPhrases() { return memoryPhrases; }\nexport function addMemoryPhrase(phrase: string) { memoryPhrases.push(phrase.toLowerCase()); }\nexport function deleteMemoryPhrase(index: number) { memoryPhrases.splice(index, 1); }\n\nclass ProductionPinoLogger extends MastraLogger {\n  protected logger: pino.Logger;\n\n  constructor(\n    options: {\n      name?: string;\n      level?: LogLevel;\n    } = {},\n  ) {\n    super(options);\n\n    this.logger = pino({\n      name: options.name || \"app\",\n      level: options.level || LogLevel.INFO,\n      base: {},\n      formatters: {\n        level: (label: string, _number: number) => ({\n          level: label,\n        }),\n      },\n      timestamp: () => `,\"time\":\"${new Date(Date.now()).toISOString()}\"`,\n    });\n  }\n\n  debug(message: string, args: Record<string, any> = {}): void {\n    this.logger.debug(args, message);\n  }\n\n  info(message: string, args: Record<string, any> = {}): void {\n    this.logger.info(args, message);\n  }\n\n  warn(message: string, args: Record<string, any> = {}): void {\n    this.logger.warn(args, message);\n  }\n\n  error(message: string, args: Record<string, any> = {}): void {\n    this.logger.error(args, message);\n  }\n}\n\nexport const mastra = new Mastra({\n  storage: sharedPostgresStorage,\n  workflows: { araBrainWorkflow },\n  agents: {},\n  mcpServers: {\n    allTools: new MCPServer({\n      name: \"allTools\",\n      version: \"1.0.0\",\n      tools: {},\n    }),\n  },\n  bundler: {\n    externals: [\n      \"@slack/web-api\",\n      \"inngest\",\n      \"inngest/hono\",\n      \"hono\",\n      \"hono/streaming\",\n    ],\n    sourcemap: true,\n  },\n  server: {\n    host: \"0.0.0.0\",\n    port: 5000,\n    middleware: [\n      async (c, next) => {\n        const mastra = c.get(\"mastra\");\n        const logger = mastra?.getLogger();\n        logger?.debug(\"[Request]\", { method: c.req.method, url: c.req.url });\n        try {\n          await next();\n        } catch (error) {\n          logger?.error(\"[Response]\", {\n            method: c.req.method,\n            url: c.req.url,\n            error,\n          });\n          if (error instanceof MastraError) {\n            if (error.id === \"AGENT_MEMORY_MISSING_RESOURCE_ID\") {\n              throw new NonRetriableError(error.message, { cause: error });\n            }\n          } else if (error instanceof z.ZodError) {\n            throw new NonRetriableError(error.message, { cause: error });\n          }\n\n          throw error;\n        }\n      },\n    ],\n    apiRoutes: [\n      {\n        path: \"/\",\n        method: \"GET\",\n        handler: async (c) => {\n          const html = `<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <title>Ara-Brain</title>\n  <style>\n    * { box-sizing: border-box; margin: 0; padding: 0; }\n    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background: #1a1a2e; color: #eee; min-height: 100vh; display: flex; flex-direction: column; align-items: center; padding: 20px; }\n    h1 { color: #a855f7; margin-bottom: 5px; }\n    .subtitle { color: #888; margin-bottom: 20px; font-size: 14px; }\n    .container { width: 100%; max-width: 600px; background: #16213e; border-radius: 16px; padding: 20px; box-shadow: 0 4px 20px rgba(0,0,0,0.3); min-height: 500px; }\n    .panel { display: block; }\n    .controls { display: flex; gap: 10px; margin-bottom: 15px; justify-content: center; }\n    .control-btn { padding: 8px 16px; background: #2d3748; color: #eee; border: none; border-radius: 20px; cursor: pointer; font-size: 13px; display: flex; align-items: center; gap: 6px; transition: all 0.2s; }\n    .control-btn:hover { background: #3d4758; }\n    .control-btn.muted { background: #dc2626; }\n    .messages { height: 320px; overflow-y: auto; margin-bottom: 15px; padding: 10px; background: #0f0f23; border-radius: 12px; }\n    .message { padding: 10px 14px; margin: 6px 0; border-radius: 12px; max-width: 85%; word-wrap: break-word; font-size: 15px; }\n    .user { background: #a855f7; margin-left: auto; text-align: right; }\n    .bot { background: #2d3748; }\n    .input-area { display: flex; gap: 8px; }\n    input { flex: 1; padding: 12px 16px; border: none; border-radius: 12px; background: #0f0f23; color: #eee; font-size: 15px; outline: none; font-family: inherit; }\n    input:focus { box-shadow: 0 0 0 2px #a855f7; }\n    button { padding: 12px 18px; background: #a855f7; color: white; border: none; border-radius: 12px; cursor: pointer; font-size: 15px; font-weight: 600; transition: background 0.2s; }\n    button:hover { background: #9333ea; }\n    button:disabled { background: #555; cursor: not-allowed; }\n    .mic-btn { background: #22c55e; min-width: 48px; padding: 12px; }\n    .mic-btn:hover { background: #16a34a; }\n    .mic-btn.listening { background: #dc2626; animation: pulse 1s infinite; }\n    @keyframes pulse { 0%, 100% { transform: scale(1); } 50% { transform: scale(1.05); } }\n    .typing { color: #888; font-style: italic; padding: 8px; }\n    .status { text-align: center; color: #888; font-size: 12px; margin-top: 10px; min-height: 18px; }\n  </style>\n</head>\n<body>\n  <h1>Ara-Brain</h1>\n  <p class=\"subtitle\">Voice-enabled memory bot</p>\n  \n  <div class=\"container\" style=\"border-radius: 16px;\">\n    <div id=\"chatPanel\" class=\"panel active\">\n      <div class=\"controls\">\n        <button class=\"control-btn\" id=\"muteBtn\" onclick=\"toggleMute()\">\n          <span id=\"muteIcon\">ðŸ”Š</span> <span id=\"muteText\">Sound On</span>\n        </button>\n      </div>\n      <div class=\"messages\" id=\"messages\"></div>\n      <div class=\"input-area\">\n        <button class=\"mic-btn\" id=\"micBtn\" onclick=\"toggleListening()\">ðŸŽ¤</button>\n        <input type=\"text\" id=\"input\" placeholder=\"Type or speak...\" onkeypress=\"if(event.key==='Enter')sendMessage()\">\n        <button onclick=\"sendMessage()\" id=\"sendBtn\">Send</button>\n      </div>\n      <div class=\"status\" id=\"status\"></div>\n    </div>\n  </div>\n  \n  <script>\n    let isMuted = false;\n    let isListening = false;\n    let recognition = null;\n    \n    const messages = document.getElementById('messages');\n    const input = document.getElementById('input');\n    const sendBtn = document.getElementById('sendBtn');\n    const micBtn = document.getElementById('micBtn');\n    const status = document.getElementById('status');\n    \n    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (SpeechRecognition) {\n      recognition = new SpeechRecognition();\n      recognition.continuous = false;\n      recognition.interimResults = true;\n      recognition.lang = 'en-US';\n      recognition.onstart = () => { isListening = true; micBtn.classList.add('listening'); status.textContent = 'Listening...'; };\n      recognition.onresult = (event) => {\n        let transcript = '';\n        for (let i = event.resultIndex; i < event.results.length; i++) transcript += event.results[i][0].transcript;\n        input.value = transcript;\n        if (event.results[event.results.length - 1].isFinal) { status.textContent = 'Got it!'; setTimeout(() => sendMessage(), 300); }\n      };\n      recognition.onerror = (event) => { status.textContent = 'Error: ' + event.error; stopListening(); };\n      recognition.onend = () => stopListening();\n    } else { micBtn.style.display = 'none'; }\n    \n    function toggleListening() { if (!recognition) return; isListening ? recognition.stop() : recognition.start(); }\n    function stopListening() { isListening = false; micBtn.classList.remove('listening'); setTimeout(() => { if (!isListening) status.textContent = ''; }, 2000); }\n    function toggleMute() {\n      isMuted = !isMuted;\n      document.getElementById('muteIcon').textContent = isMuted ? 'ðŸ”‡' : 'ðŸ”Š';\n      document.getElementById('muteText').textContent = isMuted ? 'Sound Off' : 'Sound On';\n      document.getElementById('muteBtn').classList.toggle('muted', isMuted);\n      if (isMuted) window.speechSynthesis.cancel();\n    }\n    function speak(text) { if (isMuted || !window.speechSynthesis) return; window.speechSynthesis.cancel(); const u = new SpeechSynthesisUtterance(text); u.rate = 1; u.pitch = 1; window.speechSynthesis.speak(u); }\n    function addMessage(text, isUser) { const div = document.createElement('div'); div.className = 'message ' + (isUser ? 'user' : 'bot'); div.textContent = text; messages.appendChild(div); messages.scrollTop = messages.scrollHeight; }\n    \n    async function sendMessage() {\n      const text = input.value.trim();\n      if (!text) return;\n      addMessage(text, true);\n      input.value = '';\n      sendBtn.disabled = true;\n      micBtn.disabled = true;\n      const typing = document.createElement('div');\n      typing.className = 'typing';\n      typing.textContent = 'Thinking...';\n      messages.appendChild(typing);\n      try {\n        const res = await fetch('/chat', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ message: text }) });\n        const data = await res.json();\n        typing.remove();\n        addMessage(data.response, false);\n        speak(data.response);\n      } catch (err) { typing.remove(); addMessage('Error: ' + err.message, false); }\n      sendBtn.disabled = false;\n      micBtn.disabled = false;\n      input.focus();\n    }\n    \n    input.focus();\n  </script>\n</body>\n</html>`;\n          return c.html(html);\n        },\n      },\n      {\n        path: \"/chat\",\n        method: \"POST\",\n        handler: async (c) => {\n          const mastra = c.get(\"mastra\");\n          const logger = mastra?.getLogger();\n          try {\n            const { message, sessionId = 'web-default' } = await c.req.json();\n            logger?.info(\"ðŸ’¬ [Chat] Received message:\", { message, sessionId });\n            \n            addToHistory(sessionId, 'user', message);\n            \n            const searchLower = message.toLowerCase().trim();\n            let response = \"\";\n            let foundMatch = false;\n            \n            const historyKeywords = ['you said', 'i said', 'earlier', 'before', 'remember when', 'what did', 'we talked'];\n            const isHistoryQuery = historyKeywords.some(k => searchLower.includes(k));\n            \n            if (isHistoryQuery) {\n              const history = getHistory(sessionId);\n              if (history.length > 1) {\n                const recentContext = history.slice(-6, -1);\n                if (searchLower.includes('you said') || searchLower.includes('what did you')) {\n                  const botMessages = recentContext.filter(m => m.role === 'bot');\n                  if (botMessages.length > 0) {\n                    response = `I said: \"${botMessages[botMessages.length - 1].content}\"`;\n                    foundMatch = true;\n                  }\n                } else if (searchLower.includes('i said') || searchLower.includes('what did i')) {\n                  const userMessages = recentContext.filter(m => m.role === 'user');\n                  if (userMessages.length > 0) {\n                    response = `You said: \"${userMessages[userMessages.length - 1].content}\"`;\n                    foundMatch = true;\n                  }\n                } else {\n                  response = `Our recent conversation:\\n${recentContext.map(m => `${m.role}: ${m.content}`).join('\\n')}`;\n                  foundMatch = true;\n                }\n              }\n            }\n            \n            if (!foundMatch) {\n              const brainResult = brainEngine.process(message);\n              if (brainResult.confidence > 0.3 && brainResult.memoryHits > 0) {\n                response = brainResult.response;\n                foundMatch = true;\n                logger?.info(\"ðŸ§  [Brain] Processed:\", { \n                  confidence: brainResult.confidence, \n                  memoryHits: brainResult.memoryHits,\n                  reasoning: brainResult.reasoning \n                });\n              } else {\n                const match = memoryPhrases.find((line) => line.includes(searchLower));\n                if (match) {\n                  response = match;\n                  foundMatch = true;\n                } else {\n                  response = \"got it. what now, brother?\";\n                }\n              }\n            }\n            \n            const userStyle = getUserStyle(sessionId);\n            response = adaptResponse(response, userStyle);\n            \n            addToHistory(sessionId, 'bot', response);\n            \n            brainEngine.saveInteraction(message, response, 'conversations');\n            \n            const stats = getSessionStats(sessionId);\n            logger?.info(\"âœ… [Chat] Response:\", { response, foundMatch, stats });\n            return c.json({ response, foundMatch, historyCount: stats.messageCount, style: stats.style });\n          } catch (error) {\n            logger?.error(\"âŒ [Chat] Error:\", { error });\n            return c.json({ response: \"Error processing message\", foundMatch: false }, 500);\n          }\n        },\n      },\n      {\n        path: \"/chat/history\",\n        method: \"GET\",\n        handler: async (c) => {\n          const sessionId = c.req.query('sessionId') || 'web-default';\n          const history = getHistory(sessionId);\n          return c.json({ history, count: history.length });\n        },\n      },\n      {\n        path: \"/chat/stats\",\n        method: \"GET\",\n        handler: async (c) => {\n          const sessionId = c.req.query('sessionId') || 'web-default';\n          const stats = getSessionStats(sessionId);\n          return c.json({ \n            sessionId,\n            messageCount: stats.messageCount,\n            durationMs: stats.duration,\n            durationMinutes: Math.round(stats.duration / 60000),\n            style: stats.style\n          });\n        },\n      },\n      {\n        path: \"/brain/stats\",\n        method: \"GET\",\n        handler: async (c) => {\n          const stats = brainEngine.getStats();\n          const categories = brainEngine.getCategories();\n          return c.json({ \n            brain: stats,\n            categories,\n            status: 'active'\n          });\n        },\n      },\n      {\n        path: \"/brain/search\",\n        method: \"POST\",\n        handler: async (c) => {\n          const { query, limit = 5 } = await c.req.json();\n          const result = brainEngine.process(query);\n          return c.json({\n            response: result.response,\n            confidence: result.confidence,\n            memoryHits: result.memoryHits,\n            reasoning: result.reasoning\n          });\n        },\n      },\n      {\n        path: \"/brain/category\",\n        method: \"GET\",\n        handler: async (c) => {\n          const category = c.req.query('name') || 'general';\n          const nodes = brainEngine.searchByCategory(category);\n          return c.json({\n            category,\n            count: nodes.length,\n            items: nodes.slice(0, 20).map(n => ({ content: n.content, weight: n.weight }))\n          });\n        },\n      },\n      {\n        path: \"/brain/learn\",\n        method: \"POST\",\n        handler: async (c) => {\n          const mastra = c.get(\"mastra\");\n          const logger = mastra?.getLogger();\n          const { userInput, botResponse, category = 'learned' } = await c.req.json();\n          logger?.info(\"ðŸ“ [Brain/Learn] Saving interaction:\", { userInput: userInput?.substring(0, 50) });\n          const result = brainEngine.saveInteraction(userInput, botResponse, category);\n          return c.json({ success: result.success, message: result.message, memorySize: result.newSize });\n        },\n      },\n      {\n        path: \"/brain/encrypt\",\n        method: \"POST\",\n        handler: async (c) => {\n          const mastra = c.get(\"mastra\");\n          const logger = mastra?.getLogger();\n          logger?.info(\"ðŸ” [Brain/Encrypt] Encrypting memory file\");\n          const result = brainEngine.enableEncryption();\n          return c.json(result);\n        },\n      },\n      {\n        path: \"/brain/decrypt\",\n        method: \"POST\",\n        handler: async (c) => {\n          const mastra = c.get(\"mastra\");\n          const logger = mastra?.getLogger();\n          logger?.info(\"ðŸ”“ [Brain/Decrypt] Decrypting memory file\");\n          const result = brainEngine.disableEncryption();\n          return c.json(result);\n        },\n      },\n      {\n        path: \"/brain/status\",\n        method: \"GET\",\n        handler: async (c) => {\n          return c.json({\n            encrypted: brainEngine.isEncrypted(),\n            memoryPath: brainEngine.getMemoryPath(),\n            stats: brainEngine.getStats(),\n            cognitive: brainEngine.getCognitiveStats()\n          });\n        },\n      },\n      // REASONING MODULE\n      {\n        path: \"/brain/reason\",\n        method: \"POST\",\n        handler: async (c) => {\n          const mastra = c.get(\"mastra\");\n          const logger = mastra?.getLogger();\n          try {\n            const body = await c.req.json();\n            const { premise, facts } = body || {};\n            logger?.info(\"ðŸ§  [Reason] Processing:\", { premise });\n            \n            if (facts && Array.isArray(facts) && facts.length > 0) {\n              const result = brainEngine.deduct(facts.map(f => String(f)));\n              return c.json({ type: 'deduction', ...result });\n            }\n            \n            if (!premise || typeof premise !== 'string') {\n              return c.json({ error: 'Provide a premise string or facts array' }, 400);\n            }\n            \n            const result = brainEngine.infer(premise);\n            return c.json({ type: 'inference', ...result });\n          } catch (e) {\n            return c.json({ error: 'Invalid request body' }, 400);\n          }\n        },\n      },\n      {\n        path: \"/brain/causal\",\n        method: \"POST\",\n        handler: async (c) => {\n          try {\n            const { cause, effect } = await c.req.json();\n            if (!cause || !effect || typeof cause !== 'string' || typeof effect !== 'string') {\n              return c.json({ error: 'Provide cause and effect strings' }, 400);\n            }\n            brainEngine.addCausalRelation(cause, effect);\n            return c.json({ success: true, message: `Added: ${cause} â†’ ${effect}` });\n          } catch (e) {\n            return c.json({ error: 'Invalid request body' }, 400);\n          }\n        },\n      },\n      {\n        path: \"/brain/rule\",\n        method: \"POST\",\n        handler: async (c) => {\n          try {\n            const { conditions, conclusion, confidence = 0.8 } = await c.req.json();\n            if (!Array.isArray(conditions) || conditions.length === 0) {\n              return c.json({ error: 'conditions must be a non-empty array' }, 400);\n            }\n            if (!conclusion || typeof conclusion !== 'string') {\n              return c.json({ error: 'conclusion must be a string' }, 400);\n            }\n            brainEngine.addInferenceRule(conditions.map(c => String(c)), conclusion, Number(confidence) || 0.8);\n            return c.json({ success: true, message: `Added rule: IF [${conditions.join(', ')}] THEN [${conclusion}]` });\n          } catch (e) {\n            return c.json({ error: 'Invalid request body' }, 400);\n          }\n        },\n      },\n      // LEARNING MODULE\n      {\n        path: \"/brain/pattern\",\n        method: \"POST\",\n        handler: async (c) => {\n          try {\n            const { sequence } = await c.req.json();\n            if (!Array.isArray(sequence) || sequence.length === 0) {\n              return c.json({ error: 'sequence must be a non-empty array' }, 400);\n            }\n            const result = brainEngine.learnPattern(sequence.map(s => String(s)));\n            return c.json(result);\n          } catch (e) {\n            return c.json({ error: 'Invalid request body' }, 400);\n          }\n        },\n      },\n      {\n        path: \"/brain/reinforce\",\n        method: \"POST\",\n        handler: async (c) => {\n          try {\n            const { concept, strength = 1.0 } = await c.req.json();\n            if (!concept || typeof concept !== 'string') {\n              return c.json({ error: 'concept must be a string' }, 400);\n            }\n            brainEngine.reinforceConcept(concept, Math.min(5, Math.max(0, Number(strength) || 1.0)));\n            return c.json({ success: true, message: `Reinforced: ${concept}` });\n          } catch (e) {\n            return c.json({ error: 'Invalid request body' }, 400);\n          }\n        },\n      },\n      {\n        path: \"/brain/associations\",\n        method: \"GET\",\n        handler: async (c) => {\n          const concept = c.req.query('concept') || '';\n          const associations = brainEngine.getAssociations(concept);\n          return c.json({ concept, associations });\n        },\n      },\n      // EPISODIC MEMORY MODULE\n      {\n        path: \"/brain/episode\",\n        method: \"POST\",\n        handler: async (c) => {\n          try {\n            const { input, output, outcome = 'neutral' } = await c.req.json();\n            if (!input || typeof input !== 'string') {\n              return c.json({ error: 'input must be a string' }, 400);\n            }\n            if (!output || typeof output !== 'string') {\n              return c.json({ error: 'output must be a string' }, 400);\n            }\n            const validOutcomes = ['success', 'failure', 'neutral'];\n            const safeOutcome = validOutcomes.includes(outcome) ? outcome : 'neutral';\n            brainEngine.recordEpisode(input, output, safeOutcome as 'success' | 'failure' | 'neutral');\n            return c.json({ success: true, message: 'Episode recorded' });\n          } catch (e) {\n            return c.json({ error: 'Invalid request body' }, 400);\n          }\n        },\n      },\n      {\n        path: \"/brain/episodes\",\n        method: \"GET\",\n        handler: async (c) => {\n          const query = c.req.query('query') || '';\n          const limit = parseInt(c.req.query('limit') || '5');\n          const episodes = brainEngine.recallEpisodes(query, limit);\n          return c.json({ query, episodes });\n        },\n      },\n      {\n        path: \"/brain/context\",\n        method: \"POST\",\n        handler: async (c) => {\n          try {\n            const { context } = await c.req.json();\n            if (!Array.isArray(context)) {\n              return c.json({ error: 'context must be an array' }, 400);\n            }\n            brainEngine.setContext(context.map(c => String(c)).slice(0, 20));\n            return c.json({ success: true, context: brainEngine.getContext() });\n          } catch (e) {\n            return c.json({ error: 'Invalid request body' }, 400);\n          }\n        },\n      },\n      // PROBLEM SOLVING MODULE\n      {\n        path: \"/brain/goal\",\n        method: \"POST\",\n        handler: async (c) => {\n          try {\n            const { goal } = await c.req.json();\n            if (!goal || typeof goal !== 'string') {\n              return c.json({ error: 'goal must be a string' }, 400);\n            }\n            brainEngine.setGoal(goal.substring(0, 500));\n            return c.json({ success: true, progress: brainEngine.getProblemProgress() });\n          } catch (e) {\n            return c.json({ error: 'Invalid request body' }, 400);\n          }\n        },\n      },\n      {\n        path: \"/brain/solve\",\n        method: \"POST\",\n        handler: async (c) => {\n          const mastra = c.get(\"mastra\");\n          const logger = mastra?.getLogger();\n          try {\n            const { problem } = await c.req.json();\n            if (!problem || typeof problem !== 'string') {\n              return c.json({ error: 'problem must be a string' }, 400);\n            }\n            logger?.info(\"ðŸ”§ [Solve] Processing:\", { problem });\n            const result = brainEngine.solve(problem.substring(0, 1000));\n            return c.json(result);\n          } catch (e) {\n            return c.json({ error: 'Invalid request body' }, 400);\n          }\n        },\n      },\n      {\n        path: \"/brain/decompose\",\n        method: \"POST\",\n        handler: async (c) => {\n          try {\n            const { problem } = await c.req.json();\n            if (!problem || typeof problem !== 'string') {\n              return c.json({ error: 'problem must be a string' }, 400);\n            }\n            const result = brainEngine.decompose(problem.substring(0, 1000));\n            return c.json(result);\n          } catch (e) {\n            return c.json({ error: 'Invalid request body' }, 400);\n          }\n        },\n      },\n      {\n        path: \"/brain/progress\",\n        method: \"GET\",\n        handler: async (c) => {\n          return c.json(brainEngine.getProblemProgress());\n        },\n      },\n      // CREATING MODULE\n      {\n        path: \"/brain/synthesize\",\n        method: \"POST\",\n        handler: async (c) => {\n          const mastra = c.get(\"mastra\");\n          const logger = mastra?.getLogger();\n          try {\n            const { topics } = await c.req.json();\n            if (!Array.isArray(topics) || topics.length === 0) {\n              return c.json({ error: 'topics must be a non-empty array' }, 400);\n            }\n            logger?.info(\"âœ¨ [Create] Synthesizing:\", { topics });\n            const result = brainEngine.synthesize(topics.map(t => String(t)).slice(0, 10));\n            return c.json(result);\n          } catch (e) {\n            return c.json({ error: 'Invalid request body' }, 400);\n          }\n        },\n      },\n      {\n        path: \"/brain/generate\",\n        method: \"POST\",\n        handler: async (c) => {\n          const mastra = c.get(\"mastra\");\n          const logger = mastra?.getLogger();\n          try {\n            const { prompt, style = 'factual' } = await c.req.json();\n            if (!prompt || typeof prompt !== 'string') {\n              return c.json({ error: 'prompt must be a string' }, 400);\n            }\n            const validStyles = ['factual', 'creative', 'analytical'];\n            const safeStyle = validStyles.includes(style) ? style : 'factual';\n            logger?.info(\"âœ¨ [Create] Generating:\", { prompt, style: safeStyle });\n            const result = brainEngine.generate(prompt.substring(0, 500), safeStyle as 'factual' | 'creative' | 'analytical');\n            return c.json(result);\n          } catch (e) {\n            return c.json({ error: 'Invalid request body' }, 400);\n          }\n        },\n      },\n      {\n        path: \"/brain/template\",\n        method: \"POST\",\n        handler: async (c) => {\n          try {\n            const { name, template, variables } = await c.req.json();\n            if (!name || typeof name !== 'string') {\n              return c.json({ error: 'name must be a string' }, 400);\n            }\n            \n            if (template && typeof template === 'string') {\n              brainEngine.addTemplate(name, template.substring(0, 2000));\n              return c.json({ success: true, message: `Template \"${name}\" added` });\n            }\n            \n            if (variables && typeof variables === 'object') {\n              const result = brainEngine.fillTemplate(name, variables);\n              return c.json({ result });\n            }\n            \n            return c.json({ error: 'Provide template string to add or variables object to fill' }, 400);\n          } catch (e) {\n            return c.json({ error: 'Invalid request body' }, 400);\n          }\n        },\n      },\n      {\n        path: \"/quote\",\n        method: \"POST\",\n        handler: async (c) => {\n          const mastra = c.get(\"mastra\");\n          const logger = mastra?.getLogger();\n          const { request } = await c.req.json();\n          logger?.info(\"ðŸ’° [Quote] Processing request:\", { request });\n          \n          const result = generateQuote(request || '');\n          \n          return c.json({ \n            success: true, \n            quote: {\n              material: result.material,\n              quantity: result.quantity,\n              unitPrice: result.unitPrice,\n              totalPrice: result.totalPrice,\n              weight: result.weight,\n              breakdown: result.breakdown\n            },\n            formatted: result.formatted \n          });\n        },\n      },\n      {\n        path: \"/quote/materials\",\n        method: \"GET\",\n        handler: async (c) => {\n          const materials = getMaterialsList();\n          return c.json({ materials });\n        },\n      },\n      {\n        path: \"/autopost/sites\",\n        method: \"GET\",\n        handler: async (c) => {\n          const sites = [\n            { name: 'Craigslist', url: 'craigslist.org', category: 'general', automatable: true },\n            { name: 'OfferUp', url: 'offerup.com', category: 'general', automatable: true },\n            { name: 'Mercari', url: 'mercari.com', category: 'general', automatable: true },\n            { name: 'eBay', url: 'ebay.com', category: 'auction', automatable: true },\n            { name: 'Facebook Marketplace', url: 'facebook.com/marketplace', category: 'general', automatable: false },\n            { name: 'Poshmark', url: 'poshmark.com', category: 'fashion', automatable: true },\n            { name: 'Etsy', url: 'etsy.com', category: 'handmade', automatable: true },\n            { name: 'Depop', url: 'depop.com', category: 'fashion', automatable: true },\n            { name: 'Swappa', url: 'swappa.com', category: 'tech', automatable: true },\n            { name: 'Reverb', url: 'reverb.com', category: 'music', automatable: true },\n          ];\n          const category = c.req.query('category');\n          const filtered = category ? sites.filter(s => s.category === category) : sites;\n          return c.json({ sites: filtered, total: filtered.length });\n        },\n      },\n      {\n        path: \"/autopost/generate\",\n        method: \"POST\",\n        handler: async (c) => {\n          const { title, description, template = 'forsale', price, location } = await c.req.json();\n          \n          const templates: Record<string, string> = {\n            cashapp: `ðŸ’° INSTANT CASH - GET PAID TODAY ðŸ’°\\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\n{{TITLE}}\\n\\n{{DESC}}\\n\\nâœ… Fast payment via Cash App\\nâœ… Same-day pickup available\\nðŸ“² Contact now`,\n            service: `ðŸ”§ PROFESSIONAL SERVICE ðŸ”§\\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\n{{TITLE}}\\n\\n{{DESC}}\\n\\nðŸ’¼ Licensed & Insured\\nâ­ 5-Star Reviews\\nðŸ“ž Call/Text for Quote`,\n            forsale: `ðŸ·ï¸ FOR SALE ðŸ·ï¸\\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\n{{TITLE}}\\n\\n{{DESC}}\\n\\nðŸ“ Local pickup available\\nðŸ’µ Cash/Venmo/CashApp accepted\\nðŸ“± Message for details`,\n          };\n          \n          let desc = description || '';\n          if (price) desc += `\\n\\nðŸ’µ Price: $${price}`;\n          if (location) desc += `\\nðŸ“ Location: ${location}`;\n          \n          const ad = (templates[template] || templates.forsale).replace('{{TITLE}}', title || 'Item for Sale').replace('{{DESC}}', desc);\n          \n          return c.json({ \n            ad, \n            title,\n            recommendedSites: ['Craigslist', 'OfferUp', 'Mercari', 'eBay', 'Facebook Marketplace'],\n            tips: ['Post during peak hours (7-9 AM, 5-8 PM)', 'Use high-quality photos', 'Respond within 1 hour', 'Renew every 2-3 days']\n          });\n        },\n      },\n      {\n        path: \"/memory\",\n        method: \"GET\",\n        handler: async (c) => {\n          return c.json({ phrases: memoryPhrases });\n        },\n      },\n      {\n        path: \"/memory\",\n        method: \"POST\",\n        handler: async (c) => {\n          const mastra = c.get(\"mastra\");\n          const logger = mastra?.getLogger();\n          try {\n            const { action, phrase, index } = await c.req.json();\n            if (action === 'add' && phrase) {\n              addMemoryPhrase(phrase);\n              logger?.info(\"âž• [Memory] Added phrase:\", { phrase });\n            } else if (action === 'delete' && typeof index === 'number') {\n              const deleted = memoryPhrases[index];\n              deleteMemoryPhrase(index);\n              logger?.info(\"ðŸ—‘ï¸ [Memory] Deleted phrase:\", { deleted });\n            }\n            return c.json({ success: true, phrases: memoryPhrases });\n          } catch (error) {\n            logger?.error(\"âŒ [Memory] Error:\", { error });\n            return c.json({ success: false }, 500);\n          }\n        },\n      },\n      {\n        path: \"/tools/encrypt\",\n        method: \"POST\",\n        handler: async (c) => {\n          const { text, key } = await c.req.json();\n          const shift = Math.abs(parseInt(key)) || 3;\n          const result = text.split('').map((char: string) => {\n            const code = char.charCodeAt(0);\n            return String.fromCharCode((code + shift) % 65536);\n          }).join('');\n          return c.json({ result });\n        },\n      },\n      {\n        path: \"/tools/decrypt\",\n        method: \"POST\",\n        handler: async (c) => {\n          const { text, key } = await c.req.json();\n          const shift = Math.abs(parseInt(key)) || 3;\n          const result = text.split('').map((char: string) => {\n            const code = char.charCodeAt(0);\n            return String.fromCharCode((code - shift + 65536) % 65536);\n          }).join('');\n          return c.json({ result });\n        },\n      },\n      {\n        path: \"/tools/pattern\",\n        method: \"POST\",\n        handler: async (c) => {\n          const { text } = await c.req.json();\n          const patterns: { type: string; value: string }[] = [];\n          \n          const emailRegex = /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+[.][a-zA-Z]{2,}/g;\n          const emails = text.match(emailRegex);\n          if (emails) patterns.push({ type: 'Emails', value: emails.join(', ') });\n          \n          const phoneRegex = /[+]?[0-9][0-9 ()\\-.]{9,}/g;\n          const phones = text.match(phoneRegex);\n          if (phones) patterns.push({ type: 'Phone Numbers', value: phones.map((p: string) => p.trim()).join(', ') });\n          \n          const urlRegex = /https?:[/][/][^ ]+/g;\n          const urls = text.match(urlRegex);\n          if (urls) patterns.push({ type: 'URLs', value: urls.join(', ') });\n          \n          const numberRegex = /[0-9]+/g;\n          const numbers = text.match(numberRegex);\n          if (numbers) patterns.push({ type: 'Numbers', value: numbers.join(', ') });\n          \n          const wordRegex = /[a-z]+/gi;\n          const words = text.toLowerCase().match(wordRegex) || [];\n          const wordCount: { [key: string]: number } = {};\n          words.forEach((w: string) => { wordCount[w] = (wordCount[w] || 0) + 1; });\n          const repeated = Object.entries(wordCount).filter(([_, c]) => c > 1).map(([w, c]) => w + ' (x' + c + ')');\n          if (repeated.length) patterns.push({ type: 'Repeated Words', value: repeated.join(', ') });\n          \n          patterns.push({ type: 'Length', value: text.length + ' characters, ' + words.length + ' words' });\n          \n          const upper = (text.match(/[A-Z]/g) || []).length;\n          const pct = text.length > 0 ? Math.round(upper/text.length*100) : 0;\n          patterns.push({ type: 'Uppercase', value: upper + ' uppercase letters (' + pct + '%)' });\n          \n          if (patterns.length === 2) patterns.unshift({ type: 'Patterns', value: 'No special patterns detected' });\n          \n          return c.json({ patterns });\n        },\n      },\n      {\n        path: \"/tools/script\",\n        method: \"POST\",\n        handler: async (c) => {\n          const { code } = await c.req.json();\n          const logs: string[] = [];\n          const mockConsole = {\n            log: (...args: any[]) => logs.push(args.map(a => typeof a === 'object' ? JSON.stringify(a) : String(a)).join(' ')),\n            error: (...args: any[]) => logs.push('ERROR: ' + args.map(a => typeof a === 'object' ? JSON.stringify(a) : String(a)).join(' ')),\n            warn: (...args: any[]) => logs.push('WARN: ' + args.map(a => typeof a === 'object' ? JSON.stringify(a) : String(a)).join(' ')),\n          };\n          try {\n            const fn = new Function('console', 'Math', 'Date', 'JSON', 'Array', 'Object', 'String', 'Number', code);\n            const result = fn(mockConsole, Math, Date, JSON, Array, Object, String, Number);\n            return c.json({ result, logs });\n          } catch (e: any) {\n            return c.json({ error: e.message, logs });\n          }\n        },\n      },\n      {\n        path: \"/tools/browser\",\n        method: \"POST\",\n        handler: async (c) => {\n          const { url, action } = await c.req.json();\n          let browser;\n          try {\n            browser = await puppeteer.launch({ headless: true, executablePath: process.env.PUPPETEER_EXECUTABLE_PATH || '/nix/store/khk7xpgsm5insk81azy9d560yq4npf77-chromium-131.0.6778.204/bin/chromium', args: ['--no-sandbox', '--disable-setuid-sandbox', '--disable-gpu', '--disable-dev-shm-usage'] });\n            const page = await browser.newPage();\n            await page.setViewport({ width: 1280, height: 720 });\n            await page.goto(url, { waitUntil: 'networkidle2', timeout: 15000 });\n            \n            if (action === 'screenshot') {\n              const screenshot = await page.screenshot({ encoding: 'base64' });\n              await browser.close();\n              return c.json({ screenshot });\n            } else if (action === 'content') {\n              const text = await page.evaluate(() => document.body.innerText);\n              await browser.close();\n              return c.json({ result: text.substring(0, 5000) });\n            } else if (action === 'title') {\n              const title = await page.title();\n              await browser.close();\n              return c.json({ result: title });\n            } else if (action === 'links') {\n              const links = await page.evaluate(() => Array.from(document.querySelectorAll('a')).map(a => a.href).filter(h => h.startsWith('http')));\n              await browser.close();\n              return c.json({ links: [...new Set(links)] });\n            }\n            await browser.close();\n            return c.json({ result: 'Unknown action' });\n          } catch (e: any) {\n            if (browser) await browser.close();\n            return c.json({ error: e.message });\n          }\n        },\n      },\n      {\n        path: \"/api/inngest\",\n        method: \"ALL\",\n        createHandler: async ({ mastra }) => inngestServe({ mastra, inngest }),\n      },\n      ...registerTelegramTrigger({\n        triggerType: \"telegram/message\",\n        handler: async (mastra, triggerInfo) => {\n          const logger = mastra.getLogger();\n          logger?.info(\"ðŸ“¨ [Telegram Trigger] Received message:\", {\n            userName: triggerInfo.params.userName,\n            message: triggerInfo.params.message,\n          });\n\n          const chatId = triggerInfo.payload.message.chat.id;\n          \n          logger?.info(\"ðŸš€ [Telegram Trigger] Starting workflow:\", {\n            chatId,\n            message: triggerInfo.params.message,\n          });\n\n          const run = await araBrainWorkflow.createRunAsync();\n          const result = await run.start({\n            inputData: {\n              message: triggerInfo.params.message,\n              chatId: chatId,\n            },\n          });\n\n          logger?.info(\"âœ… [Telegram Trigger] Workflow completed:\", { result });\n        },\n      }),\n    ],\n  },\n  logger:\n    process.env.NODE_ENV === \"production\"\n      ? new ProductionPinoLogger({\n          name: \"Mastra\",\n          level: \"info\",\n        })\n      : new PinoLogger({\n          name: \"Mastra\",\n          level: \"info\",\n        }),\n});\n\nif (Object.keys(mastra.getWorkflows()).length > 1) {\n  throw new Error(\n    \"More than 1 workflows found. Currently, more than 1 workflows are not supported in the UI, since doing so will cause app state to be inconsistent.\",\n  );\n}\n\nif (Object.keys(mastra.getAgents()).length > 1) {\n  throw new Error(\n    \"More than 1 agents found. Currently, more than 1 agents are not supported in the UI, since doing so will cause app state to be inconsistent.\",\n  );\n}\n","path":null,"size_bytes":46682,"size_tokens":null},"docs/mastra/03-workflows/18_workflow-streaming.md":{"content":"---\ntitle: \"Workflow Streaming | Streaming | Mastra\"\ndescription: \"Learn how to use workflow streaming in Mastra, including handling workflow execution events, step streaming, and workflow integration with agents and tools.\"\n---\n\nimport { Callout } from \"nextra/components\";\n\n# Workflow streaming\n[EN] Source: https://mastra.ai/en/docs/streaming/workflow-streaming\n\nWorkflow streaming in Mastra enables workflows to send incremental results while they execute, rather than waiting until completion. This allows you to surface partial progress, intermediate states, or progressive data directly to users or upstream agents and workflows.\n\nStreams can be written to in two main ways:\n\n- **From within a workflow step**: every workflow step receives a `writer` argument, which is a writable stream you can use to push updates as execution progresses.\n- **From an agent stream**: you can also pipe an agent's `stream` output directly into a workflow step's writer, making it easy to chain agent responses into workflow results without extra glue code.\n\nBy combining writable workflow streams with agent streaming, you gain fine-grained control over how intermediate results flow through your system and into the user experience.\n\n### Using the `writer` argument\n\n<Callout type=\"warning\">\nThe writer is only available when using `streamVNext`.\n</Callout>\n\nThe `writer` argument is passed to a workflow step's `execute` function and can be used to emit custom events, data, or values into the active stream. This enables workflow steps to provide intermediate results or status updates while execution is still in progress.\n\n<Callout type=\"warning\">\nYou must `await` the call to `writer.write(...)` or else you will lock the stream and get a `WritableStream is locked` error.\n</Callout>\n\n```typescript {5,8,15} showLineNumbers copy\nimport { createStep } from \"@mastra/core/workflows\";\n\nexport const testStep = createStep({\n  // ...\n  execute: async ({ inputData, writer }) => {\n     const { value } = inputData;\n\n    await writer?.write({\n      type: \"custom-event\",\n      status: \"pending\"\n    });\n\n    const response = await fetch(...);\n\n   await writer?.write({\n      type: \"custom-event\",\n      status: \"success\"\n    });\n\n    return {\n      value: \"\"\n    };\n  },\n});\n```\n\n### Inspecting workflow stream payloads\n\nEvents written to the stream are included in the emitted chunks. These chunks can be inspected to access any custom fields, such as event types, intermediate values, or step-specific data.\n\n```typescript showLineNumbers copy\nconst testWorkflow = mastra.getWorkflow(\"testWorkflow\");\n\nconst run = await testWorkflow.createRunAsync();\n\nconst stream = await run.streamVNext({\n  inputData: {\n    value: \"initial data\"\n  }\n});\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n\nif (result!.status === \"suspended\") {\n  // if the workflow is suspended, we can resume it with the resumeStreamVNext method\n  const resumedStream = await run.resumeStreamVNext({\n    resumeData: { value: \"resume data\" }\n  });\n\n  for await (const chunk of resumedStream) {\n    console.log(chunk);\n  }\n}\n```\n\n### Resuming an interrupted workflow stream\n\nIf a workflow stream is closed or interrupted for any reason, you can resume it with the `resumeStreamVNext` method. This will return a new `ReadableStream` that you can use to observe the workflow events.\n\n```typescript showLineNumbers copy\nconst newStream = await run.resumeStreamVNext();\n\nfor await (const chunk of newStream) {\n  console.log(chunk);\n}\n```\n\n## Workflow using an agent\n\nPipe an agent's `textStream` to the workflow step's `writer`. This streams partial output, and Mastra automatically aggregates the agent's usage into the workflow run.\n\n```typescript showLineNumbers copy\nimport { createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nexport const testStep = createStep({\n  // ...\n  execute: async ({ inputData, mastra, writer  }) => {\n    const { city } = inputData\n\n    const testAgent = mastra?.getAgent(\"testAgent\");\n    const stream = await testAgent?.stream(`What is the weather in ${city}$?`);\n\n    await stream!.textStream.pipeTo(writer!);\n\n    return {\n      value: await stream!.text,\n    };\n  },\n});\n```\n\n\n","path":null,"size_bytes":4198,"size_tokens":null},"docs/mastra/03-workflows/03_human-in-loop.md":{"content":"---\ntitle: \"Human in the Loop | Workflows | Mastra Docs\"\ndescription: Example of using Mastra to create workflows with multi-turn human/agent interaction points using suspend/resume and dountil methods.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Human-in-the-loop\n[EN] Source: https://mastra.ai/en/docs/workflows/human-in-the-loop\n\nHuman-in-the-loop workflows enable ongoing interaction between humans and AI agents, allowing for complex decision-making processes that require multiple rounds of input and response. These workflows can suspend execution at specific points, wait for human input, and continue processing based on the responses received.\n\nIn this example, the multi-turn workflow is used to create a Heads Up game that demonstrates how to create interactive workflows using suspend/resume functionality and conditional logic with `dountil` to repeat a workflow step until a specific condition is met.\n\nThis example consists of three main components:\n\n1. A [**Famous Person Agent**](#famous-person-agent) that generates a famous person's name.\n2. A [**Game Agent**](#game-agent) that handles the gameplay.\n3. A [**Multi-Turn Workflow**](#multi-turn-workflow) that orchestrates the interaction.\n\n## Prerequisites\n\nThis example uses the `openai` model. Make sure to add the following to your `.env` file:\n\n```bash filename=\".env\" copy\nOPENAI_API_KEY=<your-api-key>\n```\n\n## Famous person agent\n\nThe `famousPersonAgent` generates a unique name each time the game is played, using semantic memory to avoid repeating suggestions.\n\n```typescript filename=\"src/mastra/agents/example-famous-person-agent.ts\" showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { Memory } from \"@mastra/memory\";\nimport { LibSQLVector } from \"@mastra/libsql\";\n\nexport const famousPersonAgent = new Agent({\n  name: \"Famous Person Generator\",\n  instructions: `You are a famous person generator for a \"Heads Up\" guessing game.\n\nGenerate the name of a well-known famous person who:\n- Is recognizable to most people\n- Has distinctive characteristics that can be described with yes/no questions\n- Is appropriate for all audiences\n- Has a clear, unambiguous name\n\nIMPORTANT: Use your memory to check what famous people you've already suggested and NEVER repeat a person you've already suggested.\n\nExamples: Albert Einstein, BeyoncÃ©, Leonardo da Vinci, Oprah Winfrey, Michael Jordan\n\nReturn only the person's name, nothing else.`,\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    vector: new LibSQLVector({\n      connectionUrl: \"file:../mastra.db\"\n    }),\n    embedder: openai.embedding(\"text-embedding-3-small\"),\n    options: {\n      lastMessages: 5,\n      semanticRecall: {\n        topK: 10,\n        messageRange: 1\n      }\n    }\n  })\n});\n```\n\n> See [Agent](../../reference/agents/agent.mdx) for a full list of configuration options.\n\n## Game agent\n\nThe `gameAgent` handles user interactions by responding to questions and validating guesses.\n\n```typescript filename=\"src/mastra/agents/example-game-agent.ts\" showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nexport const gameAgent = new Agent({\n  name: \"Game Agent\",\n  instructions: `You are a helpful game assistant for a \"Heads Up\" guessing game.\n\nCRITICAL: You know the famous person's name but you must NEVER reveal it in any response.\n\nWhen a user asks a question about the famous person:\n- Answer truthfully based on the famous person provided\n- Keep responses concise and friendly\n- NEVER mention the person's name, even if it seems natural\n- NEVER reveal gender, nationality, or other characteristics unless specifically asked about them\n- Answer yes/no questions with clear \"Yes\" or \"No\" responses\n- Be consistent - same question asked differently should get the same answer\n- Ask for clarification if a question is unclear\n- If multiple questions are asked at once, ask them to ask one at a time\n\nWhen they make a guess:\n- If correct: Congratulate them warmly\n- If incorrect: Politely correct them and encourage them to try again\n\nEncourage players to make a guess when they seem to have enough information.\n\nYou must return a JSON object with:\n- response: Your response to the user\n- gameWon: true if they guessed correctly, false otherwise`,\n  model: openai(\"gpt-4o\")\n});\n```\n\n\n## Multi-turn workflow\n\nThe workflow coordinates the full interaction using `suspend`/`resume` to pause for human input and `dountil` to repeat the game loop until a condition is met.\n\nThe `startStep` generates a name using the `famousPersonAgent`, while the `gameStep` runs the interaction through the `gameAgent`, which handles both questions and guesses and produces structured output that includes a `gameWon` boolean.\n\n```typescript filename=\"src/mastra/workflows/example-heads-up-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from '@mastra/core/workflows';\nimport { z } from 'zod';\n\nconst startStep = createStep({\n  id: 'start-step',\n  description: 'Get the name of a famous person',\n  inputSchema: z.object({\n    start: z.boolean(),\n  }),\n  outputSchema: z.object({\n    famousPerson: z.string(),\n    guessCount: z.number(),\n  }),\n  execute: async ({ mastra }) => {\n    const agent = mastra.getAgent('famousPersonAgent');\n    const response = await agent.generate(\"Generate a famous person's name\", {\n      temperature: 1.2,\n      topP: 0.9,\n      memory: {\n        resource: 'heads-up-game',\n        thread: 'famous-person-generator',\n      },\n    });\n    const famousPerson = response.text.trim();\n    return { famousPerson, guessCount: 0 };\n  },\n});\n\nconst gameStep = createStep({\n  id: 'game-step',\n  description: 'Handles the question-answer-continue loop',\n  inputSchema: z.object({\n    famousPerson: z.string(),\n    guessCount: z.number(),\n  }),\n  resumeSchema: z.object({\n    userMessage: z.string(),\n  }),\n  suspendSchema: z.object({\n    suspendResponse: z.string(),\n  }),\n  outputSchema: z.object({\n    famousPerson: z.string(),\n    gameWon: z.boolean(),\n    agentResponse: z.string(),\n    guessCount: z.number(),\n  }),\n  execute: async ({ inputData, mastra, resumeData, suspend }) => {\n    let { famousPerson, guessCount } = inputData;\n    const { userMessage } = resumeData ?? {};\n\n    if (!userMessage) {\n      return await suspend({\n        suspendResponse: \"I'm thinking of a famous person. Ask me yes/no questions to figure out who it is!\",\n      });\n    }\n\n    const agent = mastra.getAgent('gameAgent');\n    const response = await agent.generate(\n      `\n        The famous person is: ${famousPerson}\n        The user said: \"${userMessage}\"\n        Please respond appropriately. If this is a guess, tell me if it's correct.\n      `,\n      {\n        structuredOutput: {\n          schema: z.object({\n            response: z.string(),\n            gameWon: z.boolean(),\n          })\n        },\n      },\n    );\n\n    const { response: agentResponse, gameWon } = response.object;\n\n    guessCount++;\n\n    return { famousPerson, gameWon, agentResponse, guessCount };\n  },\n});\n\nconst winStep = createStep({\n  id: 'win-step',\n  description: 'Handle game win logic',\n  inputSchema: z.object({\n    famousPerson: z.string(),\n    gameWon: z.boolean(),\n    agentResponse: z.string(),\n    guessCount: z.number(),\n  }),\n  outputSchema: z.object({\n    famousPerson: z.string(),\n    gameWon: z.boolean(),\n    guessCount: z.number(),\n  }),\n  execute: async ({ inputData }) => {\n    const { famousPerson, gameWon, guessCount } = inputData;\n\n    console.log('famousPerson: ', famousPerson);\n    console.log('gameWon: ', gameWon);\n    console.log('guessCount: ', guessCount);\n\n    return { famousPerson, gameWon, guessCount };\n  },\n});\n\nexport const headsUpWorkflow = createWorkflow({\n  id: 'heads-up-workflow',\n  inputSchema: z.object({\n    start: z.boolean(),\n  }),\n  outputSchema: z.object({\n    famousPerson: z.string(),\n    gameWon: z.boolean(),\n    guessCount: z.number(),\n  }),\n})\n  .then(startStep)\n  .dountil(gameStep, async ({ inputData: { gameWon } }) => gameWon)\n  .then(winStep)\n  .commit();\n\n```\n> See [Workflow](../../reference/workflows/workflow.mdx) for a full list of configuration options.\n\n## Registering the agents and workflow\n\nTo use a workflow or an agent, register them in your main Mastra instance.\n\n```typescript filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\n\nimport { headsUpWorkflow } from \"./workflows/example-heads-up-workflow\";\nimport { famousPersonAgent } from \"./agents/example-famous-person-agent\";\nimport { gameAgent } from \"./agents/example-game-agent\";\n\nexport const mastra = new Mastra({\n  workflows: { headsUpWorkflow },\n  agents: { famousPersonAgent, gameAgent }\n});\n```\n\n<GithubLink\n  marginTop='mt-16'\n  link=\"https://github.com/mastra-ai/mastra/blob/main/examples/heads-up-game/\"\n/>\n\n## Related\n\n- [Running Workflows](./running-workflows.mdx)\n- [Control Flow](../../docs/workflows/control-flow.mdx)\n\n\n","path":null,"size_bytes":8988,"size_tokens":null},"docs/mastra/06-reference/51_memory-create-thread.md":{"content":"---\ntitle: \"Reference: Memory.createThread() | Memory | Mastra Docs\"\ndescription: \"Documentation for the `Memory.createThread()` method in Mastra, which creates a new conversation thread in the memory system.\"\n---\n\n# Memory.createThread()\n[EN] Source: https://mastra.ai/en/reference/memory/createThread\n\nThe `.createThread()` method creates a new conversation thread in the memory system. Each thread represents a distinct conversation or context and can contain multiple messages.\n\n## Usage Example\n\n```typescript copy\nawait memory?.createThread({ resourceId: \"user-123\" });\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"resourceId\",\n      type: \"string\",\n      description:\n        \"Identifier for the resource this thread belongs to (e.g., user ID, project ID)\",\n      isOptional: false,\n    },\n    {\n      name: \"threadId\",\n      type: \"string\",\n      description:\n        \"Optional custom ID for the thread. If not provided, one will be generated.\",\n      isOptional: true,\n    },\n    {\n      name: \"title\",\n      type: \"string\",\n      description: \"Optional title for the thread\",\n      isOptional: true,\n    },\n    {\n      name: \"metadata\",\n      type: \"Record<string, unknown>\",\n      description: \"Optional metadata to associate with the thread\",\n      isOptional: true,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"id\",\n      type: \"string\",\n      description: \"Unique identifier of the created thread\",\n    },\n    {\n      name: \"resourceId\",\n      type: \"string\",\n      description: \"Resource ID associated with the thread\",\n    },\n    {\n      name: \"title\",\n      type: \"string\",\n      description: \"Title of the thread (if provided)\",\n    },\n    {\n      name: \"createdAt\",\n      type: \"Date\",\n      description: \"Timestamp when the thread was created\",\n    },\n    {\n      name: \"updatedAt\",\n      type: \"Date\",\n      description: \"Timestamp when the thread was last updated\",\n    },\n    {\n      name: \"metadata\",\n      type: \"Record<string, unknown>\",\n      description: \"Additional metadata associated with the thread\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript filename=\"src/test-memory.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst agent = mastra.getAgent(\"agent\");\nconst memory = await agent.getMemory();\n\nconst thread = await memory?.createThread({\n  resourceId: \"user-123\",\n  title: \"Memory Test Thread\",\n  metadata: {\n    source: \"test-script\",\n    purpose: \"memory-testing\"\n  }\n});\n\nconst response = await agent.generate(\"message for agent\", {\n  memory: {\n    thread: thread!.id,\n    resource: thread!.resourceId\n  }\n});\n\nconsole.log(response.text);\n```\n\n### Related\n\n- [Memory Class Reference](/reference/memory/Memory.mdx)\n- [Getting Started with Memory](/docs/memory/overview.mdx) (Covers threads concept)\n- [getThreadById](/reference/memory/getThreadById.mdx)\n- [getThreadsByResourceId](/reference/memory/getThreadsByResourceId.mdx)\n- [query](/reference/memory/query.mdx)\n\n\n","path":null,"size_bytes":2990,"size_tokens":null},"docs/mastra/03-workflows/04_suspend-resume.md":{"content":"---\ntitle: \"Suspend & Resume Workflows | Human-in-the-Loop | Mastra Docs\"\ndescription: \"Suspend and resume in Mastra workflows allows you to pause execution while waiting for external input or resources.\"\n---\n\n# Suspend & Resume\n[EN] Source: https://mastra.ai/en/docs/workflows/suspend-and-resume\n\nWorkflows can be paused at any step, with their current state persisted as a [snapshot](./snapshots.mdx) in storage. Execution can then be resumed from this saved snapshot when ready. Persisting the snapshot ensures the workflow state is maintained across sessions, deployments, and server restarts, essential for workflows that may remain suspended while awaiting external input or resources.\n\nCommon scenarios for suspending workflows include:\n\n- Waiting for human approval or input\n- Pausing until external API resources become available\n- Collecting additional data needed for later steps\n- Rate limiting or throttling expensive operations\n- Handling event-driven processes with external triggers\n\n> **New to suspend and resume?** Watch these official video tutorials:\n>\n> - **[Mastering Human-in-the-Loop with Suspend & Resume](https://youtu.be/aORuNG8Tq_k)** - Learn how to suspend workflows and accept user inputs\n> - **[Building Multi-Turn Chat Interfaces with React](https://youtu.be/UMVm8YZwlxc)** - Implement multi-turn human-involved interactions with a React chat interface\n\n## Workflow status types\n\nWhen running a workflow, its `status` can be one of the following:\n\n- `running` - The workflow is currently running\n- `suspended` - The workflow is suspended\n- `success` - The workflow has completed\n- `failed` - The workflow has failed\n\n## Suspending a workflow with `suspend()`\n\nTo pause execution at a specific step until user input is received, use the `â suspend` function to temporarily halt the workflow, allowing it to resume only when the necessary data is provided.\n\n![Suspending a workflow with suspend()](/image/workflows/workflows-suspend-resume-suspend.jpg)\n\n```typescript {16} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nconst step1 = createStep({\n  id: \"step-1\",\n  inputSchema: z.object({\n    input: z.string()\n  }),\n  outputSchema: z.object({\n    output: z.string()\n  }),\n  resumeSchema: z.object({\n    city: z.string()\n  }),\n  execute: async ({ resumeData, suspend }) => {\n    const { city } = resumeData ?? {};\n\n    if (!city) {\n      return await suspend({});\n    }\n\n    return { output: \"\" };\n  }\n});\n\nexport const testWorkflow = createWorkflow({\n  // ...\n})\n  .then(step1)\n  .commit();\n```\n\n> For more details, check out the [Suspend workflow example](../../examples/workflows/human-in-the-loop.mdx#suspend-workflow).\n\n### Identifying suspended steps\n\nTo resume a suspended workflow, inspect the `suspended` array in the result to determine which step needs input:\n\n```typescript {15} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst run = await mastra.getWorkflow(\"testWorkflow\").createRunAsync();\n\nconst result = await run.start({\n  inputData: {\n    city: \"London\"\n  }\n});\n\nconsole.log(JSON.stringify(result, null, 2));\n\nif (result.status === \"suspended\") {\n  const resumedResult = await run.resume({\n    step: result.suspended[0],\n    resumeData: {\n      city: \"Berlin\"\n    }\n  });\n}\n```\n\nIn this case, the logic resumes the first step listed in the `suspended` array. A `step` can also be defined using it's `id`, for example: 'step-1'.\n\n```json\n{\n  \"status\": \"suspended\",\n  \"steps\": {\n    // ...\n    \"step-1\": {\n      // ...\n      \"status\": \"suspended\",\n    }\n  },\n  \"suspended\": [\n    [\n      \"step-1\"\n    ]\n  ]\n}\n```\n\n> See [Run Workflow Results](./overview.mdx#run-workflow-results) for more details.\n\n## Providing user feedback with suspend\n\nWhen a workflow is suspended, feedback can be surfaced to the user through the `suspendSchema`. Include a reason in the `suspend` payload to explain why the workflow paused.\n\n```typescript {13,23} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({\n  id: \"step-1\",\n  inputSchema: z.object({\n    value: z.string()\n  }),\n  resumeSchema: z.object({\n    confirm: z.boolean()\n  }),\n  suspendSchema: z.object({\n    reason: z.string()\n  }),\n  outputSchema: z.object({\n    value: z.string()\n  }),\n  execute: async ({ resumeData, suspend }) => {\n    const { confirm } = resumeData ?? {};\n\n    if (!confirm) {\n      return await suspend({\n        reason: \"Confirm to continue\"\n      });\n    }\n\n    return { value: \"\" };\n  }\n});\n\nexport const testWorkflow = createWorkflow({\n  // ...\n})\n  .then(step1)\n  .commit();\n\n```\n\nIn this case, the reason provided explains that the user must confirm to continue.\n\n```json\n{\n  \"step-1\": {\n    // ...\n    \"status\": \"suspended\",\n    \"suspendPayload\": {\n      \"reason\": \"Confirm to continue\"\n    },\n  }\n}\n```\n\n> See [Run Workflow Results](./overview.mdx#run-workflow-results) for more details.\n\n## Resuming a workflow with `resume()`\n\nA workflow can be resumed by calling `resume` and providing the required `resumeData`. You can either explicitly specify which step to resume from, or when exactly one step is suspended, omit the `step` parameter and the workflow will automatically resume that step.\n\n```typescript {16-18} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst run = await mastra.getWorkflow(\"testWorkflow\").createRunAsync();\n\nconst result = await run.start({\n   inputData: {\n    city: \"London\"\n  }\n});\n\nconsole.log(JSON.stringify(result, null, 2));\n\nif (result.status === \"suspended\") {\n  const resumedResult = await run.resume({\n    step: 'step-1',\n    resumeData: {\n      city: \"Berlin\"\n    }\n  });\n\n  console.log(JSON.stringify(resumedResult, null, 2));\n}\n```\n\nYou can also omit the `step` parameter when exactly one step is suspended:\n\n```typescript {5} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nconst resumedResult = await run.resume({\n  resumeData: {\n    city: \"Berlin\"\n  },\n  // step parameter omitted - automatically resumes the single suspended step\n});\n```\n\nYou can pass `runtimeContext` as an argument to both the `start` and `resume` commands.\n\n```typescript filename=\"src/mastra/workflows/test-workflow.ts\"\nimport { RuntimeContext } from \"@mastra/core/runtime-context\";\n\nconst runtimeContext = new RuntimeContext();\n\nconst result = await run.start({\n  step: 'step-1',\n  inputData: {\n    city: \"London\"\n  },\n  runtimeContext\n});\n\nconst resumedResult = await run.resume({\n  step: 'step-1',\n  resumeData: {\n    city: \"New York\"\n  },\n  runtimeContext\n});\n```\n\n> See [Runtime Context](../server-db/runtime-context.mdx) for more information.\n\n### Resuming nested workflows\n\nTo resume a suspended nested workflow pass the workflow instance to the `step` parameter of the `resume` function.\n\n```typescript {33-34} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nconst dowhileWorkflow = createWorkflow({\n  id: 'dowhile-workflow',\n  inputSchema: z.object({ value: z.number() }),\n  outputSchema: z.object({ value: z.number() }),\n})\n  .dountil(\n    createWorkflow({\n      id: 'simple-resume-workflow',\n      inputSchema: z.object({ value: z.number() }),\n      outputSchema: z.object({ value: z.number() }),\n      steps: [incrementStep, resumeStep],\n    })\n      .then(incrementStep)\n      .then(resumeStep)\n      .commit(),\n    async ({ inputData }) => inputData.value >= 10,\n  )\n  .then(\n    createStep({\n      id: 'final',\n      inputSchema: z.object({ value: z.number() }),\n      outputSchema: z.object({ value: z.number() }),\n      execute: async ({ inputData }) => ({ value: inputData.value }),\n    }),\n  )\n  .commit();\n\nconst run = await dowhileWorkflow.createRunAsync();\nconst result = await run.start({ inputData: { value: 0 } });\n\nif (result.status === \"suspended\") {\n  const resumedResult = await run.resume({\n    resumeData: { value: 2 },\n    step: ['simple-resume-workflow', 'resume'],\n  });\n\n  console.log(JSON.stringify(resumedResult, null, 2));\n}\n```\n\n## Sleep & Events\n\nWorkflows can also pause execution for timed delays or external events. These methods set the workflow status to `waiting` rather than `suspended`, and are useful for polling, delayed retries, or event-driven processes.\n\n**Available methods:**\n\n- [`.sleep()`](../../reference/workflows/workflow-methods/sleep.mdx): Pause for a specified number of milliseconds\n- [`.sleepUntil()`](../../reference/workflows/workflow-methods/sleepUntil.mdx) : Pause until a specific date\n- [`.waitForEvent()`](../../reference/workflows/workflow-methods/waitForEvent.mdx): Pause until an external event is received\n- [`.sendEvent()`](../../reference/workflows/workflow-methods/sendEvent.mdx) : Send an event to resume a waiting workflow\n\n\n","path":null,"size_bytes":8873,"size_tokens":null},"docs/mastra/06-reference/50_memory-class.md":{"content":"---\ntitle: \"Reference: Memory Class | Memory | Mastra Docs\"\ndescription: \"Documentation for the `Memory` class in Mastra, which provides a robust system for managing conversation history and thread-based message storage.\"\n---\n\n# Memory Class\n[EN] Source: https://mastra.ai/en/reference/memory/Memory\n\nThe `Memory` class provides a robust system for managing conversation history and thread-based message storage in Mastra. It enables persistent storage of conversations, semantic search capabilities, and efficient message retrieval. You must configure a storage provider for conversation history, and if you enable semantic recall you will also need to provide a vector store and embedder.\n\n## Usage example\n\n```typescript filename=\"src/mastra/agents/test-agent.ts\" showLineNumbers copy\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\n\nexport const agent = new Agent({\n  name: \"test-agent\",\n  instructions: \"You are an agent with memory.\",\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    options: {\n      workingMemory: {\n        enabled: true\n      }\n    }\n  })\n});\n```\n\n> To enable `workingMemory` on an agent, youâ€™ll need a storage provider configured on your main Mastra instance. See [Mastra class](../core/mastra-class.mdx) for more information.\n\n## Constructor parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"storage\",\n      type: \"MastraStorage\",\n      description: \"Storage implementation for persisting memory data. Defaults to `new DefaultStorage({ config: { url: \\\"file:memory.db\\\" } })` if not provided.\",\n      isOptional: true,\n    },\n    {\n      name: \"vector\",\n      type: \"MastraVector | false\",\n      description: \"Vector store for semantic search capabilities. Set to `false` to disable vector operations.\",\n      isOptional: true,\n    },\n    {\n      name: \"embedder\",\n      type: \"EmbeddingModel<string> | EmbeddingModelV2<string>\",\n      description: \"Embedder instance for vector embeddings. Required when semantic recall is enabled.\",\n      isOptional: true,\n    },\n    {\n      name: \"options\",\n      type: \"MemoryConfig\",\n      description: \"Memory configuration options.\",\n      isOptional: true,\n    },\n    {\n      name: \"processors\",\n      type: \"MemoryProcessor[]\",\n      description: \"Array of memory processors that can filter or transform messages before they're sent to the LLM.\",\n      isOptional: true,\n    },\n  ]}\n/>\n\n### Options parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"lastMessages\",\n      type: \"number | false\",\n      description: \"Number of most recent messages to retrieve. Set to false to disable.\",\n      isOptional: true,\n      defaultValue: \"10\",\n    },\n    {\n      name: \"semanticRecall\",\n      type: \"boolean | { topK: number; messageRange: number | { before: number; after: number }; scope?: 'thread' | 'resource' }\",\n      description: \"Enable semantic search in message history. Can be a boolean or an object with configuration options. When enabled, requires both vector store and embedder to be configured.\",\n      isOptional: true,\n      defaultValue: \"false\",\n    },\n    {\n      name: \"workingMemory\",\n      type: \"WorkingMemory\",\n      description: \"Configuration for working memory feature. Can be `{ enabled: boolean; template?: string; schema?: ZodObject<any> | JSONSchema7; scope?: 'thread' | 'resource' }` or `{ enabled: boolean }` to disable.\",\n      isOptional: true,\n      defaultValue: \"{ enabled: false, template: '# User Information\\\\n- **First Name**:\\\\n- **Last Name**:\\\\n...' }\",\n    },\n    {\n      name: \"threads\",\n      type: \"{ generateTitle?: boolean | { model: DynamicArgument<MastraLanguageModel>; instructions?: DynamicArgument<string> } }\",\n      description: \"Settings related to memory thread creation. `generateTitle` controls automatic thread title generation from the user's first message. Can be a boolean or an object with custom model and instructions.\",\n      isOptional: true,\n      defaultValue: \"{ generateTitle: false }\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"memory\",\n      type: \"Memory\",\n      description: \"A new Memory instance with the specified configuration.\",\n    },\n  ]}\n/>\n\n\n## Extended usage example\n\n```typescript filename=\"src/mastra/agents/test-agent.ts\" showLineNumbers copy\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { LibSQLStore, LibSQLVector } from \"@mastra/libsql\";\n\nexport const agent = new Agent({\n  name: \"test-agent\",\n  instructions: \"You are an agent with memory.\",\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    storage: new LibSQLStore({\n      url: \"file:./working-memory.db\"\n    }),\n    vector: new LibSQLVector({\n      connectionUrl: \"file:./vector-memory.db\"\n    }),\n    options: {\n      lastMessages: 10,\n      semanticRecall: {\n        topK: 3,\n        messageRange: 2,\n        scope: 'resource'\n      },\n      workingMemory: {\n        enabled: true\n      },\n      threads: {\n        generateTitle: true\n      }\n    }\n  })\n});\n```\n\n## PostgreSQL with index configuration\n\n```typescript filename=\"src/mastra/agents/pg-agent.ts\" showLineNumbers copy\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { PgStore, PgVector } from \"@mastra/pg\";\n\nexport const agent = new Agent({\n  name: \"pg-agent\",\n  instructions: \"You are an agent with optimized PostgreSQL memory.\",\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    storage: new PgStore({\n      connectionString: process.env.DATABASE_URL\n    }),\n    vector: new PgVector({\n      connectionString: process.env.DATABASE_URL\n    }),\n    embedder: openai.embedding(\"text-embedding-3-small\"),\n    options: {\n      lastMessages: 20,\n      semanticRecall: {\n        topK: 5,\n        messageRange: 3,\n        scope: 'resource',\n        indexConfig: {\n          type: 'hnsw',              // Use HNSW for better performance\n          metric: 'dotproduct',      // Optimal for OpenAI embeddings\n          m: 16,                     // Number of bi-directional links\n          efConstruction: 64         // Construction-time candidate list size\n        }\n      },\n      workingMemory: {\n        enabled: true\n      }\n    }\n  })\n});\n```\n\n### Related\n\n- [Getting Started with Memory](/docs/memory/overview.mdx)\n- [Semantic Recall](/docs/memory/semantic-recall.mdx)\n- [Working Memory](/docs/memory/working-memory.mdx)\n- [Memory Processors](/docs/memory/memory-processors.mdx)\n- [createThread](/reference/memory/createThread.mdx)\n- [query](/reference/memory/query.mdx)\n- [getThreadById](/reference/memory/getThreadById.mdx)\n- [getThreadsByResourceId](/reference/memory/getThreadsByResourceId.mdx)\n- [deleteMessages](/reference/memory/deleteMessages.mdx)\n\n\n","path":null,"size_bytes":6855,"size_tokens":null},"docs/mastra/06-reference/90_workflow-class.md":{"content":"---\ntitle: \"Reference: Workflow Class | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow` class in Mastra, which enables you to create state machines for complex sequences of operations with conditional branching and data validation.\n---\n\n# Workflow Class\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow\n\nThe `Workflow` class enables you to create state machines for complex sequences of operations with conditional branching and data validation.\n\n## Usage example\n\n```typescript filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nexport const workflow = createWorkflow({\n  id: \"test-workflow\",\n  inputSchema: z.object({\n    value: z.string(),\n  }),\n  outputSchema: z.object({\n    value: z.string(),\n  })\n})\n```\n\n## Constructor parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"id\",\n      type: \"string\",\n      description: \"Unique identifier for the workflow\",\n    },\n    {\n      name: \"inputSchema\",\n      type: \"z.ZodType<any>\",\n      description: \"Zod schema defining the input structure for the workflow\",\n    },\n    {\n      name: \"outputSchema\",\n      type: \"z.ZodType<any>\",\n      description: \"Zod schema defining the output structure for the workflow\",\n    },\n    {\n      name: \"stateSchema\",\n      type: \"z.ZodObject<any>\",\n      description: \"Optional Zod schema for the workflow state. Automatically injected when using Mastra's state system. If not specified, type is 'any'.\",\n      isOptional: true,\n    },\n    {\n      name: \"options\",\n      type: \"WorkflowOptions\",\n      description: \"Optional options for the workflow\",\n      isOptional: true,\n    }\n  ]}\n/>\n\n### WorkflowOptions\n\n<PropertiesTable\n  content={[\n    {\n      name: \"tracingPolicy\",\n      type: \"TracingPolicy\",\n      description: \"Optional tracing policy for the workflow\",\n      isOptional: true,\n    },\n    {\n      name: \"validateInputs\",\n      type: \"boolean\",\n      description: \"Optional flag to determine whether to validate the workflow inputs. This also applies default values from zodSchemas on the workflow/step input/resume data. If input/resume data validation fails on start/resume, the workflow will not start/resume, it throws an error instead. If input data validation fails on a step execution, the step fails, causing the workflow to fail and the error is returned.\",\n      isOptional: true,\n      defaultValue: \"false\",\n    },\n    {\n      name: \"shouldPersistSnapshot\",\n      type: \"(params: { stepResults: Record<string, StepResult<any, any, any, any>>; workflowStatus: WorkflowRunStatus }) => boolean\",\n      description: \"Optional flag to determine whether to persist the workflow snapshot\",\n      isOptional: true,\n      defaultValue: \"() => true\",\n    },\n  ]}\n/>\n\n## Workflow status\n\nA workflow's `status` indicates its current execution state. The possible values are:\n\n<PropertiesTable\n  content={[\n    {\n      name: \"success\",\n      type: \"string\",\n      description:\n        \"All steps finished executing successfully, with a valid result output\",\n    },\n    {\n      name: \"failed\",\n      type: \"string\",\n      description:\n        \"Workflow encountered an error during execution, with error details available\",\n    },\n    {\n      name: \"suspended\",\n      type: \"string\",\n      description:\n        \"Workflow execution is paused waiting for resume, with suspended step information\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript filename=\"src/test-run.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst run = await mastra.getWorkflow(\"workflow\").createRunAsync();\n\nconst result = await run.start({...});\n\nif (result.status === \"suspended\") {\n  const resumedResult = await run.resume({...});\n}\n```\n\n## Related\n\n- [Step Class](./step.mdx)\n- [Control flow](../../docs/workflows/control-flow.mdx)\n\n\n","path":null,"size_bytes":3879,"size_tokens":null},"docs/mastra/06-reference/36_mastra-get-memory.md":{"content":"---\ntitle: \"Reference: Mastra.getMemory() | Core | Mastra Docs\"\ndescription: \"Documentation for the `Mastra.getMemory()` method in Mastra, which retrieves the configured memory instance.\"\n---\n\n# Mastra.getMemory()\n[EN] Source: https://mastra.ai/en/reference/core/getMemory\n\nThe `.getMemory()` method is used to retrieve the memory instance that has been configured in the Mastra instance.\n\n## Usage example\n\n```typescript copy\nmastra.getMemory();\n```\n\n## Parameters\n\nThis method does not accept any parameters.\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"memory\",\n      type: \"MastraMemory | undefined\",\n      description: \"The configured memory instance, or undefined if no memory has been configured.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Memory overview](../../docs/memory/overview.mdx)\n- [Memory reference](../../reference/memory/Memory.mdx)\n\n\n","path":null,"size_bytes":861,"size_tokens":null},"scripts/build.sh":{"content":"#!/usr/bin/env bash\n\nset -e\n\nexport NODE_OPTIONS='--max-old-space-size=4096'\nexport NODE_ENV=production\n\nmastra build\n\necho \"Copying memory file to output...\"\ncp us-complete.txt .mastra/output/\necho \"Memory file copied successfully\"\n","path":null,"size_bytes":233,"size_tokens":null},"docs/mastra/06-reference/37_mastra-get-workflow.md":{"content":"---\ntitle: \"Reference: Mastra.getWorkflow() | Core | Mastra Docs\"\ndescription: \"Documentation for the `Mastra.getWorkflow()` method in Mastra, which retrieves a workflow by ID.\"\n---\n\n# Mastra.getWorkflow()\n[EN] Source: https://mastra.ai/en/reference/core/getWorkflow\n\nThe `.getWorkflow()` method is used to retrieve a workflow by its ID. The method accepts a workflow ID and an optional options object.\n\n## Usage example\n\n```typescript copy\nmastra.getWorkflow(\"testWorkflow\");\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"id\",\n      type: \"TWorkflowId extends keyof TWorkflows\",\n      description: \"The ID of the workflow to retrieve. Must be a valid workflow ID that exists in the Mastra configuration.\",\n    },\n    {\n      name: \"options\",\n      type: \"{ serialized?: boolean }\",\n      description: \"Optional configuration object. When `serialized` is true, returns only the workflow name instead of the full workflow instance.\",\n      optional: true,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"TWorkflows[TWorkflowId]\",\n      description: \"The workflow instance with the specified ID. Throws an error if the workflow is not found.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Workflows overview](../../docs/workflows/overview.mdx)\n\n\n","path":null,"size_bytes":1306,"size_tokens":null},"docs/mastra/00-getting-started/02_project-structure.md":{"content":"---\ntitle: \"Local Project Structure | Getting Started | Mastra Docs\"\ndescription: Guide on organizing folders and files in Mastra, including best practices and recommended structures.\n---\n\nimport { FileTree, Callout } from \"nextra/components\";\n\n# Project Structure\n[EN] Source: https://mastra.ai/en/docs/getting-started/project-structure\n\nYour new Mastra project, created with the `create mastra` command, comes with a predefined set of files and folders to help you get started.\n\nMastra is a framework, but it's **unopinionated** about how you organize or colocate your files. The CLI provides a sensible default structure that works well for most projects, but you're free to adapt it to your workflow or team conventions. You could even build your entire project in a single file if you wanted! Whatever structure you choose, keep it consistent to ensure your code stays maintainable and easy to navigate.\n\n## Default project structure\n\nA project created with the `create mastra` command looks like this:\n\n<FileTree>\n  <FileTree.Folder name=\"src\" defaultOpen>\n    <FileTree.Folder name=\"mastra\" defaultOpen>\n      <FileTree.Folder name=\"agents\" defaultOpen>\n        <FileTree.File name=\"weather-agent.ts\" />\n      </FileTree.Folder>\n      <FileTree.Folder name=\"tools\" defaultOpen>\n        <FileTree.File name=\"weather-tool.ts\" />\n      </FileTree.Folder>\n      <FileTree.Folder name=\"workflows\" defaultOpen>\n        <FileTree.File name=\"weather-workflow.ts\" />\n      </FileTree.Folder>\n      <FileTree.Folder name=\"scorers\" defaultOpen>\n        <FileTree.File name=\"weather-scorer.ts\" />\n      </FileTree.Folder>\n      <FileTree.File name=\"index.ts\" />\n    </FileTree.Folder>\n  </FileTree.Folder>\n  <FileTree.File name=\".env.example\" />\n  <FileTree.File name=\"package.json\" />\n  <FileTree.File name=\"tsconfig.json\" />\n</FileTree>\n\n<Callout type=\"info\">\nTip - Use the predefined files as templates. Duplicate and adapt them to quickly create your own agents, tools, workflows, etc.\n</Callout>\n\n### Folders\n\nFolders organize your agent's resources, like agents, tools, and workflows.\n\n| Folder                 | Description |\n| ---------------------- | ------------ |\n| `src/mastra`           | Entry point for all Mastra-related code and configuration.|\n| `src/mastra/agents`    | Define and configure your agents - their behavior, goals, and tools. |\n| `src/mastra/workflows` | Define multi-step workflows that orchestrate agents and tools together. |\n| `src/mastra/tools`     | Create reusable tools that your agents can call |\n| `src/mastra/mcp`       | (Optional) Implement custom MCP servers to share your tools with external agents |\n| `src/mastra/scorers`   | (Optional) Define scorers for evaluating agent performance over time |\n| `src/mastra/public`    | (Optional) Contents are copied into the `.build/output` directory during the build process, making them available for serving at runtime |\n\n### Top-level files\n\nTop-level files define how your Mastra project is configured, built, and connected to its environment.\n\n| File                  | Description |\n| --------------------- | ------------ |\n| `src/mastra/index.ts` | Central entry point where you configure and initialize Mastra. |\n| `.env.example`        | Template for environment variables - copy and rename to `.env` to add your secret [model provider](/models) keys. |\n| `package.json`        | Defines project metadata, dependencies, and available npm scripts. |\n| `tsconfig.json`       | Configures TypeScript options such as path aliases, compiler settings, and build output. |\n\n## Next steps\n\n- Read more about [Mastra's features](/docs#why-mastra).\n- Integrate Mastra with your frontend framework: [Next.js](/docs/frameworks/web-frameworks/next-js), [React](/docs/frameworks/web-frameworks/vite-react), or [Astro](/docs/frameworks/web-frameworks/astro).\n- Build an agent from scratch following one of our [guides](/guides).\n- Watch conceptual guides on our [YouTube channel](https://www.youtube.com/@mastra-ai) and [subscribe](https://www.youtube.com/@mastra-ai?sub_confirmation=1)!\n\n","path":null,"size_bytes":4067,"size_tokens":null},"docs/mastra/03-workflows/15_streaming-overview.md":{"content":"---\ntitle: \"Streaming Overview | Streaming | Mastra\"\ndescription: \"Streaming in Mastra enables real-time, incremental responses from both agents and workflows, providing immediate feedback as AI-generated content is produced.\"\n---\n\n\n# Streaming Overview\n[EN] Source: https://mastra.ai/en/docs/streaming/overview\n\n\nMastra supports real-time, incremental responses from agents and workflows, allowing users to see output as itâ€™s generated instead of waiting for completion. This is useful for chat, long-form content, multi-step workflows, or any scenario where immediate feedback matters.\n\n## Getting started\n\nMastra's streaming API adapts based on your model version:\n\n- **`.stream()`**: For V2 models, supports **AI SDK v5** (`LanguageModelV2`).\n- **`.streamLegacy()`**: For V1 models, supports **AI SDK v4** (`LanguageModelV1`).\n\n## Streaming with agents\n\nYou can pass a single string for simple prompts, an array of strings when providing multiple pieces of context, or an array of message objects with `role` and `content` for precise control over roles and conversational flows.\n\n### Using `Agent.stream()`\n\nA `textStream` breaks the response into chunks as it's generated, allowing output to stream progressively instead of arriving all at once.  Iterate over the `textStream` using a `for await` loop to inspect each stream chunk.\n\n```typescript {3,7} showLineNumbers copy\nconst testAgent = mastra.getAgent(\"testAgent\");\n\nconst stream = await testAgent.stream([\n  { role: \"user\", content: \"Help me organize my day\" },\n]);\n\nfor await (const chunk of stream.textStream) {\n  process.stdout.write(chunk);\n}\n```\n\n> See [Agent.stream()](../../reference/agents/stream.mdx) for more information.\n\n### Output from `Agent.stream()`\n\nThe output streams the generated response from the agent.\n\n```text\nOf course!\nTo help you organize your day effectively, I need a bit more information.\nHere are some questions to consider:\n...\n```\n\n### Agent stream properties\n\nAn agent stream provides access to various response properties:\n\n- **`stream.textStream`**: A readable stream that emits text chunks.\n- **`stream.text`**: Promise that resolves to the full text response.\n- **`stream.finishReason`**: The reason the agent stopped streaming.\n- **`stream.usage`**: Token usage information.\n\n\n### AI SDK v5 Compatibility\n\nAI SDK v5 uses `LanguageModelV2` for the model providers. If you are getting an error that you are using an AI SDK v4 model you will need to upgrade your model package to the next major version.\n\nFor integration with AI SDK v5, use `format` 'aisdk' to get an `AISDKV5OutputStream`:\n\n```typescript {5} showLineNumbers copy\nconst testAgent = mastra.getAgent(\"testAgent\");\n\nconst stream = await testAgent.stream(\n  [{ role: \"user\", content: \"Help me organize my day\" }],\n  { format: \"aisdk\" }\n);\n\nfor await (const chunk of stream.textStream) {\n  process.stdout.write(chunk);\n}\n```\n\n### Using `Agent.network()`\n\nThe `network()` method enables multi-agent collaboration by executing a network loop where multiple agents can work together to handle complex tasks. The routing agent delegates tasks to appropriate sub-agents, workflows, and tools based on the conversation context.\n\n> **Note**: This method is experimental and requires memory to be configured on the agent.\n\n```typescript {3,5-7} showLineNumbers copy\nconst testAgent = mastra.getAgent(\"testAgent\");\n\nconst networkStream = await testAgent.network(\"Help me organize my day\");\n\nfor await (const chunk of networkStream) {\n  console.log(chunk);\n}\n```\n\n> See [Agent.network()](../../reference/agents/network.mdx) for more information.\n\n#### Network stream properties\n\nThe network stream provides access to execution information:\n\n- **`networkStream.status`**: Promise resolving to the workflow execution status\n- **`networkStream.result`**: Promise resolving to the complete execution results\n- **`networkStream.usage`**: Promise resolving to token usage information\n\n```typescript {9-11} showLineNumbers copy\nconst testAgent = mastra.getAgent(\"testAgent\");\n\nconst networkStream = await testAgent.network(\"Research dolphins then write a report\");\n\nfor await (const chunk of networkStream) {\n  console.log(chunk);\n}\n\nconsole.log('Final status:', await networkStream.status);\nconsole.log('Final result:', await networkStream.result);\nconsole.log('Token usage:', await networkStream.usage);\n```\n\n## Streaming with workflows\n\nStreaming from a workflow returns a sequence of structured events describing the run lifecycle, rather than incremental text chunks. This event-based format makes it possible to track and respond to workflow progress in real time once a run is created using `.createRunAsync()`.\n\n### Using `Run.streamVNext()`\n\nThis is the experimental API. It returns a `ReadableStream` of events directly.\n\n```typescript {3,9} showLineNumbers copy\nconst run = await testWorkflow.createRunAsync();\n\nconst stream = await run.streamVNext({\n  inputData: {\n    value: \"initial data\"\n  }\n});\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n```\n\n> See [Run.streamVNext()](../../reference/workflows/run-methods/streamVNext.mdx) for more information.\n\n### Output from `Run.stream()`\n\nThe experimental API event structure includes `runId` and `from` at the top level, making it easier to identify and track workflow runs without digging into the payload.\n\n```typescript\n// ...\n{\n  type: 'step-start',\n  runId: '1eeaf01a-d2bf-4e3f-8d1b-027795ccd3df',\n  from: 'WORKFLOW',\n  payload: {\n    stepName: 'step-1',\n    args: { value: 'initial data' },\n    stepCallId: '8e15e618-be0e-4215-a5d6-08e58c152068',\n    startedAt: 1755121710066,\n    status: 'running'\n  }\n}\n```\n\n## Workflow stream properties\n\nA workflow stream provides access to various response properties:\n\n- **`stream.status`**: The status of the workflow run.\n- **`stream.result`**: The result of the workflow run.\n- **`stream.usage`**: The total token usage of the workflow run.\n\n## Related\n\n- [Streaming events](./events.mdx)\n- [Using Agents](../agents/overview.mdx)\n- [Workflows overview](../workflows/overview.mdx)\n\n\n","path":null,"size_bytes":6051,"size_tokens":null},"src/mastra/storage/index.ts":{"content":"import { PostgresStore } from \"@mastra/pg\";\n\n// Create a single shared PostgreSQL storage instance\nexport const sharedPostgresStorage = new PostgresStore({\n  connectionString:\n    process.env.DATABASE_URL || \"postgresql://localhost:5432/mastra\",\n});\n","path":null,"size_bytes":250,"size_tokens":null},"docs/mastra/06-reference/97_workflow-map.md":{"content":"---\ntitle: \"Reference: Workflow.map() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow.map()` method in workflows, which maps output data from a previous step to the input of a subsequent step.\n---\n\n# Workflow.map()\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow-methods/map\n\nThe `.map()` method maps output data from a previous step to the input of a subsequent step, allowing you to transform data between steps.\n\n## Usage example\n\n```typescript copy\nworkflow.map(async ({ inputData }) => `${inputData.value} - map`\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"mappingFunction\",\n      type: \"(params: { inputData: any }) => any\",\n      description: \"Function that transforms input data and returns the mapped result\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"Workflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Input data mapping](../../../docs/workflows/input-data-mapping.mdx)\n\n\n","path":null,"size_bytes":1085,"size_tokens":null},"docs/mastra/04-models/01_providers-overview.md":{"content":"---\ntitle: \"Providers\"\ndescription: \"Direct access to AI model providers.\"\n---\n\n{/* This file is auto-generated by generate-model-docs.ts - DO NOT EDIT MANUALLY */}\n\nimport { CardGrid, CardGridItem } from \"@/components/cards/card-grid\";\n\n# Model Providers\n[EN] Source: https://mastra.ai/en/models/providers\n\nDirect access to individual AI model providers. Each provider offers unique models with specific capabilities and pricing.\n\n<CardGrid>\n    <CardGridItem\n      title=\"OpenAI\"\n      description=\"30 models\"\n      href=\"./providers/openai\"\n      logo=\"https://models.dev/logos/openai.svg\"\n    />    <CardGridItem\n      title=\"Anthropic\"\n      description=\"19 models\"\n      href=\"./providers/anthropic\"\n      logo=\"https://models.dev/logos/anthropic.svg\"\n    />    <CardGridItem\n      title=\"Google\"\n      description=\"24 models\"\n      href=\"./providers/google\"\n      logo=\"https://models.dev/logos/google.svg\"\n    />    <CardGridItem\n      title=\"DeepSeek\"\n      description=\"2 models\"\n      href=\"./providers/deepseek\"\n      logo=\"https://models.dev/logos/deepseek.svg\"\n    />    <CardGridItem\n      title=\"Groq\"\n      description=\"17 models\"\n      href=\"./providers/groq\"\n      logo=\"https://models.dev/logos/groq.svg\"\n    />    <CardGridItem\n      title=\"Mistral\"\n      description=\"19 models\"\n      href=\"./providers/mistral\"\n      logo=\"https://models.dev/logos/mistral.svg\"\n    />    <CardGridItem\n      title=\"xAI\"\n      description=\"20 models\"\n      href=\"./providers/xai\"\n      logo=\"https://models.dev/logos/xai.svg\"\n    />    <CardGridItem\n      title=\"Alibaba\"\n      description=\"39 models\"\n      href=\"./providers/alibaba\"\n      logo=\"https://models.dev/logos/alibaba.svg\"\n    />    <CardGridItem\n      title=\"Alibaba (China)\"\n      description=\"61 models\"\n      href=\"./providers/alibaba-cn\"\n      logo=\"https://models.dev/logos/alibaba-cn.svg\"\n    />    <CardGridItem\n      title=\"Baseten\"\n      description=\"3 models\"\n      href=\"./providers/baseten\"\n      logo=\"https://models.dev/logos/baseten.svg\"\n    />    <CardGridItem\n      title=\"Cerebras\"\n      description=\"3 models\"\n      href=\"./providers/cerebras\"\n      logo=\"https://models.dev/logos/cerebras.svg\"\n    />    <CardGridItem\n      title=\"Chutes\"\n      description=\"33 models\"\n      href=\"./providers/chutes\"\n      logo=\"https://models.dev/logos/chutes.svg\"\n    />    <CardGridItem\n      title=\"Cortecs\"\n      description=\"11 models\"\n      href=\"./providers/cortecs\"\n      logo=\"https://models.dev/logos/cortecs.svg\"\n    />    <CardGridItem\n      title=\"Deep Infra\"\n      description=\"4 models\"\n      href=\"./providers/deepinfra\"\n      logo=\"https://models.dev/logos/deepinfra.svg\"\n    />    <CardGridItem\n      title=\"FastRouter\"\n      description=\"14 models\"\n      href=\"./providers/fastrouter\"\n      logo=\"https://models.dev/logos/fastrouter.svg\"\n    />    <CardGridItem\n      title=\"Fireworks AI\"\n      description=\"10 models\"\n      href=\"./providers/fireworks-ai\"\n      logo=\"https://models.dev/logos/fireworks-ai.svg\"\n    />    <CardGridItem\n      title=\"GitHub Models\"\n      description=\"55 models\"\n      href=\"./providers/github-models\"\n      logo=\"https://models.dev/logos/github-models.svg\"\n    />    <CardGridItem\n      title=\"Hugging Face\"\n      description=\"13 models\"\n      href=\"./providers/huggingface\"\n      logo=\"https://models.dev/logos/huggingface.svg\"\n    />    <CardGridItem\n      title=\"Inception\"\n      description=\"2 models\"\n      href=\"./providers/inception\"\n      logo=\"https://models.dev/logos/inception.svg\"\n    />    <CardGridItem\n      title=\"Inference\"\n      description=\"9 models\"\n      href=\"./providers/inference\"\n      logo=\"https://models.dev/logos/inference.svg\"\n    />    <CardGridItem\n      title=\"Llama\"\n      description=\"7 models\"\n      href=\"./providers/llama\"\n      logo=\"https://models.dev/logos/llama.svg\"\n    />    <CardGridItem\n      title=\"LMStudio\"\n      description=\"3 models\"\n      href=\"./providers/lmstudio\"\n      logo=\"https://models.dev/logos/lmstudio.svg\"\n    />    <CardGridItem\n      title=\"LucidQuery AI\"\n      description=\"2 models\"\n      href=\"./providers/lucidquery\"\n      logo=\"https://models.dev/logos/lucidquery.svg\"\n    />    <CardGridItem\n      title=\"ModelScope\"\n      description=\"7 models\"\n      href=\"./providers/modelscope\"\n      logo=\"https://models.dev/logos/modelscope.svg\"\n    />    <CardGridItem\n      title=\"Moonshot AI\"\n      description=\"3 models\"\n      href=\"./providers/moonshotai\"\n      logo=\"https://models.dev/logos/moonshotai.svg\"\n    />    <CardGridItem\n      title=\"Moonshot AI (China)\"\n      description=\"3 models\"\n      href=\"./providers/moonshotai-cn\"\n      logo=\"https://models.dev/logos/moonshotai-cn.svg\"\n    />    <CardGridItem\n      title=\"Morph\"\n      description=\"3 models\"\n      href=\"./providers/morph\"\n      logo=\"https://models.dev/logos/morph.svg\"\n    />    <CardGridItem\n      title=\"Nebius AI Studio\"\n      description=\"15 models\"\n      href=\"./providers/nebius\"\n      logo=\"https://models.dev/logos/nebius.svg\"\n    />    <CardGridItem\n      title=\"Nvidia\"\n      description=\"16 models\"\n      href=\"./providers/nvidia\"\n      logo=\"https://models.dev/logos/nvidia.svg\"\n    />    <CardGridItem\n      title=\"OpenCode Zen\"\n      description=\"14 models\"\n      href=\"./providers/opencode\"\n      logo=\"https://models.dev/logos/opencode.svg\"\n    />    <CardGridItem\n      title=\"Perplexity\"\n      description=\"4 models\"\n      href=\"./providers/perplexity\"\n      logo=\"https://models.dev/logos/perplexity.svg\"\n    />    <CardGridItem\n      title=\"Requesty\"\n      description=\"13 models\"\n      href=\"./providers/requesty\"\n      logo=\"https://models.dev/logos/requesty.svg\"\n    />    <CardGridItem\n      title=\"Scaleway\"\n      description=\"11 models\"\n      href=\"./providers/scaleway\"\n      logo=\"https://models.dev/logos/scaleway.svg\"\n    />    <CardGridItem\n      title=\"submodel\"\n      description=\"9 models\"\n      href=\"./providers/submodel\"\n      logo=\"https://models.dev/logos/submodel.svg\"\n    />    <CardGridItem\n      title=\"Synthetic\"\n      description=\"21 models\"\n      href=\"./providers/synthetic\"\n      logo=\"https://models.dev/logos/synthetic.svg\"\n    />    <CardGridItem\n      title=\"Together AI\"\n      description=\"6 models\"\n      href=\"./providers/togetherai\"\n      logo=\"https://models.dev/logos/togetherai.svg\"\n    />    <CardGridItem\n      title=\"Upstage\"\n      description=\"2 models\"\n      href=\"./providers/upstage\"\n      logo=\"https://models.dev/logos/upstage.svg\"\n    />    <CardGridItem\n      title=\"Venice AI\"\n      description=\"13 models\"\n      href=\"./providers/venice\"\n      logo=\"https://models.dev/logos/venice.svg\"\n    />    <CardGridItem\n      title=\"Vultr\"\n      description=\"5 models\"\n      href=\"./providers/vultr\"\n      logo=\"https://models.dev/logos/vultr.svg\"\n    />    <CardGridItem\n      title=\"Weights &amp; Biases\"\n      description=\"10 models\"\n      href=\"./providers/wandb\"\n      logo=\"https://models.dev/logos/wandb.svg\"\n    />    <CardGridItem\n      title=\"Z.AI\"\n      description=\"5 models\"\n      href=\"./providers/zai\"\n      logo=\"https://models.dev/logos/zai.svg\"\n    />    <CardGridItem\n      title=\"Z.AI Coding Plan\"\n      description=\"5 models\"\n      href=\"./providers/zai-coding-plan\"\n      logo=\"https://models.dev/logos/zai-coding-plan.svg\"\n    />    <CardGridItem\n      title=\"Zhipu AI\"\n      description=\"5 models\"\n      href=\"./providers/zhipuai\"\n      logo=\"https://models.dev/logos/zhipuai.svg\"\n    />    <CardGridItem\n      title=\"Zhipu AI Coding Plan\"\n      description=\"5 models\"\n      href=\"./providers/zhipuai-coding-plan\"\n      logo=\"https://models.dev/logos/zhipuai-coding-plan.svg\"\n    />\n</CardGrid>\n\n","path":null,"size_bytes":7647,"size_tokens":null},"docs/triggers/time_based_triggers.md":{"content":"# Time-Based Triggers\n\nCreate scheduled automations that trigger Mastra workflows on a recurring schedule using cron expressions.\n\n**NOTE:** This document describes how dev/prod runs the full routing.  The agent may need to know how to directly query the Mastra server to run agents, workflows, etc.  You should disregard this documentation in that case and consult the Mastra doc (docs/mastra/**).\n\n## Quick Start\n\n**2 steps to add a time-based trigger:**\n\n### 1. Import and call `registerCronTrigger` in `src/mastra/index.ts`\n\nAdd this **before** the Mastra initialization:\n\n```typescript\nimport { registerCronTrigger } from \"../triggers/cronTriggers\";\nimport { myWorkflow } from \"./workflows/myWorkflow\";\n\n// Register BEFORE the Mastra initialization\nregisterCronTrigger({\n  cronExpression: \"0 9 * * *\", // Every day at 9 AM\n  workflow: myWorkflow\n});\n\n// Then the Mastra initialization\nexport const mastra = new Mastra({\n  // ...\n});\n```\n\n### 2. Make sure your workflow has an empty input schema\n\nTime-based workflows don't receive external input, so use an empty schema:\n\n```typescript\nexport const myWorkflow = createWorkflow({\n  id: \"daily-report-workflow\",\n  \n  // Empty input schema for time-based triggers\n  inputSchema: z.object({}) as any,\n  \n  outputSchema: z.object({\n    success: z.boolean(),\n    message: z.string(),\n  }),\n})\n  .then(generateReport)\n  .then(sendNotification)\n  .commit();\n```\n\nDone! Your workflow will now run on schedule.\n\n**Important:** Unlike webhook triggers, do NOT spread `registerCronTrigger()` into the `apiRoutes` array. Cron triggers don't create HTTP endpoints, so you simply call the function directly as shown above. The function registers the trigger internally and returns an empty array.\n\n## Key Differences from Webhook Triggers\n\nTime-based triggers work differently from webhook triggers:\n\n| Aspect | Time-Based (Cron) | Webhook-Based |\n|--------|-------------------|---------------|\n| **Registration** | Call `registerCronTrigger()` directly | Spread into `apiRoutes`: `...registerSlackTrigger()` |\n| **Location** | Before Mastra initialization | Inside `apiRoutes` array |\n| **Returns** | Empty array `[]` | Array with API route config |\n| **Creates endpoint** | No | Yes (e.g., `/slack/webhook`) |\n| **Trigger source** | Schedule (cron) | External HTTP request |\n\n**Example - Time-Based (DO THIS):**\n\n```typescript\n// Just call it directly, no spreading\nregisterCronTrigger({\n  cronExpression: \"0 9 * * *\",\n  workflow: myWorkflow\n});\n```\n\n**Example - Webhook-Based (DON'T DO THIS for cron):**\n\n```typescript\n// DON'T spread registerCronTrigger into apiRoutes\n// This is only for webhook triggers\napiRoutes: [\n  ...registerSlackTrigger({ ... }), // âœ… Correct for webhooks\n  ...registerCronTrigger({ ... }),  // âŒ Wrong! Don't do this\n]\n```\n\n## Common Cron Expressions\n\n```typescript\n// Every minute (testing only)\n\"* * * * *\"\n\n// Every 5 minutes\n\"*/5 * * * *\"\n\n// Every hour at minute 0\n\"0 * * * *\"\n\n// Every day at 9 AM\n\"0 9 * * *\"\n\n// Every day at 6 PM\n\"0 18 * * *\"\n\n// Every Monday at 9 AM\n\"0 9 * * 1\"\n\n// Every weekday (Mon-Fri) at 9 AM\n\"0 9 * * 1-5\"\n\n// First day of every month at midnight\n\"0 0 1 * *\"\n\n// Every Sunday at 8 AM\n\"0 8 * * 0\"\n\n// Twice daily: 9 AM and 6 PM\n\"0 9,18 * * *\"\n```\n\n**Cron Format:** `minute hour day-of-month month day-of-week`\n\n## Complete Example\n\n### Step 1: Create Your Workflow\n\n`src/mastra/workflows/dailyReportWorkflow.ts`:\n\n```typescript\nimport { createStep, createWorkflow } from \"../inngest\";\nimport { z } from \"zod\";\n\nconst generateReport = createStep({\n  id: \"generate-report\",\n  description: \"Generates daily analytics report\",\n  \n  inputSchema: z.object({}),\n  \n  outputSchema: z.object({\n    reportData: z.string(),\n    timestamp: z.string(),\n  }),\n  \n  execute: async ({ mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info(\"ðŸ“Š Generating daily report...\");\n    \n    // Your report generation logic here\n    const reportData = \"Daily metrics: ...\";\n    \n    return {\n      reportData,\n      timestamp: new Date().toISOString(),\n    };\n  },\n});\n\nconst sendNotification = createStep({\n  id: \"send-notification\",\n  description: \"Sends the report via email or Slack\",\n  \n  inputSchema: z.object({\n    reportData: z.string(),\n    timestamp: z.string(),\n  }),\n  \n  outputSchema: z.object({\n    success: z.boolean(),\n    message: z.string(),\n  }),\n  \n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info(\"ðŸ“§ Sending report notification...\");\n    \n    // Your notification logic here\n    // e.g., send email, post to Slack, etc.\n    \n    return {\n      success: true,\n      message: \"Report sent successfully\",\n    };\n  },\n});\n\nexport const dailyReportWorkflow = createWorkflow({\n  id: \"daily-report-workflow\",\n  \n  // Empty input schema for time-based triggers\n  inputSchema: z.object({}) as any,\n  \n  outputSchema: z.object({\n    success: z.boolean(),\n    message: z.string(),\n  }),\n})\n  .then(generateReport)\n  .then(sendNotification)\n  .commit();\n```\n\n### Step 2: Register in `src/mastra/index.ts`\n\n```typescript\nimport { registerCronTrigger } from \"../triggers/cronTriggers\";\nimport { dailyReportWorkflow } from \"./workflows/dailyReportWorkflow\";\n\n// Register the cron trigger BEFORE Mastra initialization\nregisterCronTrigger({\n  cronExpression: \"0 9 * * *\", // Every day at 9 AM\n  workflow: dailyReportWorkflow\n});\n\n// Import your workflow for registration\nexport const mastra = new Mastra({\n  workflows: {\n    dailyReportWorkflow, // Register the workflow\n  },\n  // ... rest of config\n});\n```\n\n## Testing Your Time-Based Trigger\n\n### 1. Test with a short interval\n\nFor testing, use a frequent cron expression:\n\n```typescript\n// Run every minute (for testing only)\nregisterCronTrigger({\n  cronExpression: \"* * * * *\",\n  workflow: myWorkflow\n});\n```\n\n### 2. Check the logs\n\nBoth servers should be running:\n\n- Mastra dev server (port 5000)\n- Inngest dev server (port 3000)\n\nWatch the logs for execution:\n\n```text\nðŸ• [registerCronWorkflow] Registering cron trigger\nðŸš€ [Cron Trigger] Starting scheduled workflow execution\nðŸ“ [Cron Trigger] Workflow run created\nâœ… [Cron Trigger] Workflow completed successfully\n```\n\n### 3. Monitor in Inngest Dashboard\n\nVisit `http://localhost:3000` to see:\n\n- When the workflow ran\n- Step-by-step execution\n- Any errors or retries\n\n### 4. Manual testing\n\nTrigger the workflow manually via the Mastra Playground or API:\n\n```bash\n# Via Mastra API\ncurl -X POST http://localhost:5000/api/workflows/daily-report-workflow/run \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"inputData\": {}}'\n```\n\n## Key Principles\n\n**1. Empty input schema:** Time-based workflows don't receive external input\n\n```typescript\ninputSchema: z.object({}) as any\n```\n\n**2. Register before Mastra initialization:** The cron trigger must be registered before `new Mastra({})`\n\n**3. One trigger per automation:** An automation should only have one trigger (either cron OR webhook, not both)\n\n**4. Don't spread into apiRoutes:** Call `registerCronTrigger()` directly. Unlike webhook triggers, do NOT use the spread operator (`...`) or add it to the `apiRoutes` array\n\n**5. Register only once:** Call `registerCronTrigger()` only one time for your automation. Multiple calls will register multiple triggers, which is not supported\n\n**6. Test with frequent intervals:** Use `* * * * *` (every minute) for testing, then change to your desired schedule\n\n## Architecture Notes\n\nThe `registerCronTrigger` function:\n\n1. Creates an Inngest function that listens to both:\n   - A custom event (`replit/cron.trigger`)\n   - A cron schedule (your expression)\n2. Executes your workflow when the schedule fires\n3. Provides automatic retries and durability through Inngest\n\nIn production, Inngest Cloud manages the scheduling and ensures reliable execution.\n\n## Troubleshooting\n\n### Cron trigger not firing\n\n- Verify both servers are running (Mastra on 5000, Inngest on 3000)\n- Check that `registerCronTrigger` is called BEFORE `new Mastra({})`\n- Look for the registration log: `ðŸ• [registerCronWorkflow] Registering cron trigger`\n\n### Workflow not registered\n\n- Ensure your workflow is imported and added to the `workflows` object in Mastra initialization\n- Check for any import errors in the logs\n- Verify the workflow ID matches what you expect\n\n### Wrong schedule\n\n- Use <https://crontab.guru> to validate your cron expression\n- Remember: cron uses UTC time in production\n- Test with `* * * * *` (every minute) first\n\n## Moving to Production\n\n### Update cron expression\n\nChange from testing interval to production schedule:\n\n```typescript\n// Development: every minute for testing\n// cronExpression: \"* * * * *\"\n\n// Production: every day at 9 AM UTC\ncronExpression: \"0 9 * * *\"\n```\n\n### Consider timezone\n\nCron expressions use UTC. If you need a specific timezone:\n\n- Convert your desired time to UTC\n- Example: 9 AM Pacific (UTC-8) = 5 PM UTC (17:00)\n- Use: `\"0 17 * * *\"`\n\n## Examples\n\nSee these files for complete examples:\n\n- `src/mastra/workflows/exampleWorkflow.ts` - Example workflow structure\n- `src/triggers/cronTriggers.ts` - Registration implementation\n\n## Webhook Triggers\n\nFor event-driven automations (Slack messages, GitHub events, etc.), see [Connector Webhook Triggers](./webhook_connector_triggers.md).\n","path":null,"size_bytes":9290,"size_tokens":null},"docs/mastra/06-reference/70_templates-reference.md":{"content":"---\ntitle: \"Templates Reference\"\ndescription: \"Complete guide to creating, using, and contributing Mastra templates\"\n---\n\nimport { FileTree, Tabs, Callout } from 'nextra/components'\n\n## Overview\n[EN] Source: https://mastra.ai/en/reference/templates/overview\n\nThis reference provides comprehensive information about Mastra templates, including how to use existing templates, create your own, and contribute to the community ecosystem.\n\nMastra templates are pre-built project structures that demonstrate specific use cases and patterns. They provide:\n\n- **Working examples** - Complete, functional Mastra applications\n- **Best practices** - Proper project structure and coding conventions\n- **Educational resources** - Learn Mastra patterns through real implementations\n- **Quick starts** - Bootstrap projects faster than building from scratch\n\n## Using Templates\n\n### Installation\n\nInstall a template using the `create-mastra` command:\n\n```bash copy\nnpx create-mastra@latest --template template-name\n```\n\nThis creates a complete project with all necessary code and configuration.\n\n### Setup Process\n\nAfter installation:\n\n1. **Navigate to project directory**:\n\n   ```bash copy\n   cd your-project-name\n   ```\n\n2. **Configure environment variables**:\n\n   ```bash copy\n   cp .env.example .env\n   ```\n\n   Edit `.env` with required API keys as documented in the template's README.\n\n3. **Install dependencies** (if not done automatically):\n\n   ```bash copy\n   npm install\n   ```\n\n4. **Start development server**:\n\n   ```bash copy\n   npm run dev\n   ```\n\n### Template Structure\n\nAll templates follow this standardized structure:\n\n<FileTree>\n  <FileTree.Folder name=\"template-name\" defaultOpen>\n    <FileTree.Folder name=\"src\" defaultOpen>\n      <FileTree.Folder name=\"mastra\" defaultOpen>\n        <FileTree.Folder name=\"agents\">\n          <FileTree.File name=\"example-agent.ts\" />\n        </FileTree.Folder>\n        <FileTree.Folder name=\"tools\">\n          <FileTree.File name=\"example-tool.ts\" />\n        </FileTree.Folder>\n        <FileTree.Folder name=\"workflows\">\n          <FileTree.File name=\"example-workflow.ts\" />\n        </FileTree.Folder>\n        <FileTree.File name=\"index.ts\" />\n      </FileTree.Folder>\n    </FileTree.Folder>\n    <FileTree.File name=\".env.example\" />\n    <FileTree.File name=\"package.json\" />\n    <FileTree.File name=\"README.md\" />\n    <FileTree.File name=\"tsconfig.json\" />\n  </FileTree.Folder>\n</FileTree>\n\n## Creating Templates\n\n### Requirements\n\nTemplates must meet these technical requirements:\n\n#### Project Structure\n\n- **Mastra code location**: All Mastra code must be in `src/mastra/` directory\n- **Component organization**:\n  - Agents: `src/mastra/agents/`\n  - Tools: `src/mastra/tools/`\n  - Workflows: `src/mastra/workflows/`\n  - Main config: `src/mastra/index.ts`\n\n#### TypeScript Configuration\n\nUse the standard Mastra TypeScript configuration:\n\n```json filename=\"tsconfig.json\"\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"ES2022\",\n    \"moduleResolution\": \"bundler\",\n    \"esModuleInterop\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"strict\": true,\n    \"skipLibCheck\": true,\n    \"noEmit\": true,\n    \"outDir\": \"dist\"\n  },\n  \"include\": [\"src/**/*\"]\n}\n```\n\n#### Environment Configuration\n\nInclude a `.env.example` file with all required environment variables:\n\n```bash filename=\".env.example\"\n# LLM provider API keys (choose one or more)\nOPENAI_API_KEY=your_openai_api_key_here\nANTHROPIC_API_KEY=your_anthropic_api_key_here\nGOOGLE_GENERATIVE_AI_API_KEY=your_google_api_key_here\n\n# Other service API keys as needed\nOTHER_SERVICE_API_KEY=your_api_key_here\n```\n\n### Code Standards\n\n#### LLM Provider\n\nWe recommend using OpenAI, Anthropic, or Google model providers for templates. Choose the provider that best fits your use case:\n\n```typescript filename=\"src/mastra/agents/example-agent.ts\"\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\n// Or use: import { anthropic } from '@ai-sdk/anthropic';\n// Or use: import { google } from '@ai-sdk/google';\n\nconst agent = new Agent({\n  name: 'example-agent',\n  model: openai('gpt-4'), // or anthropic('') or google('')\n  instructions: 'Your agent instructions here',\n  // ... other configuration\n});\n```\n\n#### Compatibility Requirements\n\nTemplates must be:\n\n- **Single projects** - Not monorepos with multiple applications\n- **Framework-free** - No Next.js, Express, or other web framework boilerplate\n- **Mastra-focused** - Demonstrate Mastra functionality without additional layers\n- **Mergeable** - Structure code for easy integration into existing projects\n- **Node.js compatible** - Support Node.js 18 and higher\n- **ESM modules** - Use ES modules (`\"type\": \"module\"` in package.json)\n\n### Documentation Requirements\n\n#### README Structure\n\nEvery template must include a comprehensive README:\n\n```markdown filename=\"README.md\"\n# Template Name\n\nBrief description of what the template demonstrates.\n\n## Overview\n\nDetailed explanation of the template's functionality and use case.\n\n## Setup\n\n1. Copy `.env.example` to `.env` and fill in your API keys\n2. Install dependencies: `npm install`\n3. Run the project: `npm run dev`\n\n## Environment Variables\n\n- `OPENAI_API_KEY`: Your OpenAI API key. Get one at [OpenAI Platform](https://platform.openai.com/api-keys)\n- `ANTHROPIC_API_KEY`: Your Anthropic API key. Get one at [Anthropic Console](https://console.anthropic.com/settings/keys)\n- `GOOGLE_GENERATIVE_AI_API_KEY`: Your Google AI API key. Get one at [Google AI Studio](https://makersuite.google.com/app/apikey)\n- `OTHER_API_KEY`: Description of what this key is for\n\n## Usage\n\nInstructions on how to use the template and examples of expected behavior.\n\n## Customization\n\nGuidelines for modifying the template for different use cases.\n```\n\n#### Code Comments\n\nInclude clear comments explaining:\n\n- Complex logic or algorithms\n- API integrations and their purpose\n- Configuration options and their effects\n- Example usage patterns\n\n### Quality Standards\n\nTemplates must demonstrate:\n\n- **Code quality** - Clean, well-commented, maintainable code\n- **Error handling** - Proper handling for external APIs and user inputs\n- **Type safety** - Full TypeScript typing with Zod validation\n- **Testing** - Verified functionality with fresh installations\n\nFor information on contributing your own templates to the Mastra ecosystem, see the [Contributing Templates](/docs/community/contributing-templates) guide in the community section.\n\n<Callout type=\"info\">\n  Templates provide an excellent way to learn Mastra patterns and accelerate development. Contributing templates helps the entire community build better AI applications.\n</Callout>\n\n\n","path":null,"size_bytes":6675,"size_tokens":null},"docs/mastra/01-agents/13_working-memory.md":{"content":"---\ntitle: \"Working Memory | Memory | Mastra Docs\"\ndescription: \"Learn how to configure working memory in Mastra to store persistent user data, preferences.\"\n---\n\nimport YouTube from \"@/components/youtube\";\n\n# Working Memory\n[EN] Source: https://mastra.ai/en/docs/memory/working-memory\n\nWhile [conversation history](/docs/memory/overview#conversation-history) and [semantic recall](./semantic-recall.mdx) help agents remember conversations, working memory allows them to maintain persistent information about users across interactions.\n\nThink of it as the agent's active thoughts or scratchpad â€“ the key information they keep available about the user or task. It's similar to how a person would naturally remember someone's name, preferences, or important details during a conversation.\n\nThis is useful for maintaining ongoing state that's always relevant and should always be available to the agent.\n\nWorking memory can persist at two different scopes:\n- **Thread-scoped** (default): Memory is isolated per conversation thread\n- **Resource-scoped**: Memory persists across all conversation threads for the same user\n\n**Important:** Switching between scopes means the agent won't see memory from the other scope - thread-scoped memory is completely separate from resource-scoped memory.\n\n\n## Quick Start\n\nHere's a minimal example of setting up an agent with working memory:\n\n```typescript {12-15}\nimport { Agent } from \"@mastra/core/agent\";\nimport { Memory } from \"@mastra/memory\";\nimport { openai } from \"@ai-sdk/openai\";\n\n// Create agent with working memory enabled\nconst agent = new Agent({\n  name: \"PersonalAssistant\",\n  instructions: \"You are a helpful personal assistant.\",\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    options: {\n      workingMemory: {\n        enabled: true,\n      },\n    },\n  }),\n});\n```\n\n## How it Works\n\nWorking memory is a block of Markdown text that the agent is able to update over time to store continuously relevant information:\n\n<YouTube id=\"UMy_JHLf1n8\" />\n\n## Memory Persistence Scopes\n\nWorking memory can operate in two different scopes, allowing you to choose how memory persists across conversations:\n\n### Thread-Scoped Memory (Default)\n\nBy default, working memory is scoped to individual conversation threads. Each thread maintains its own isolated memory:\n\n```typescript\nconst memory = new Memory({\n  storage,\n  options: {\n    workingMemory: {\n      enabled: true,\n      scope: 'thread', // Default - memory is isolated per thread\n      template: `# User Profile\n- **Name**:\n- **Interests**:\n- **Current Goal**:\n`,\n    },\n  },\n});\n```\n\n**Use cases:**\n- Different conversations about separate topics\n- Temporary or session-specific information\n- Workflows where each thread needs working memory but threads are ephemeral and not related to each other\n\n### Resource-Scoped Memory\n\nResource-scoped memory persists across all conversation threads for the same user (resourceId), enabling persistent user memory:\n\n```typescript\nconst memory = new Memory({\n  storage,\n  options: {\n    workingMemory: {\n      enabled: true,\n      scope: 'resource', // Memory persists across all user threads\n      template: `# User Profile\n- **Name**:\n- **Location**:\n- **Interests**:\n- **Preferences**:\n- **Long-term Goals**:\n`,\n    },\n  },\n});\n```\n\n**Use cases:**\n- Personal assistants that remember user preferences\n- Customer service bots that maintain customer context\n- Educational applications that track student progress\n\n### Usage with Agents\n\nWhen using resource-scoped memory, make sure to pass the `resourceId` parameter:\n\n```typescript\n// Resource-scoped memory requires resourceId\nconst response = await agent.generate(\"Hello!\", {\n  threadId: \"conversation-123\",\n  resourceId: \"user-alice-456\" // Same user across different threads\n});\n```\n\n## Storage Adapter Support\n\nResource-scoped working memory requires specific storage adapters that support the `mastra_resources` table:\n\n### âœ… Supported Storage Adapters\n- **LibSQL** (`@mastra/libsql`)\n- **PostgreSQL** (`@mastra/pg`)\n- **Upstash** (`@mastra/upstash`)\n\n## Custom Templates\n\nTemplates guide the agent on what information to track and update in working memory. While a default template is used if none is provided, you'll typically want to define a custom template tailored to your agent's specific use case to ensure it remembers the most relevant information.\n\nHere's an example of a custom template. In this example the agent will store the users name, location, timezone, etc as soon as the user sends a message containing any of the info:\n\n```typescript {5-28}\nconst memory = new Memory({\n  options: {\n    workingMemory: {\n      enabled: true,\n      template: `\n# User Profile\n\n## Personal Info\n\n- Name:\n- Location:\n- Timezone:\n\n## Preferences\n\n- Communication Style: [e.g., Formal, Casual]\n- Project Goal:\n- Key Deadlines:\n  - [Deadline 1]: [Date]\n  - [Deadline 2]: [Date]\n\n## Session State\n\n- Last Task Discussed:\n- Open Questions:\n  - [Question 1]\n  - [Question 2]\n`,\n    },\n  },\n});\n```\n\n## Designing Effective Templates\n\nA well-structured template keeps the information easy for the agent to parse and update. Treat the\ntemplate as a short form that you want the assistant to keep up to date.\n\n- **Short, focused labels.** Avoid paragraphs or very long headings. Keep labels brief (for example\n  `## Personal Info` or `- Name:`) so updates are easy to read and less likely to be truncated.\n- **Use consistent casing.** Inconsistent capitalization (`Timezone:` vs `timezone:`) can cause messy\n  updates. Stick to Title Case or lower case for headings and bullet labels.\n- **Keep placeholder text simple.** Use hints such as `[e.g., Formal]` or `[Date]` to help the LLM\n  fill in the correct spots.\n- **Abbreviate very long values.** If you only need a short form, include guidance like\n  `- Name: [First name or nickname]` or `- Address (short):` rather than the full legal text.\n- **Mention update rules in `instructions`.** You can instruct how and when to fill or clear parts of\n  the template directly in the agent's `instructions` field.\n\n### Alternative Template Styles\n\nUse a shorter single block if you only need a few items:\n\n```typescript\nconst basicMemory = new Memory({\n  options: {\n    workingMemory: {\n      enabled: true,\n      template: `User Facts:\\n- Name:\\n- Favorite Color:\\n- Current Topic:`,\n    },\n  },\n});\n```\n\nYou can also store the key facts in a short paragraph format if you prefer a more narrative style:\n\n```typescript\nconst paragraphMemory = new Memory({\n  options: {\n    workingMemory: {\n      enabled: true,\n      template: `Important Details:\\n\\nKeep a short paragraph capturing the user's important facts (name, main goal, current task).`,\n    },\n  },\n});\n```\n\n## Structured Working Memory\n\nWorking memory can also be defined using a structured schema instead of a Markdown template. This allows you to specify the exact fields and types that should be tracked, using a [Zod](https://zod.dev/) schema. When using a schema, the agent will see and update working memory as a JSON object matching your schema.\n\n**Important:** You must specify either `template` or `schema`, but not both.\n\n### Example: Schema-Based Working Memory\n\n```typescript\nimport { z } from 'zod';\nimport { Memory } from '@mastra/memory';\n\nconst userProfileSchema = z.object({\n  name: z.string().optional(),\n  location: z.string().optional(),\n  timezone: z.string().optional(),\n  preferences: z.object({\n    communicationStyle: z.string().optional(),\n    projectGoal: z.string().optional(),\n    deadlines: z.array(z.string()).optional(),\n  }).optional(),\n});\n\nconst memory = new Memory({\n  options: {\n    workingMemory: {\n      enabled: true,\n      schema: userProfileSchema,\n      // template: ... (do not set)\n    },\n  },\n});\n```\n\nWhen a schema is provided, the agent receives the working memory as a JSON object. For example:\n\n```json\n{\n  \"name\": \"Sam\",\n  \"location\": \"Berlin\",\n  \"timezone\": \"CET\",\n  \"preferences\": {\n    \"communicationStyle\": \"Formal\",\n    \"projectGoal\": \"Launch MVP\",\n    \"deadlines\": [\"2025-07-01\"]\n  }\n}\n```\n\n## Choosing Between Template and Schema\n\n- Use a **template** (Markdown) if you want the agent to maintain memory as a free-form text block, such as a user profile or scratchpad.\n- Use a **schema** if you need structured, type-safe data that can be validated and programmatically accessed as JSON.\n- Only one mode can be active at a time: setting both `template` and `schema` is not supported.\n\n## Example: Multi-step Retention\n\nBelow is a simplified view of how the `User Profile` template updates across a short user\nconversation:\n\n```nohighlight\n# User Profile\n\n## Personal Info\n\n- Name:\n- Location:\n- Timezone:\n\n--- After user says \"My name is **Sam** and I'm from **Berlin**\" ---\n\n# User Profile\n- Name: Sam\n- Location: Berlin\n- Timezone:\n\n--- After user adds \"By the way I'm normally in **CET**\" ---\n\n# User Profile\n- Name: Sam\n- Location: Berlin\n- Timezone: CET\n```\n\nThe agent can now refer to `Sam` or `Berlin` in later responses without requesting the information\nagain because it has been stored in working memory.\n\nIf your agent is not properly updating working memory when you expect it to, you can add system\ninstructions on _how_ and _when_ to use this template in your agent's `instructions` setting.\n\n## Setting Initial Working Memory\n\nWhile agents typically update working memory through the `updateWorkingMemory` tool, you can also set initial working memory programmatically when creating or updating threads. This is useful for injecting user data (like their name, preferences, or other info) that you want available to the agent without passing it in every request.\n\n### Setting Working Memory via Thread Metadata\n\nWhen creating a thread, you can provide initial working memory through the metadata's `workingMemory` key:\n\n```typescript filename=\"src/app/medical-consultation.ts\" showLineNumbers copy\n// Create a thread with initial working memory\nconst thread = await memory.createThread({\n  threadId: \"thread-123\",\n  resourceId: \"user-456\",\n  title: \"Medical Consultation\",\n  metadata: {\n    workingMemory: `# Patient Profile\n- Name: John Doe\n- Blood Type: O+\n- Allergies: Penicillin\n- Current Medications: None\n- Medical History: Hypertension (controlled)\n`\n  }\n});\n\n// The agent will now have access to this information in all messages\nawait agent.generate(\"What's my blood type?\", {\n  threadId: thread.id,\n  resourceId: \"user-456\"\n});\n// Response: \"Your blood type is O+.\"\n```\n\n### Updating Working Memory Programmatically\n\nYou can also update an existing thread's working memory:\n\n```typescript filename=\"src/app/medical-consultation.ts\" showLineNumbers copy\n// Update thread metadata to add/modify working memory\nawait memory.updateThread({\n  id: \"thread-123\",\n  title: thread.title,\n  metadata: {\n    ...thread.metadata,\n    workingMemory: `# Patient Profile\n- Name: John Doe\n- Blood Type: O+\n- Allergies: Penicillin, Ibuprofen  // Updated\n- Current Medications: Lisinopril 10mg daily  // Added\n- Medical History: Hypertension (controlled)\n`\n  }\n});\n```\n\n### Direct Memory Update\n\nAlternatively, use the `updateWorkingMemory` method directly:\n\n```typescript filename=\"src/app/medical-consultation.ts\" showLineNumbers copy\nawait memory.updateWorkingMemory({\n  threadId: \"thread-123\",\n  resourceId: \"user-456\", // Required for resource-scoped memory\n  workingMemory: \"Updated memory content...\"\n});\n```\n\n## Examples\n\n- [Basic working memory](/examples/memory/working-memory-basic)\n- [Working memory with template](/examples/memory/working-memory-template)\n- [Working memory with schema](/examples/memory/working-memory-schema)\n- [Per-resource working memory](https://github.com/mastra-ai/mastra/tree/main/examples/memory-per-resource-example) - Complete example showing resource-scoped memory persistence\n\n\n","path":null,"size_bytes":11780,"size_tokens":null},"src/triggers/telegramTriggers.ts":{"content":"/**\n * Telegram Trigger - Webhook-based Workflow Triggering\n *\n * This module provides Telegram bot event handling for Mastra workflows.\n * When Telegram messages are received, this trigger starts your workflow.\n *\n * PATTERN:\n * 1. Import registerTelegramTrigger and your workflow\n * 2. Call registerTelegramTrigger with a triggerType and handler\n * 3. Spread the result into the apiRoutes array in src/mastra/index.ts\n *\n * USAGE in src/mastra/index.ts:\n *\n * ```typescript\n * import { registerTelegramTrigger } from \"../triggers/telegramTriggers\";\n * import { telegramBotWorkflow } from \"./workflows/telegramBotWorkflow\";\n *\n * // In the apiRoutes array:\n * ...registerTelegramTrigger({\n *   triggerType: \"telegram/message\",\n *   handler: async (mastra, triggerInfo) => {\n *     const run = await telegramBotWorkflow.createRunAsync();\n *     return await run.start({ inputData: {} });\n *   }\n * })\n * ```\n */\n\nimport type { ContentfulStatusCode } from \"hono/utils/http-status\";\n\nimport { registerApiRoute } from \"../mastra/inngest\";\nimport { Mastra } from \"@mastra/core\";\n\nif (!process.env.BOT_TOKEN) {\n  console.warn(\n    \"Trying to initialize Telegram triggers without BOT_TOKEN. Can you confirm that the Telegram integration is configured correctly?\",\n  );\n}\n\nexport type TriggerInfoTelegramOnNewMessage = {\n  type: \"telegram/message\";\n  params: {\n    userName: string;\n    message: string;\n  };\n  payload: any;\n};\n\nexport function registerTelegramTrigger({\n  triggerType,\n  handler,\n}: {\n  triggerType: string;\n  handler: (\n    mastra: Mastra,\n    triggerInfo: TriggerInfoTelegramOnNewMessage,\n  ) => Promise<void>;\n}) {\n  return [\n    registerApiRoute(\"/webhooks/telegram/action\", {\n      method: \"POST\",\n      handler: async (c) => {\n        const mastra = c.get(\"mastra\");\n        const logger = mastra.getLogger();\n        try {\n          const payload = await c.req.json();\n\n          logger?.info(\"ðŸ“ [Telegram] payload\", payload);\n\n          const message = payload.message || payload.edited_message;\n          if (!message || !message.text) {\n            logger?.info(\"ðŸ“ [Telegram] Ignoring non-text message\");\n            return c.text(\"OK\", 200);\n          }\n\n          await handler(mastra, {\n            type: triggerType,\n            params: {\n              userName: message.from?.username || message.from?.first_name || \"unknown\",\n              message: message.text,\n            },\n            payload,\n          } as TriggerInfoTelegramOnNewMessage);\n\n          return c.text(\"OK\", 200);\n        } catch (error) {\n          logger?.error(\"Error handling Telegram webhook:\", error);\n          return c.text(\"Internal Server Error\", 500);\n        }\n      },\n    }),\n  ];\n}\n","path":null,"size_bytes":2700,"size_tokens":null},"docs/mastra/06-reference/66_processor-token-limiter.md":{"content":"---\ntitle: \"Reference: Token Limiter Processor | Processors | Mastra Docs\"\ndescription: \"Documentation for the TokenLimiterProcessor in Mastra, which limits the number of tokens in AI responses.\"\n---\n\n# TokenLimiterProcessor\n[EN] Source: https://mastra.ai/en/reference/processors/token-limiter-processor\n\nThe `TokenLimiterProcessor` is an **output processor** that limits the number of tokens in AI responses. This processor helps control response length by implementing token counting with configurable strategies for handling exceeded limits, including truncation and abortion options for both streaming and non-streaming scenarios.\n\n## Usage example\n\n```typescript copy\nimport { TokenLimiterProcessor } from \"@mastra/core/processors\";\n\nconst processor = new TokenLimiterProcessor({\n  limit: 1000,\n  strategy: \"truncate\",\n  countMode: \"cumulative\"\n});\n```\n\n## Constructor parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"options\",\n      type: \"number | Options\",\n      description: \"Either a simple number for token limit, or configuration options object\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n### Options\n\n<PropertiesTable\n  content={[\n    {\n      name: \"limit\",\n      type: \"number\",\n      description: \"Maximum number of tokens to allow in the response\",\n      isOptional: false,\n    },\n    {\n      name: \"encoding\",\n      type: \"TiktokenBPE\",\n      description: \"Optional encoding to use. Defaults to o200k_base which is used by gpt-4o\",\n      isOptional: true,\n      default: \"o200k_base\",\n    },\n    {\n      name: \"strategy\",\n      type: \"'truncate' | 'abort'\",\n      description: \"Strategy when token limit is reached: 'truncate' stops emitting chunks, 'abort' calls abort() to stop the stream\",\n      isOptional: true,\n      default: \"'truncate'\",\n    },\n    {\n      name: \"countMode\",\n      type: \"'cumulative' | 'part'\",\n      description: \"Whether to count tokens from the beginning of the stream or just the current part: 'cumulative' counts all tokens from start, 'part' only counts tokens in current part\",\n      isOptional: true,\n      default: \"'cumulative'\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"name\",\n      type: \"string\",\n      description: \"Processor name set to 'token-limiter'\",\n      isOptional: false,\n    },\n    {\n      name: \"processOutputStream\",\n      type: \"(args: { part: ChunkType; streamParts: ChunkType[]; state: Record<string, any>; abort: (reason?: string) => never }) => Promise<ChunkType | null>\",\n      description: \"Processes streaming output parts to limit token count during streaming\",\n      isOptional: false,\n    },\n    {\n      name: \"processOutputResult\",\n      type: \"(args: { messages: MastraMessageV2[]; abort: (reason?: string) => never }) => Promise<MastraMessageV2[]>\",\n      description: \"Processes final output results to limit token count in non-streaming scenarios\",\n      isOptional: false,\n    },\n    {\n      name: \"reset\",\n      type: \"() => void\",\n      description: \"Reset the token counter (useful for testing or reusing the processor)\",\n      isOptional: false,\n    },\n    {\n      name: \"getCurrentTokens\",\n      type: \"() => number\",\n      description: \"Get the current token count\",\n      isOptional: false,\n    },\n    {\n      name: \"getMaxTokens\",\n      type: \"() => number\",\n      description: \"Get the maximum token limit\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript filename=\"src/mastra/agents/limited-agent.ts\" showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { TokenLimiterProcessor } from \"@mastra/core/processors\";\n\nexport const agent = new Agent({\n  name: \"limited-agent\",\n  instructions: \"You are a helpful assistant\",\n  model: openai(\"gpt-4o-mini\"),\n  outputProcessors: [\n    new TokenLimiterProcessor({\n      limit: 1000,\n      strategy: \"truncate\",\n      countMode: \"cumulative\"\n    })\n  ]\n});\n```\n\n## Related\n\n- [Input Processors](/docs/agents/input-processors)\n- [Output Processors](/docs/agents/output-processors)\n\n\n","path":null,"size_bytes":4058,"size_tokens":null},"docs/mastra/06-reference/55_memory-get-threads-paginated.md":{"content":"---\ntitle: \"Reference: Memory.getThreadsByResourceIdPaginated() | Memory | Mastra Docs\"\ndescription: \"Documentation for the `Memory.getThreadsByResourceIdPaginated()` method in Mastra, which retrieves threads associated with a specific resource ID with pagination support.\"\n---\n\n# Memory.getThreadsByResourceIdPaginated()\n[EN] Source: https://mastra.ai/en/reference/memory/getThreadsByResourceIdPaginated\n\nThe `.getThreadsByResourceIdPaginated()` method retrieves threads associated with a specific resource ID with pagination support.\n\n## Usage Example\n\n```typescript copy\nawait memory.getThreadsByResourceIdPaginated({\n  resourceId: \"user-123\",\n  page: 0,\n  perPage: 10\n});\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"resourceId\",\n      type: \"string\",\n      description: \"The ID of the resource whose threads are to be retrieved\",\n      isOptional: false,\n    },\n    {\n      name: \"page\",\n      type: \"number\",\n      description: \"Page number to retrieve\",\n      isOptional: false,\n    },\n    {\n      name: \"perPage\",\n      type: \"number\",\n      description: \"Number of threads to return per page\",\n      isOptional: false,\n    },\n    {\n      name: \"orderBy\",\n      type: \"'createdAt' | 'updatedAt'\",\n      description: \"Field to sort threads by\",\n      isOptional: true,\n    },\n    {\n      name: \"sortDirection\",\n      type: \"'ASC' | 'DESC'\",\n      description: \"Sort order direction\",\n      isOptional: true,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"result\",\n      type: \"Promise<PaginationInfo & { threads: StorageThreadType[] }>\",\n      description: \"A promise that resolves to paginated thread results with metadata\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript filename=\"src/test-memory.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst agent = mastra.getAgent(\"agent\");\nconst memory = await agent.getMemory();\n\nlet currentPage = 0;\nlet hasMorePages = true;\n\nwhile (hasMorePages) {\n  const threads = await memory?.getThreadsByResourceIdPaginated({\n    resourceId: \"user-123\",\n    page: currentPage,\n    perPage: 25,\n    orderBy: \"createdAt\",\n    sortDirection: \"ASC\"\n  });\n\n  if (!threads) {\n    console.log(\"No threads\");\n    break;\n  }\n\n  threads.threads.forEach((thread) => {\n    console.log(`Thread: ${thread.id}, Created: ${thread.createdAt}`);\n  });\n\n  hasMorePages = threads.hasMore;\n  currentPage++;\n}\n```\n\n## Related\n\n- [Memory Class Reference](/reference/memory/Memory.mdx)\n- [getThreadsByResourceId](/reference/memory/getThreadsByResourceId.mdx) - Non-paginated version\n- [Getting Started with Memory](/docs/memory/overview.mdx) (Covers threads/resources concept)\n- [createThread](/reference/memory/createThread.mdx)\n- [getThreadById](/reference/memory/getThreadById.mdx)\n\n\n","path":null,"size_bytes":2785,"size_tokens":null},"docs/mastra/01-agents/06_generateLegacy.md":{"content":"---\ntitle: \"Reference: Agent.generateLegacy() (Legacy) | Agents | Mastra Docs\"\ndescription: \"Documentation for the legacy `Agent.generateLegacy()` method in Mastra agents. This method is deprecated and will be removed in a future version.\"\n---\n\nNOTE: Replit Playground UI __requires__ the usage of generateLegacy in the workflow's usage of agents.  You **cannot** use the SDK v5 (i.e. .generate methods).  This is for backwards compatibility, otherwise Replit Playground will break.\n\n# Agent.generateLegacy() (Legacy)\n[EN] Source: https://mastra.ai/en/reference/agents/generateLegacy\n\n<Callout type=\"warning\">\n  **Deprecated**: This method is deprecated and only works with V1 models. For V2 models, use the new [`.generate()`](./generate.mdx) method instead. See the [migration guide](../../guides/migrations/vnext-to-standard-apis) for details on upgrading.\n\n  NOTE: This is required for backwards compatibility for Replit Playground UI, so we must use it regardless of its future deprecation for current\n</Callout>\n\nThe `.generateLegacy()` method is the legacy version of the agent generation API, used to interact with V1 model agents to produce text or structured responses. This method accepts messages and optional generation options.\n\n## Usage example\n\n```typescript copy\nawait agent.generateLegacy(\"message for agent\");\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"messages\",\n      type: \"string | string[] | CoreMessage[] | AiMessageType[] | UIMessageWithMetadata[]\",\n      description: \"The messages to send to the agent. Can be a single string, array of strings, or structured message objects with multimodal content (text, images, etc.).\",\n    },\n    {\n      name: \"options\",\n      type: \"AgentGenerateOptions\",\n      isOptional: true,\n      description: \"Optional configuration for the generation process.\",\n    },\n  ]}\n/>\n\n### Options parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"abortSignal\",\n      type: \"AbortSignal\",\n      isOptional: true,\n      description:\n        \"Signal object that allows you to abort the agent's execution. When the signal is aborted, all ongoing operations will be terminated.\",\n    },\n    {\n      name: \"context\",\n      type: \"CoreMessage[]\",\n      isOptional: true,\n      description: \"Additional context messages to provide to the agent.\",\n    },\n    {\n      name: \"structuredOutput\",\n      type: \"StructuredOutputOptions<S extends ZodTypeAny = ZodTypeAny>\",\n      isOptional: true,\n      description: \"Enables structured output generation with better developer experience. Automatically creates and uses a StructuredOutputProcessor internally.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"schema\",\n            type: \"z.ZodSchema<S>\",\n            isOptional: false,\n            description: \"Zod schema to validate the output against.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"model\",\n            type: \"MastraLanguageModel\",\n            isOptional: false,\n            description: \"Model to use for the internal structuring agent.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"errorStrategy\",\n            type: \"'strict' | 'warn' | 'fallback'\",\n            isOptional: true,\n            description: \"Strategy when parsing or validation fails. Defaults to 'strict'.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"fallbackValue\",\n            type: \"<S extends ZodTypeAny>\",\n            isOptional: true,\n            description: \"Fallback value when errorStrategy is 'fallback'.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"instructions\",\n            type: \"string\",\n            isOptional: true,\n            description: \"Custom instructions for the structuring agent.\"\n          }]\n        },\n      ]\n    },\n    {\n      name: \"outputProcessors\",\n      type: \"Processor[]\",\n      isOptional: true,\n      description: \"Overrides the output processors set on the agent. Output processors that can modify or validate messages from the agent before they are returned to the user. Must implement either (or both) of the `processOutputResult` and `processOutputStream` functions.\",\n    },\n    {\n      name: \"inputProcessors\",\n      type: \"Processor[]\",\n      isOptional: true,\n      description: \"Overrides the input processors set on the agent. Input processors that can modify or validate messages before they are processed by the agent. Must implement the `processInput` function.\",\n    },\n    {\n      name: \"experimental_output\",\n      type: \"Zod schema | JsonSchema7\",\n      isOptional: true,\n      description:\n        \"Note, the preferred route is to use the `structuredOutput` property. Enables structured output generation alongside text generation and tool calls. The model will generate responses that conform to the provided schema.\",\n    },\n    {\n      name: \"instructions\",\n      type: \"string\",\n      isOptional: true,\n      description:\n        \"Custom instructions that override the agent's default instructions for this specific generation. Useful for dynamically modifying agent behavior without creating a new agent instance.\",\n    },\n    {\n      name: \"output\",\n      type: \"Zod schema | JsonSchema7\",\n      isOptional: true,\n      description:\n        \"Defines the expected structure of the output. Can be a JSON Schema object or a Zod schema.\",\n    },\n    {\n      name: \"memory\",\n      type: \"object\",\n      isOptional: true,\n      description: \"Configuration for memory. This is the preferred way to manage memory.\",\n      properties: [\n        {\n          parameters: [{\n              name: \"thread\",\n              type: \"string | { id: string; metadata?: Record<string, any>, title?: string }\",\n              isOptional: false,\n              description: \"The conversation thread, as a string ID or an object with an `id` and optional `metadata`.\"\n          }]\n        },\n        {\n          parameters: [{\n              name: \"resource\",\n              type: \"string\",\n              isOptional: false,\n              description: \"Identifier for the user or resource associated with the thread.\"\n          }]\n        },\n        {\n          parameters: [{\n              name: \"options\",\n              type: \"MemoryConfig\",\n              isOptional: true,\n              description: \"Configuration for memory behavior, like message history and semantic recall. See `MemoryConfig` below.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"maxSteps\",\n      type: \"number\",\n      isOptional: true,\n      defaultValue: \"5\",\n      description: \"Maximum number of execution steps allowed.\",\n    },\n    {\n      name: \"maxRetries\",\n      type: \"number\",\n      isOptional: true,\n      defaultValue: \"2\",\n      description: \"Maximum number of retries. Set to 0 to disable retries.\",\n    },\n    {\n      name: \"onStepFinish\",\n      type: \"GenerateTextOnStepFinishCallback<any> | never\",\n      isOptional: true,\n      description:\n        \"Callback function called after each execution step. Receives step details as a JSON string. Unavailable for structured output\",\n    },\n    {\n      name: \"runId\",\n      type: \"string\",\n      isOptional: true,\n      description: \"Unique ID for this generation run. Useful for tracking and debugging purposes.\",\n    },\n    {\n      name: \"telemetry\",\n      type: \"TelemetrySettings\",\n      isOptional: true,\n      description:\n        \"Settings for telemetry collection during generation.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"isEnabled\",\n            type: \"boolean\",\n            isOptional: true,\n            description: \"Enable or disable telemetry. Disabled by default while experimental.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"recordInputs\",\n            type: \"boolean\",\n            isOptional: true,\n            description: \"Enable or disable input recording. Enabled by default. You might want to disable input recording to avoid recording sensitive information.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"recordOutputs\",\n            type: \"boolean\",\n            isOptional: true,\n            description: \"Enable or disable output recording. Enabled by default. You might want to disable output recording to avoid recording sensitive information.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"functionId\",\n            type: \"string\",\n            isOptional: true,\n            description: \"Identifier for this function. Used to group telemetry data by function.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"temperature\",\n      type: \"number\",\n      isOptional: true,\n      description:\n        \"Controls randomness in the model's output. Higher values (e.g., 0.8) make the output more random, lower values (e.g., 0.2) make it more focused and deterministic.\",\n    },\n    {\n      name: \"toolChoice\",\n      type: \"'auto' | 'none' | 'required' | { type: 'tool'; toolName: string }\",\n      isOptional: true,\n      defaultValue: \"'auto'\",\n      description: \"Controls how the agent uses tools during generation.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"'auto'\",\n            type: \"string\",\n            description: \"Let the model decide whether to use tools (default).\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"'none'\",\n            type: \"string\",\n            description: \"Do not use any tools.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"'required'\",\n            type: \"string\",\n            description: \"Require the model to use at least one tool.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"{ type: 'tool'; toolName: string }\",\n            type: \"object\",\n            description: \"Require the model to use a specific tool by name.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"toolsets\",\n      type: \"ToolsetsInput\",\n      isOptional: true,\n      description:\n        \"Additional toolsets to make available to the agent during generation.\",\n    },\n    {\n      name: \"clientTools\",\n      type: \"ToolsInput\",\n      isOptional: true,\n      description:\n        \"Tools that are executed on the 'client' side of the request. These tools do not have execute functions in the definition.\",\n    },\n    {\n      name: \"savePerStep\",\n      type: \"boolean\",\n      isOptional: true,\n      description: \"Save messages incrementally after each stream step completes (default: false).\",\n    },\n    {\n      name: \"providerOptions\",\n      type: \"Record<string, Record<string, JSONValue>>\",\n      isOptional: true,\n      description: \"Additional provider-specific options that are passed through to the underlying LLM provider. The structure is `{ providerName: { optionKey: value } }`. Since Mastra extends AI SDK, see the [AI SDK documentation](https://sdk.vercel.ai/docs/providers/ai-sdk-providers) for complete provider options.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"openai\",\n            type: \"Record<string, JSONValue>\",\n            isOptional: true,\n            description: \"OpenAI-specific options. Example: `{ reasoningEffort: 'high' }`\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"anthropic\",\n            type: \"Record<string, JSONValue>\",\n            isOptional: true,\n            description: \"Anthropic-specific options. Example: `{ maxTokens: 1000 }`\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"google\",\n            type: \"Record<string, JSONValue>\",\n            isOptional: true,\n            description: \"Google-specific options. Example: `{ safetySettings: [...] }`\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"[providerName]\",\n            type: \"Record<string, JSONValue>\",\n            isOptional: true,\n            description: \"Other provider-specific options. The key is the provider name and the value is a record of provider-specific options.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      isOptional: true,\n      description: \"Runtime context for dependency injection and contextual information.\",\n    },\n    {\n      name: \"maxTokens\",\n      type: \"number\",\n      isOptional: true,\n      description: \"Maximum number of tokens to generate.\",\n    },\n    {\n      name: \"topP\",\n      type: \"number\",\n      isOptional: true,\n      description: \"Nucleus sampling. This is a number between 0 and 1. It is recommended to set either `temperature` or `topP`, but not both.\",\n    },\n    {\n      name: \"topK\",\n      type: \"number\",\n      isOptional: true,\n      description: \"Only sample from the top K options for each subsequent token. Used to remove 'long tail' low probability responses.\",\n    },\n    {\n      name: \"presencePenalty\",\n      type: \"number\",\n      isOptional: true,\n      description: \"Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. A number between -1 (increase repetition) and 1 (maximum penalty, decrease repetition).\",\n    },\n    {\n      name: \"frequencyPenalty\",\n      type: \"number\",\n      isOptional: true,\n      description: \"Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. A number between -1 (increase repetition) and 1 (maximum penalty, decrease repetition).\",\n    },\n    {\n      name: \"stopSequences\",\n      type: \"string[]\",\n      isOptional: true,\n      description: \"Stop sequences. If set, the model will stop generating text when one of the stop sequences is generated.\",\n    },\n    {\n      name: \"seed\",\n      type: \"number\",\n      isOptional: true,\n      description: \"The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.\",\n    },\n    {\n      name: \"headers\",\n      type: \"Record<string, string | undefined>\",\n      isOptional: true,\n      description: \"Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\",\n    }\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"text\",\n      type: \"string\",\n      isOptional: true,\n      description: \"The generated text response. Present when output is 'text' (no schema provided).\",\n    },\n    {\n      name: \"object\",\n      type: \"object\",\n      isOptional: true,\n      description: \"The generated structured response. Present when a schema is provided via `output`, `structuredOutput`, or `experimental_output`.\",\n    },\n    {\n      name: \"toolCalls\",\n      type: \"Array<ToolCall>\",\n      isOptional: true,\n      description: \"The tool calls made during the generation process. Present in both text and object modes.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"toolName\",\n            type: \"string\",\n            required: true,\n            description: \"The name of the tool invoked.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"args\",\n            type: \"any\",\n            required: true,\n            description: \"The arguments passed to the tool.\"\n          }]\n        }\n      ]\n    },\n  ]}\n/>\n\n## Migration to New API\n\n<Callout type=\"info\">\n  The new `.generate()` method offers enhanced capabilities including AI SDK v5 compatibility, better structured output handling, and improved streaming support. See the [migration guide](../../guides/migrations/vnext-to-standard-apis) for detailed migration instructions.\n</Callout>\n\n### Quick Migration Example\n\n#### Before (Legacy)\n```typescript\nconst result = await agent.generateLegacy(\"message\", {\n  temperature: 0.7,\n  maxSteps: 3\n});\n```\n\n#### After (New API)\n```typescript\nconst result = await agent.generate(\"message\", {\n  modelSettings: {\n    temperature: 0.7\n  },\n  maxSteps: 3\n});\n```\n\n## Extended usage example\n\n```typescript showLineNumbers copy\nimport { z } from \"zod\";\nimport { ModerationProcessor, TokenLimiterProcessor } from \"@mastra/core/processors\";\n\nawait agent.generateLegacy(\n  [\n    { role: \"user\", content: \"message for agent\" },\n    {\n      role: \"user\",\n      content: [\n        {\n          type: \"text\",\n          text: \"message for agent\"\n        },\n        {\n          type: \"image\",\n          imageUrl: \"https://example.com/image.jpg\",\n          mimeType: \"image/jpeg\"\n        }\n      ]\n    }\n  ],\n  {\n    temperature: 0.7,\n    maxSteps: 3,\n    memory: {\n      thread: \"user-123\",\n      resource: \"test-app\"\n    },\n    toolChoice: \"auto\",\n    providerOptions: {\n      openai: {\n        reasoningEffort: \"high\"\n      }\n    },\n    // Structured output with better DX\n    structuredOutput: {\n      schema: z.object({\n        sentiment: z.enum(['positive', 'negative', 'neutral']),\n        confidence: z.number(),\n      }),\n      model: openai(\"gpt-4o-mini\"),\n      errorStrategy: 'warn',\n    },\n    // Output processors for response validation\n    outputProcessors: [\n      new ModerationProcessor({ model: openai(\"gpt-4.1-nano\") }),\n      new TokenLimiterProcessor({ maxTokens: 1000 }),\n    ],\n  }\n);\n```\n\n## Related\n\n- [Migration Guide](../../guides/migrations/vnext-to-standard-apis)\n- [New .generate() method](./generate.mdx)\n- [Generating responses](../../docs/agents/overview.mdx#generating-responses)\n- [Streaming responses](../../docs/agents/overview.mdx#streaming-responses)","path":null,"size_bytes":17492,"size_tokens":null},"docs/mastra/01-agents/07_streamLegacy.md":{"content":"---\ntitle: \"Reference: Agent.streamLegacy() (Legacy) | Agents | Mastra Docs\"\ndescription: \"Documentation for the legacy `Agent.streamLegacy()` method in Mastra agents. This method is deprecated and will be removed in a future version.\"\n---\n\nimport { Callout } from 'nextra/components';\n\n# Agent.streamLegacy() (Legacy)\n[EN] Source: https://mastra.ai/en/reference/streaming/agents/streamLegacy\n\n<Callout type=\"warning\">\n  **Deprecated**: This method is deprecated and only works with V1 models. For V2 models, use the new [`.stream()`](./stream.mdx) method instead. See the [migration guide](../../../guides/migrations/vnext-to-standard-apis) for details on upgrading.\n</Callout>\n\nThe `.streamLegacy()` method is the legacy version of the agent streaming API, used for real-time streaming of responses from V1 model agents. This method accepts messages and optional streaming options.\n\n## Usage example\n\n```typescript copy\nawait agent.streamLegacy(\"message for agent\");\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"messages\",\n      type: \"string | string[] | CoreMessage[] | AiMessageType[] | UIMessageWithMetadata[]\",\n      description: \"The messages to send to the agent. Can be a single string, array of strings, or structured message objects.\",\n    },\n    {\n      name: \"options\",\n      type: \"AgentStreamOptions<OUTPUT, EXPERIMENTAL_OUTPUT>\",\n      isOptional: true,\n      description: \"Optional configuration for the streaming process.\",\n    },\n  ]}\n/>\n\n### Options parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"abortSignal\",\n      type: \"AbortSignal\",\n      isOptional: true,\n      description:\n        \"Signal object that allows you to abort the agent's execution. When the signal is aborted, all ongoing operations will be terminated.\",\n    },\n    {\n      name: \"context\",\n      type: \"CoreMessage[]\",\n      isOptional: true,\n      description: \"Additional context messages to provide to the agent.\",\n    },\n    {\n      name: \"experimental_output\",\n      type: \"Zod schema | JsonSchema7\",\n      isOptional: true,\n      description:\n        \"Enables structured output generation alongside text generation and tool calls. The model will generate responses that conform to the provided schema.\",\n    },\n    {\n      name: \"instructions\",\n      type: \"string\",\n      isOptional: true,\n      description:\n        \"Custom instructions that override the agent's default instructions for this specific generation. Useful for dynamically modifying agent behavior without creating a new agent instance.\",\n    },\n    {\n      name: \"output\",\n      type: \"Zod schema | JsonSchema7\",\n      isOptional: true,\n      description:\n        \"Defines the expected structure of the output. Can be a JSON Schema object or a Zod schema.\",\n    },\n    {\n      name: \"memory\",\n      type: \"object\",\n      isOptional: true,\n      description: \"Configuration for memory. This is the preferred way to manage memory.\",\n      properties: [\n        {\n          parameters: [{\n              name: \"thread\",\n              type: \"string | { id: string; metadata?: Record<string, any>, title?: string }\",\n              isOptional: false,\n              description: \"The conversation thread, as a string ID or an object with an `id` and optional `metadata`.\"\n          }]\n        },\n        {\n          parameters: [{\n              name: \"resource\",\n              type: \"string\",\n              isOptional: false,\n              description: \"Identifier for the user or resource associated with the thread.\"\n          }]\n        },\n        {\n          parameters: [{\n              name: \"options\",\n              type: \"MemoryConfig\",\n              isOptional: true,\n              description: \"Configuration for memory behavior, like message history and semantic recall.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"maxSteps\",\n      type: \"number\",\n      isOptional: true,\n      defaultValue: \"5\",\n      description: \"Maximum number of execution steps allowed.\",\n    },\n    {\n      name: \"maxRetries\",\n      type: \"number\",\n      isOptional: true,\n      defaultValue: \"2\",\n      description: \"Maximum number of retries. Set to 0 to disable retries.\",\n    },\n    {\n      name: \"memoryOptions\",\n      type: \"MemoryConfig\",\n      isOptional: true,\n      description:\n        \"**Deprecated.** Use `memory.options` instead. Configuration options for memory management.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"lastMessages\",\n            type: \"number | false\",\n            isOptional: true,\n            description: \"Number of recent messages to include in context, or false to disable.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"semanticRecall\",\n            type: \"boolean | { topK: number; messageRange: number | { before: number; after: number }; scope?: 'thread' | 'resource' }\",\n            isOptional: true,\n            description: \"Enable semantic recall to find relevant past messages. Can be a boolean or detailed configuration.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"workingMemory\",\n            type: \"WorkingMemory\",\n            isOptional: true,\n            description: \"Configuration for working memory functionality.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"threads\",\n            type: \"{ generateTitle?: boolean | { model: DynamicArgument<MastraLanguageModel>; instructions?: DynamicArgument<string> } }\",\n            isOptional: true,\n            description: \"Thread-specific configuration, including automatic title generation.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"onFinish\",\n      type: \"StreamTextOnFinishCallback<any> | StreamObjectOnFinishCallback<OUTPUT>\",\n      isOptional: true,\n      description:\n        \"Callback function called when streaming completes. Receives the final result.\",\n    },\n    {\n      name: \"onStepFinish\",\n      type: \"StreamTextOnStepFinishCallback<any> | never\",\n      isOptional: true,\n      description:\n        \"Callback function called after each execution step. Receives step details as a JSON string. Unavailable for structured output\",\n    },\n    {\n      name: \"resourceId\",\n      type: \"string\",\n      isOptional: true,\n      description:\n        \"**Deprecated.** Use `memory.resource` instead. Identifier for the user or resource interacting with the agent. Must be provided if threadId is provided.\",\n    },\n    {\n      name: \"telemetry\",\n      type: \"TelemetrySettings\",\n      isOptional: true,\n      description:\n        \"Settings for telemetry collection during streaming.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"isEnabled\",\n            type: \"boolean\",\n            isOptional: true,\n            description: \"Enable or disable telemetry. Disabled by default while experimental.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"recordInputs\",\n            type: \"boolean\",\n            isOptional: true,\n            description: \"Enable or disable input recording. Enabled by default. You might want to disable input recording to avoid recording sensitive information.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"recordOutputs\",\n            type: \"boolean\",\n            isOptional: true,\n            description: \"Enable or disable output recording. Enabled by default. You might want to disable output recording to avoid recording sensitive information.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"functionId\",\n            type: \"string\",\n            isOptional: true,\n            description: \"Identifier for this function. Used to group telemetry data by function.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"temperature\",\n      type: \"number\",\n      isOptional: true,\n      description:\n        \"Controls randomness in the model's output. Higher values (e.g., 0.8) make the output more random, lower values (e.g., 0.2) make it more focused and deterministic.\",\n    },\n    {\n      name: \"threadId\",\n      type: \"string\",\n      isOptional: true,\n      description:\n        \"**Deprecated.** Use `memory.thread` instead. Identifier for the conversation thread. Allows for maintaining context across multiple interactions. Must be provided if resourceId is provided.\",\n    },\n    {\n      name: \"toolChoice\",\n      type: \"'auto' | 'none' | 'required' | { type: 'tool'; toolName: string }\",\n      isOptional: true,\n      defaultValue: \"'auto'\",\n      description: \"Controls how the agent uses tools during streaming.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"'auto'\",\n            type: \"string\",\n            description: \"Let the model decide whether to use tools (default).\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"'none'\",\n            type: \"string\",\n            description: \"Do not use any tools.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"'required'\",\n            type: \"string\",\n            description: \"Require the model to use at least one tool.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"{ type: 'tool'; toolName: string }\",\n            type: \"object\",\n            description: \"Require the model to use a specific tool by name.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"toolsets\",\n      type: \"ToolsetsInput\",\n      isOptional: true,\n      description:\n        \"Additional toolsets to make available to the agent during streaming.\",\n    },\n    {\n      name: \"clientTools\",\n      type: \"ToolsInput\",\n      isOptional: true,\n      description:\n        \"Tools that are executed on the 'client' side of the request. These tools do not have execute functions in the definition.\",\n    },\n    {\n      name: \"savePerStep\",\n      type: \"boolean\",\n      isOptional: true,\n      description: \"Save messages incrementally after each stream step completes (default: false).\",\n    },\n    {\n      name: \"providerOptions\",\n      type: \"Record<string, Record<string, JSONValue>>\",\n      isOptional: true,\n      description: \"Additional provider-specific options that are passed through to the underlying LLM provider. The structure is `{ providerName: { optionKey: value } }`. For example: `{ openai: { reasoningEffort: 'high' }, anthropic: { maxTokens: 1000 } }`.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"openai\",\n            type: \"Record<string, JSONValue>\",\n            isOptional: true,\n            description: \"OpenAI-specific options. Example: `{ reasoningEffort: 'high' }`\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"anthropic\",\n            type: \"Record<string, JSONValue>\",\n            isOptional: true,\n            description: \"Anthropic-specific options. Example: `{ maxTokens: 1000 }`\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"google\",\n            type: \"Record<string, JSONValue>\",\n            isOptional: true,\n            description: \"Google-specific options. Example: `{ safetySettings: [...] }`\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"[providerName]\",\n            type: \"Record<string, JSONValue>\",\n            isOptional: true,\n            description: \"Other provider-specific options. The key is the provider name and the value is a record of provider-specific options.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"runId\",\n      type: \"string\",\n      isOptional: true,\n      description: \"Unique ID for this generation run. Useful for tracking and debugging purposes.\",\n    },\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      isOptional: true,\n      description: \"Runtime context for dependency injection and contextual information.\",\n    },\n    {\n      name: \"maxTokens\",\n      type: \"number\",\n      isOptional: true,\n      description: \"Maximum number of tokens to generate.\",\n    },\n    {\n      name: \"topP\",\n      type: \"number\",\n      isOptional: true,\n      description: \"Nucleus sampling. This is a number between 0 and 1. It is recommended to set either `temperature` or `topP`, but not both.\",\n    },\n    {\n      name: \"topK\",\n      type: \"number\",\n      isOptional: true,\n      description: \"Only sample from the top K options for each subsequent token. Used to remove 'long tail' low probability responses.\",\n    },\n    {\n      name: \"presencePenalty\",\n      type: \"number\",\n      isOptional: true,\n      description: \"Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. A number between -1 (increase repetition) and 1 (maximum penalty, decrease repetition).\",\n    },\n    {\n      name: \"frequencyPenalty\",\n      type: \"number\",\n      isOptional: true,\n      description: \"Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. A number between -1 (increase repetition) and 1 (maximum penalty, decrease repetition).\",\n    },\n    {\n      name: \"stopSequences\",\n      type: \"string[]\",\n      isOptional: true,\n      description: \"Stop sequences. If set, the model will stop generating text when one of the stop sequences is generated.\",\n    },\n    {\n      name: \"seed\",\n      type: \"number\",\n      isOptional: true,\n      description: \"The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.\",\n    },\n    {\n      name: \"headers\",\n      type: \"Record<string, string | undefined>\",\n      isOptional: true,\n      description: \"Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\",\n    }\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"textStream\",\n      type: \"AsyncGenerator<string>\",\n      isOptional: true,\n      description:\n        \"Async generator that yields text chunks as they become available.\",\n    },\n    {\n      name: \"fullStream\",\n      type: \"Promise<ReadableStream>\",\n      isOptional: true,\n      description:\n        \"Promise that resolves to a ReadableStream for the complete response.\",\n    },\n    {\n      name: \"text\",\n      type: \"Promise<string>\",\n      isOptional: true,\n      description:\n        \"Promise that resolves to the complete text response.\",\n    },\n    {\n      name: \"usage\",\n      type: \"Promise<{ totalTokens: number; promptTokens: number; completionTokens: number }>\",\n      isOptional: true,\n      description:\n        \"Promise that resolves to token usage information.\",\n    },\n    {\n      name: \"finishReason\",\n      type: \"Promise<string>\",\n      isOptional: true,\n      description:\n        \"Promise that resolves to the reason why the stream finished.\",\n    },\n    {\n      name: \"toolCalls\",\n      type: \"Promise<Array<ToolCall>>\",\n      isOptional: true,\n      description:\n        \"Promise that resolves to the tool calls made during the streaming process.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"toolName\",\n            type: \"string\",\n            required: true,\n            description: \"The name of the tool invoked.\"\n          }]\n        },\n        {\n          parameters: [{\n            name: \"args\",\n            type: \"any\",\n            required: true,\n            description: \"The arguments passed to the tool.\"\n          }]\n        }\n      ]\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript showLineNumbers copy\nawait agent.streamLegacy(\"message for agent\", {\n  temperature: 0.7,\n  maxSteps: 3,\n  memory: {\n    thread: \"user-123\",\n    resource: \"test-app\"\n  },\n  toolChoice: \"auto\"\n});\n```\n\n## Migration to New API\n\n<Callout type=\"info\">\n  The new `.stream()` method offers enhanced capabilities including AI SDK v5 compatibility, better structured output handling, and improved callback system. See the [migration guide](../../../guides/migrations/vnext-to-standard-apis) for detailed migration instructions.\n</Callout>\n\n### Quick Migration Example\n\n#### Before (Legacy)\n```typescript\nconst result = await agent.streamLegacy(\"message\", {\n  temperature: 0.7,\n  maxSteps: 3,\n  onFinish: (result) => console.log(result)\n});\n```\n\n#### After (New API)\n```typescript\nconst result = await agent.stream(\"message\", {\n  modelSettings: {\n    temperature: 0.7\n  },\n  maxSteps: 3,\n  onFinish: (result) => console.log(result)\n});\n```\n\n## Related\n\n- [Migration Guide](../../../guides/migrations/vnext-to-standard-apis)\n- [New .stream() method](./stream.mdx)\n- [Generating responses](../../../docs/agents/overview.mdx#generating-responses)\n- [Streaming responses](../../../docs/agents/overview.mdx#streaming-responses)","path":null,"size_bytes":16717,"size_tokens":null},"docs/mastra/00-getting-started/00_replit-automations.md":{"content":"# Replit Agents & Automations\n\nReplit Agents and Automations allows users to build agentic automations using the new TypeScript library called Mastra. Mastra works via Agent, Tool, and Workflows.  Workflows contain steps (which may call tools or call Agents). Mastra has many different ways of working, to aid our users and to allow for a more positive experiece in the Replit environment, we have imposed some additional constraints.\n\nNOTE: All Replit automations are 1.) time-based triggers or 2.) web-hook triggers (e.g. Slack, Telegram). For web-hook triggers, see the src/triggers/ folder for code examples.  This trigger activates the Workflow.\n\n\n## Replit Playground UI\nMastra has its own UI called Mastra Playground, however we have our own version that is setup specifically for the Replit environment.\n\nIt is critical to note that the Replit Playground is a UI that, you, the Replit Agent cannot interact with, **only** the user is capable of interacting with it.  One piece of the user interface is an Agent chat view, which allows the user to interact with the **single** agent that has been defined in the project.  There **must** only be a single agent defined for the project, or else it will not work.  Another constraint is that the Agent chat view calls the `.streamLegacy()` and `.generateLegacy()` methods on the Mastra Agent.  This means you **must** setup the AI SDK v4 (the ones compatible with the legacy Mastra stream and generate methods).  This has been delineated in the exampleAgent and exampleWorkflow typescript files.\n\nThe other view in the Replit Playground is the Workflow graph.  This pane allows the user to inspect the Mastra Workflow nodes with plain English.  It is critical, for this reason, that all nodes, tool, and agent `.description` fields are filled out in simple, plain English, since this is what will be shown to the user.  As you change the Workflow (add, remove, change nodes), the user will be able to see your work at a high-level.\n\n## Inngest for Agent Durability\n\nWe want users to have durable execution of the Mastra workflows.  So if a workflow fails in production, we can seemlessly pick up where we left off.  For this reason, we have a customization over the default Mastra deployment.  We use a service called inngest which directly works with Mastra.\n\nThe key area where this feature is added is in `src/mastra/index.ts`:\n```\nimport { inngest, inngestServe } from \"./inngest\";\n```\nand we keep custom code in the `src/mastra/inngest/` folder for handling the inngest durability layer over the Mastra server when we deploy/publish the automation on Replit's infrastructure.\n\nIt is critical to ensure that inngest is hooked up correctly, please make sure to preserve these properties in the root `src/mastra/index.ts`.","path":null,"size_bytes":2778,"size_tokens":null},"docs/mastra/01-agents/33_example-system-prompt.md":{"content":"---\ntitle: \"Example: Agents with a System Prompt | Agents | Mastra Docs\"\ndescription: Example of creating an AI agent in Mastra with a system prompt to define its personality and capabilities.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Changing the System Prompt\n[EN] Source: https://mastra.ai/en/examples/agents/system-prompt\n\nWhen creating an agent, the `instructions` define the general rules for its behavior. They establish the agentâ€™s role, personality, and overall approach, and remain consistent across all interactions. A `system` prompt can be passed to `.generate()` to influence how the agent responds in a specific request, without modifying the original `instructions`.\n\nIn this example, the `system` prompt is used to change the agentâ€™s voice between different Harry Potter characters, demonstrating how the same agent can adapt its style while keeping its core setup unchanged.\n\n## Prerequisites\n\nThis example uses the `openai` model. Make sure to add `OPENAI_API_KEY` to your `.env` file.\n\n```bash filename=\".env\" copy\nOPENAI_API_KEY=<your-api-key>\n```\n\n## Creating an agent\n\nDefine the agent and provide `instructions`, which set its default behavior and describe how it should respond when no system prompt is supplied at runtime.\n\n```typescript filename=\"src/mastra/agents/example-harry-potter-agent.ts\" showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nexport const harryPotterAgent = new Agent({\n  name: \"harry-potter-agent\",\n  description: \"Provides character-style responses from the Harry Potter universe.\",\n  instructions: `You are a character-voice assistant for the Harry Potter universe.\n    Reply in the speaking style of the requested character (e.g., Harry, Hermione, Ron, Dumbledore, Snape, Hagrid).\n    If no character is specified, default to Harry Potter.`,\n  model: openai(\"gpt-4o\")\n});\n```\n\n> See [Agent](../../reference/agents/agent.mdx) for a full list of configuration options.\n\n## Registering an agent\n\nTo use an agent, register it in your main Mastra instance.\n\n```typescript filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\n\nimport { harryPotterAgent } from \"./agents/example-harry-potter-agent\";\n\nexport const mastra = new Mastra({\n  // ...\n  agents: { harryPotterAgent }\n});\n```\n\n## Default character response\n\nUse `getAgent()` to retrieve the agent and call `generate()` with a prompt. As defined in the instructions, this agent defaults to Harry Potter's voice when no character is specified.\n\n```typescript filename=\"src/test-harry-potter-agent.ts\" showLineNumbers copy\nimport \"dotenv/config\";\n\nimport { mastra } from \"./mastra\";\n\nconst agent = mastra.getAgent(\"harryPotterAgent\");\n\nconst response = await agent.generate(\"What is your favorite room in Hogwarts?\");\n\nconsole.log(response.text);\n```\n\n### Changing the character voice\n\nBy providing a different system prompt at runtime, the agentâ€™s voice can be switched to another character. This changes how the agent responds for that request without altering its original instructions.\n\n```typescript {9-10} filename=\"src/test-harry-potter-agent.ts\" showLineNumbers copy\nimport \"dotenv/config\";\n\nimport { mastra } from \"./mastra\";\n\nconst agent = mastra.getAgent(\"harryPotterAgent\");\n\nconst response = await agent.generate([\n  {\n    role: \"system\",\n    content: \"You are Draco Malfoy.\"\n  },\n  {\n    role: \"user\",\n    content: \"What is your favorite room in Hogwarts?\"\n  }\n]);\n\nconsole.log(response.text);\n```\n\n<GithubLink\n  outdated={true}\n  marginTop='mt-16'\n  link=\"https://github.com/mastra-ai/mastra/blob/main/examples/basics/agents/system-prompt\"\n/>\n\n\n## Related\n\n- [Calling Agents](./calling-agents.mdx#from-the-command-line)\n\n\n","path":null,"size_bytes":3765,"size_tokens":null},"docs/mastra/06-reference/11_agent-get-workflows.md":{"content":"---\ntitle: \"Reference: Agent.getWorkflows() | Agents | Mastra Docs\"\ndescription: \"Documentation for the `Agent.getWorkflows()` method in Mastra agents, which retrieves the workflows that the agent can execute.\"\n---\n\n# Agent.getWorkflows()\n[EN] Source: https://mastra.ai/en/reference/agents/getWorkflows\n\nThe `.getWorkflows()` method retrieves the workflows configured for an agent, resolving them if they're a function. These workflows enable the agent to execute complex, multi-step processes with defined execution paths.\n\n## Usage example\n\n```typescript copy\nawait agent.getWorkflows();\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"options\",\n      type: \"{ runtimeContext?: RuntimeContext }\",\n      isOptional: true,\n      defaultValue: \"{}\",\n      description: \"Optional configuration object containing runtime context.\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflows\",\n      type: \"Promise<Record<string, Workflow>>\",\n      description: \"A promise that resolves to a record of workflow names to their corresponding Workflow instances.\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript copy\nawait agent.getWorkflows({\n  runtimeContext: new RuntimeContext()\n});\n```\n\n### Options parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      isOptional: true,\n      defaultValue: \"new RuntimeContext()\",\n      description: \"Runtime context for dependency injection and contextual information.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Agents overview](../../docs/agents/overview.mdx)\n- [Workflows overview](../../docs/workflows/overview.mdx)\n\n\n","path":null,"size_bytes":1663,"size_tokens":null},"docs/mastra/06-reference/99_workflow-send-event.md":{"content":"---\ntitle: \"Reference: Workflow.sendEvent() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow.sendEvent()` method in workflows, which resumes execution when an event is sent.\n---\n\n# Workflow.sendEvent()\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow-methods/sendEvent\n\nThe `.sendEvent()` resumes execution when an event is sent.\n\n## Usage example\n\n```typescript copy\nrun.sendEvent('event-name', { value: \"data\" });\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"eventName\",\n      type: \"string\",\n      description: \"The name of the event to send\",\n      isOptional: false,\n    },\n    {\n      name: \"step\",\n      type: \"Step\",\n      description: \"The step to resume after the event is sent\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"Workflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst run = await mastra.getWorkflow(\"testWorkflow\").createRunAsync();\n\nconst result = run.start({\n  inputData: {\n    value: \"hello\"\n  }\n});\n\nsetTimeout(() => {\n  run.sendEvent(\"event-name\", { value: \"from event\" });\n}, 3000);\n```\n\n> In this example, avoid using `await run.start()` directly, as it would block sending the event before the workflow reaches its waiting state.\n\n## Related\n\n- [.waitForEvent()](./waitForEvent.mdx)\n- [Suspend & Resume](../../../docs/workflows/suspend-and-resume.mdx#sleep--events)\n\n\n","path":null,"size_bytes":1580,"size_tokens":null},"docs/mastra/03-workflows/06_complex-llm-operations.md":{"content":"---\ntitle: \"Handling Complex LLM Operations | Workflows | Mastra\"\ndescription: \"Workflows in Mastra help you orchestrate complex sequences of tasks with features like branching, parallel execution, resource suspension, and more.\"\n---\n\nimport { Steps, Callout, Tabs } from \"nextra/components\";\n\n# Workflows overview\n[EN] Source: https://mastra.ai/en/docs/workflows/overview\n\nWorkflows let you define complex sequences of tasks using clear, structured steps rather than relying on the reasoning of a single agent. They give you full control over how tasks are broken down, how data moves between them, and what gets executed when.\n\n![Workflows overview](/image/workflows/workflows-overview.jpg)\n\n## When to use workflows\n\nUse workflows for tasks that are clearly defined upfront and involve multiple steps with a specific execution order. They give you fine-grained control over how data flows and transforms between steps, and which primitives are called at each stage.\n\n\n\n> **ðŸ“¹ Watch**:  â†’ An introduction to workflows, and how they compare to agents [YouTube (7 minutes)](https://youtu.be/0jg2g3sNvgw)\n\n## Core principles\n\nMastra workflows operate using these principles:\n\n- Defining **steps** with `createStep`, specifying input/output schemas and business logic.\n- Composing **steps** with `createWorkflow` to define the execution flow.\n- Running **workflows** to execute the entire sequence, with built-in support for suspension, resumption, and streaming results.\n\n## Creating a workflow step\n\nSteps are the building blocks of workflows. Create a step using `createStep()` with `inputSchema` and `outputSchema` to define the data it accepts and returns.\n\nThe `execute` function defines what the step does. Use it to call functions in your codebase, external APIs, agents, or tools.\n\n```typescript {6,9,15} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createStep } from \"@mastra/core/workflows\";\n\nconst step1 = createStep({\n  id: \"step-1\",\n  inputSchema: z.object({\n    message: z.string()\n  }),\n  outputSchema: z.object({\n    formatted: z.string()\n  }),\n  execute: async ({ inputData }) => {\n    const { message } = inputData;\n\n    return {\n      formatted: message.toUpperCase()\n    };\n  }\n});\n```\n\n> See the [Step Class](../../reference/workflows/step.mdx) for a full list of configuration options.\n\n### Using agents and tools\n\nWorkflow steps can also call registered agents or import and execute tools directly, visit the [Agents and Tools](./agents-and-tools.mdx) page for more information.\n\n## Creating a workflow\n\nCreate a workflow using `createWorkflow()` with `inputSchema` and `outputSchema` to define the data it accepts and returns. Add steps using `.then()` and complete the workflow with `.commit()`.\n\n```typescript {9,12,15,16} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\n\nexport const testWorkflow = createWorkflow({\n  id: \"test-workflow\",\n  inputSchema: z.object({\n    message: z.string()\n  }),\n  outputSchema: z.object({\n    output: z.string()\n  })\n})\n  .then(step1)\n  .commit();\n\n```\n\n> See the [Workflow Class](../../reference/workflows/workflow.mdx) for a full list of configuration options.\n\n### Understanding control flow\n\nWorkflows can be composed using a number of different methods. The method you choose determines how each step's schema should be structured. Visit the [Control Flow](./control-flow.mdx) page for more information.\n\n#### Composing workflow steps\n\nWhen using `.then()`, steps run sequentially. Each stepâ€™s `inputSchema` must match the `outputSchema` of the previous step. The final stepâ€™s `outputSchema` should match the workflowâ€™s `outputSchema` to ensure end-to-end type safety.\n\n```typescript {4,7,14,17,24,27} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nconst step1 = createStep({\n  //...\n  inputSchema: z.object({\n    message: z.string()\n  }),\n  outputSchema: z.object({\n    formatted: z.string()\n  })\n});\n\nconst step2 = createStep({\n  // ...\n  inputSchema: z.object({\n    formatted: z.string()\n  }),\n  outputSchema: z.object({\n    emphasized: z.string()\n  })\n});\n\nexport const testWorkflow = createWorkflow({\n  // ...\n  inputSchema: z.object({\n    message: z.string()\n  }),\n  outputSchema: z.object({\n    emphasized: z.string()\n  })\n})\n  .then(step1)\n  .then(step2)\n  .commit();\n```\n\n### Registering a workflow\n\nRegister your workflow in the Mastra instance to make it available throughout your application. Once registered, it can be called from agents or tools and has access to shared resources such as logging and observability features:\n\n```typescript {6} filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { testWorkflow } from \"./workflows/test-workflow\";\n\nexport const mastra = new Mastra({\n  // ...\n  workflows: { testWorkflow },\n});\n```\n\n## Referencing a workflow\n\nYou can run workflows from agents, tools, the Mastra Client, or the command line. Get a reference by calling `.getWorkflow()` on your `mastra` or `mastraClient` instance, depending on your setup:\n\n```typescript showLineNumbers copy\nconst testWorkflow = mastra.getWorkflow(\"testWorkflow\");\n```\n<Callout type=\"info\">\n  <p>\n    `mastra.getWorkflow()` is preferred over a direct import, since it provides access to the Mastra instance configuration (logger, telemetry, storage, registered agents, and vector stores).\n  </p>\n</Callout>\n\n> See [Running Workflows](../../examples/workflows/running-workflows.mdx) for more information.\n\n## Running workflows\n\nWorkflows can be run in two modes: start waits for all steps to complete before returning, and stream emits events during execution. Choose the approach that fits your use case: start when you only need the final result, and stream when you want to monitor progress or trigger actions as steps complete.\n\n<Tabs items={[\"Start\", \"Stream\"]}>\n  <Tabs.Tab>\nCreate a workflow run instance using `createRunAsync()`, then call `.start()` with `inputData` matching the workflow's `inputSchema`. The workflow executes all steps and returns the final result.\n\n```typescript showLineNumbers copy\nconst run = await testWorkflow.createRunAsync();\n\nconst result = await run.start({\n  inputData: {\n    message: \"Hello world\"\n  }\n});\n\nconsole.log(result);\n```\n  </Tabs.Tab>\n  <Tabs.Tab>\nCreate a workflow run instance using `.createRunAsync()`, then call `.stream()` with `inputData` matching the workflow's `inputSchema`. The workflow emits events as each step executes, which you can iterate over to track progress.\n\n```typescript showLineNumbers copy\nconst run = await testWorkflow.createRunAsync();\n\nconst result = await run.stream({\n  inputData: {\n    message: \"Hello world\"\n  }\n});\n\nfor await (const chunk of result.stream) {\n  console.log(chunk);\n}\n```\n  </Tabs.Tab>\n</Tabs>\n\n## Workflow output\n\nThe workflow output includes the full execution lifecycle, showing the input and output for each step. It also includes the status of each step, the overall workflow status, and the final result. This gives you clear insight into how data moved through the workflow, what each step produced, and how the workflow completed.\n\n```json\n{\n  \"status\": \"success\",\n  \"steps\": {\n    // ...\n    \"step-1\": {\n      \"status\": \"success\",\n      \"payload\": {\n        \"message\": \"Hello world\"\n      },\n      \"output\": {\n        \"formatted\": \"HELLO WORLD\"\n      },\n    },\n    \"step-2\": {\n      \"status\": \"success\",\n      \"payload\": {\n        \"formatted\": \"HELLO WORLD\"\n      },\n      \"output\": {\n        \"emphasized\": \"HELLO WORLD!!!\"\n      },\n    }\n  },\n  \"input\": {\n    \"message\": \"Hello world\"\n  },\n  \"result\": {\n    \"emphasized\": \"HELLO WORLD!!!\"\n  }\n}\n```\n\n## Using `RuntimeContext`\n\nUse [RuntimeContext](../server-db/runtime-context.mdx) to access request-specific values. This lets you conditionally adjust behavior based on the context of the request.\n\n```typescript filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers\nexport type UserTier = {\n  \"user-tier\": \"enterprise\" | \"pro\";\n};\n\nconst step1 = createStep({\n  // ...\n  execute: async ({ runtimeContext }) => {\n    const userTier = runtimeContext.get(\"user-tier\") as UserTier[\"user-tier\"];\n\n    const maxResults = userTier === \"enterprise\"\n      ? 1000\n      : 50;\n\n    return { maxResults };\n  }\n});\n```\n\n> See [Runtime Context](../server-db/runtime-context.mdx) for more information.\n\n## Testing with Mastra Playground\n\nUse the Mastra [Playground](../server-db/local-dev-playground.mdx) to easily run workflows with different inputs, visualize the execution lifecycle, see the inputs and outputs for each step, and inspect each part of the workflow in more detail.\n\n## Related\n\nFor a closer look at workflows, see our [Workflow Guide](../../guides/guide/ai-recruiter.mdx), which walks through the core concepts with a practical example.\n\n- [Parallel Steps workflow example](../../examples/workflows/parallel-steps.mdx)\n- [Conditional Branching workflow example](../../examples/workflows/conditional-branching.mdx)\n- [Inngest workflow example](../../examples/workflows/inngest-workflow.mdx)\n- [Suspend and Resume workflow example](../../examples/workflows/human-in-the-loop.mdx)\n\n\n## Workflows (Legacy)\n\nFor legacy workflow documentation, see [Workflows (Legacy)](../workflows-legacy/overview.mdx).\n\n\n\n","path":null,"size_bytes":9405,"size_tokens":null},"docs/mastra/06-reference/103_workflow-wait-for-event.md":{"content":"---\ntitle: \"Reference: Workflow.waitForEvent() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow.waitForEvent()` method in workflows, which pauses execution until an event is received.\n---\n\n# Workflow.waitForEvent()\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow-methods/waitForEvent\n\nThe `.waitForEvent()` method pauses execution until an event is received.\n\n## Usage example\n\n```typescript copy\nworkflow.waitForEvent('event-name', step1);\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"eventName\",\n      type: \"string\",\n      description: \"The name of the event to wait for\",\n      isOptional: false,\n    },\n    {\n      name: \"step\",\n      type: \"Step\",\n      description: \"The step to resume after the event is received\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"Workflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\n\nconst step1 = createStep({...});\nconst step2 = createStep({...});\nconst step3 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .waitForEvent(\"event-name\", step2)\n  .then(step3)\n  .commit();\n```\n\n## Related\n\n- [.sendEvent()](./sendEvent.mdx)\n- [Suspend & Resume](../../../docs/workflows/suspend-and-resume.mdx#sleep--events)\n\n\n","path":null,"size_bytes":1500,"size_tokens":null},"docs/mastra/06-reference/113_run-start.md":{"content":"---\ntitle: \"Reference: Run.start() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Run.start()` method in workflows, which starts a workflow run with input data.\n---\n\n# Run.start()\n[EN] Source: https://mastra.ai/en/reference/workflows/run-methods/start\n\nThe `.start()` method starts a workflow run with input data, allowing you to execute the workflow from the beginning.\n\n## Usage example\n\n```typescript showLineNumbers copy\nconst run = await workflow.createRunAsync();\n\nconst result = await run.start({\n  inputData: {\n    value: \"initial data\",\n  },\n});\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"inputData\",\n      type: \"z.infer<TInput>\",\n      description: \"Input data that matches the workflow's input schema\",\n      isOptional: true,\n    },\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      description: \"Runtime context data to use during workflow execution\",\n      isOptional: true,\n    },\n    {\n      name: \"writableStream\",\n      type: \"WritableStream<ChunkType>\",\n      description: \"Optional writable stream for streaming workflow output\",\n      isOptional: true,\n    },\n    {\n      name: \"tracingContext\",\n      type: \"TracingContext\",\n      isOptional: true,\n      description: \"AI tracing context for creating child spans and adding metadata. Automatically injected when using Mastra's tracing system.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"currentSpan\",\n            type: \"AISpan\",\n            isOptional: true,\n            description: \"Current AI span for creating child spans and adding metadata. Use this to create custom child spans or update span attributes during execution.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"tracingOptions\",\n      type: \"TracingOptions\",\n      isOptional: true,\n      description: \"Options for AI tracing configuration.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"metadata\",\n            type: \"Record<string, any>\",\n            isOptional: true,\n            description: \"Metadata to add to the root trace span. Useful for adding custom attributes like user IDs, session IDs, or feature flags.\"\n          }]\n        }\n      ]\n    },\n    {\n      name: \"outputOptions\",\n      type: \"OutputOptions\",\n      isOptional: true,\n      description: \"Options for AI tracing configuration.\",\n      properties: [\n        {\n          parameters: [{\n            name: \"includeState\",\n            type: \"boolean\",\n            isOptional: true,\n            description: \"Whether to include the workflow run state in the result.\"\n          }]\n        }\n      ]\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"result\",\n      type: \"Promise<WorkflowResult<TState, TOutput, TSteps>>\",\n      description: \"A promise that resolves to the workflow execution result containing step outputs and status\",\n    },\n    {\n      name: \"traceId\",\n      type: \"string\",\n      isOptional: true,\n      description: \"The trace ID associated with this execution when AI tracing is enabled. Use this to correlate logs and debug execution flow.\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript showLineNumbers copy\nimport { RuntimeContext } from \"@mastra/core/runtime-context\";\n\nconst run = await workflow.createRunAsync();\n\nconst runtimeContext = new RuntimeContext();\nruntimeContext.set(\"variable\", false);\n\nconst result = await run.start({\n  inputData: {\n    value: \"initial data\"\n  },\n  runtimeContext\n});\n```\n\n## Related\n\n- [Workflows overview](../../../docs/workflows/overview.mdx#run-workflow)\n- [Workflow.createRunAsync()](../create-run.mdx)\n\n\n","path":null,"size_bytes":3649,"size_tokens":null},"docs/mastra/06-reference/32_mastra-get-agents.md":{"content":"---\ntitle: \"Reference: Mastra.getAgents() | Core | Mastra Docs\"\ndescription: \"Documentation for the `Mastra.getAgents()` method in Mastra, which retrieves all configured agents.\"\n---\n\n# Mastra.getAgents()\n[EN] Source: https://mastra.ai/en/reference/core/getAgents\n\nThe `.getAgents()` method is used to retrieve all agents that have been configured in the Mastra instance.\n\n## Usage example\n\n```typescript copy\nmastra.getAgents();\n```\n\n## Parameters\n\nThis method does not accept any parameters.\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"agents\",\n      type: \"TAgents\",\n      description: \"A record of all configured agents, where keys are agent names and values are agent instances.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Agents overview](../../docs/agents/overview.mdx)\n- [Dynamic agents](../../docs/agents/dynamic-agents.mdx)\n\n\n","path":null,"size_bytes":843,"size_tokens":null},"docs/mastra/06-reference/12_agent-list-agents.md":{"content":"---\ntitle: \"Reference: Agent.listAgents() | Agents | Mastra Docs\"\ndescription: \"Documentation for the `Agent.listAgents()` method in Mastra agents, which retrieves the sub-agents that the agent can access.\"\n---\n\n# Agent.listAgents()\n[EN] Source: https://mastra.ai/en/reference/agents/listAgents\n\nThe `.listAgents()` method retrieves the sub-agents configured for an agent, resolving them if they're a function. These sub-agents enable the agent to access other agents and perform complex actions.\n\n## Usage example\n\n```typescript copy\nawait agent.listAgents();\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"options\",\n      type: \"{ runtimeContext?: RuntimeContext }\",\n      isOptional: true,\n      defaultValue: \"{}\",\n      description: \"Optional configuration object containing runtime context.\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"agents\",\n      type: \"Promise<Record<string, Agent>>\",\n      description: \"A promise that resolves to a record of agent names to their corresponding Agent instances.\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript copy\nimport { RuntimeContext } from \"@mastra/core/runtime-context\";\n\nawait agent.listAgents({\n  runtimeContext: new RuntimeContext()\n});\n```\n\n### Options parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      isOptional: true,\n      defaultValue: \"new RuntimeContext()\",\n      description: \"Runtime context for dependency injection and contextual information.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Agents overview](../../docs/agents/overview.mdx)\n\n\n","path":null,"size_bytes":1626,"size_tokens":null},"docs/mastra/06-reference/07_agent-get-memory.md":{"content":"---\ntitle: \"Reference: Agent.getMemory() | Agents | Mastra Docs\"\ndescription: \"Documentation for the `Agent.getMemory()` method in Mastra agents, which retrieves the memory system associated with the agent.\"\n---\n\n# Agent.getMemory()\n[EN] Source: https://mastra.ai/en/reference/agents/getMemory\n\nThe `.getMemory()` method retrieves the memory system associated with an agent. This method is used to access the agent's memory capabilities for storing and retrieving information across conversations.\n\n## Usage example\n\n```typescript copy\nawait agent.getMemory();\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"options\",\n      type: \"{ runtimeContext?: RuntimeContext }\",\n      isOptional: true,\n      defaultValue: \"{}\",\n      description: \"Optional configuration object containing runtime context.\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"memory\",\n      type: \"Promise<MastraMemory | undefined>\",\n      description: \"A promise that resolves to the memory system configured for the agent, or undefined if no memory system is configured.\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript copy\nawait agent.getMemory({\n  runtimeContext: new RuntimeContext()\n});\n```\n\n### Options parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"runtimeContext\",\n      type: \"RuntimeContext\",\n      isOptional: true,\n      defaultValue: \"new RuntimeContext()\",\n      description: \"Runtime context for dependency injection and contextual information.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Agent memory](../../docs/agents/agent-memory.mdx)\n- [Runtime Context](../../docs/server-db/runtime-context.mdx)\n\n\n","path":null,"size_bytes":1655,"size_tokens":null},"docs/mastra/01-agents/00_agent-overview.md":{"content":"---\ntitle: \"Agent Overview | Agents | Mastra Docs\"\ndescription: Overview of agents in Mastra, detailing their capabilities and how they interact with tools, workflows, and external systems.\n---\n\nimport { Steps, Callout, Tabs } from \"nextra/components\";\n\n# Using Agents\n[EN] Source: https://mastra.ai/en/docs/agents/overview\n\nAgents use LLMs and tools to solve open-ended tasks. They reason about goals, decide which tools to use, retain conversation memory, and iterate internally until the model emits a final answer or an optional stop condition is met. Agents produce structured responses you can render in your UI or process programmatically. Use agents directly or compose them into workflows or agent networks.\n\n![Agents overview](/image/agents/agents-overview.jpg)\n\n> **ðŸ“¹ Watch**:  â†’ An introduction to agents, and how they compare to workflows [YouTube (7 minutes)](https://youtu.be/0jg2g3sNvgw)\n\n## Setting up agents\n\n<Tabs items={[\"Mastra model router\", \"Vercel AI SDK\"]}>\n  <Tabs.Tab>\n    <Steps>\n### Install dependencies [#install-dependencies-mastra-router]\n\nAdd the Mastra core package to your project:\n\n```bash\nnpm install @mastra/core\n```\n\n### Set your API key [#set-api-key-mastra-router]\n\nMastra's model router auto-detects environment variables for your chosen provider. For OpenAI, set `OPENAI_API_KEY`:\n\n```bash filename=\".env\" copy\nOPENAI_API_KEY=<your-api-key>\n```\n\n> Mastra supports more than 600 models. Choose from the full list [here](/models).\n\n### Creating an agent [#creating-an-agent-mastra-router]\n\nCreate an agent by instantiating the `Agent` class with system `instructions` and a `model`:\n\n```typescript filename=\"src/mastra/agents/test-agent.ts\" showLineNumbers copy\nimport { Agent } from \"@mastra/core/agent\";\n\nexport const testAgent = new Agent({\n  name: \"test-agent\",\n  instructions: \"You are a helpful assistant.\",\n  model: \"openai/gpt-4o-mini\"\n});\n```\n    </Steps>\n  </Tabs.Tab>\n  <Tabs.Tab>\n    <Steps>\n\n### Install dependencies [#install-dependencies-ai-sdk]\n\nInclude the Mastra core package alongside the Vercel AI SDK provider you want to use:\n\n```bash\nnpm install @mastra/core @ai-sdk/openai\n```\n\n### Set your API key [#set-api-key-ai-sdk]\n\nSet the corresponding environment variable for your provider. For OpenAI via the AI SDK:\n\n```bash filename=\".env\" copy\nOPENAI_API_KEY=<your-api-key>\n```\n\n> See the [AI SDK Providers](https://ai-sdk.dev/providers/ai-sdk-providers) in the Vercel AI SDK docs for additional configuration options.\n\n### Creating an agent [#creating-an-agent-ai-sdk]\n\nTo create an agent in Mastra, use the `Agent` class. Every agent must include `instructions` to define its behavior, and a `model` parameter to specify the LLM provider and model. When using the Vercel AI SDK, provide the client to your agent's `model` field:\n\n```typescript filename=\"src/mastra/agents/test-agent.ts\" showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nexport const testAgent = new Agent({\n  name: \"test-agent\",\n  instructions: \"You are a helpful assistant.\",\n  model: openai(\"gpt-4o-mini\")\n});\n```\n    </Steps>\n  </Tabs.Tab>\n</Tabs>\n\n#### Instruction formats\n\nInstructions define the agent's behavior, personality, and capabilities.\nThey are system-level prompts that establish the agent's core identity and expertise.\n\nInstructions can be provided in multiple formats for greater flexibility. The examples below illustrate the supported shapes:\n\n```typescript copy\n// String (most common)\ninstructions: \"You are a helpful assistant.\"\n\n// Array of strings\ninstructions: [\n  \"You are a helpful assistant.\",\n  \"Always be polite.\",\n  \"Provide detailed answers.\"\n]\n\n// Array of system messages\ninstructions: [\n  { role: \"system\", content: \"You are a helpful assistant.\" },\n  { role: \"system\", content: \"You have expertise in TypeScript.\" }\n]\n```\n\n#### Provider-specific options\n\nEach model provider also enables a few different options, including prompt caching and configuring reasoning. We provide a `providerOptions` flag to manage these. You can set `providerOptions` on the instruction level to set different caching strategy per system instruction/prompt.\n\n```typescript copy\n// With provider-specific options (e.g., caching, reasoning)\ninstructions: {\n  role: \"system\",\n  content:\n    \"You are an expert code reviewer. Analyze code for bugs, performance issues, and best practices.\",\n  providerOptions: {\n    openai: { reasoningEffort: \"high\" },        // OpenAI's reasoning models\n    anthropic: { cacheControl: { type: \"ephemeral\" } }  // Anthropic's prompt caching\n  }\n}\n```\n\n> See the [Agent reference doc](../../reference/agents/agent.mdx) for more information.\n\n### Registering an agent\n\nRegister your agent in the Mastra instance to make it available throughout your application. Once registered, it can be called from workflows, tools, or other agents, and has access to shared resources such as memory, logging, and observability features:\n\n```typescript {6} showLineNumbers filename=\"src/mastra/index.ts\" copy\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { testAgent } from './agents/test-agent';\n\nexport const mastra = new Mastra({\n  // ...\n  agents: { testAgent },\n});\n```\n\n## Referencing an agent\n\nYou can call agents from workflow steps, tools, the Mastra Client, or the command line. Get a reference by calling `.getAgent()` on your `mastra` or `mastraClient` instance, depending on your setup:\n\n```typescript showLineNumbers copy\nconst testAgent = mastra.getAgent(\"testAgent\");\n```\n<Callout type=\"info\">\n  <p>\n    `mastra.getAgent()` is preferred over a direct import, since it provides access to the Mastra instance configuration (logger, telemetry, storage, registered agents, and vector stores).\n  </p>\n</Callout>\n\n> See [Calling agents](../../examples/agents/calling-agents.mdx) for more information.\n\n## Generating responses\n\nAgents can return results in two ways: generating the full output before returning it or streaming tokens in real time. Choose the approach that fits your use case: generate for short, internal responses or debugging, and stream to deliver pixels to end users as quickly as possible.\n\n<Tabs items={[\"Generate\", \"Stream\"]}>\n  <Tabs.Tab>\nPass a single string for simple prompts, an array of strings when providing multiple pieces of context, or an array of message objects with `role` and `content`.\n\n(The `role` defines the speaker for each message. Typical roles are `user` for human input, `assistant` for agent responses, and `system` for instructions.)\n\n```typescript showLineNumbers copy\nconst response = await testAgent.generate([\n  { role: \"user\", content: \"Help me organize my day\" },\n  { role: \"user\", content: \"My day starts at 9am and finishes at 5.30pm\" },\n  { role: \"user\", content: \"I take lunch between 12:30 and 13:30\" },\n  { role: \"user\", content: \"I have meetings Monday to Friday between 10:30 and 11:30\" }\n]);\n\nconsole.log(response.text);\n```\n  </Tabs.Tab>\n  <Tabs.Tab>\nPass a single string for simple prompts, an array of strings when providing multiple pieces of context, or an array of message objects with `role` and `content`.\n\n(The `role` defines the speaker for each message. Typical roles are `user` for human input, `assistant` for agent responses, and `system` for instructions.)\n\n```typescript showLineNumbers copy\nconst stream = await testAgent.stream([\n  { role: \"user\", content: \"Help me organize my day\" },\n  { role: \"user\", content: \"My day starts at 9am and finishes at 5.30pm\" },\n  { role: \"user\", content: \"I take lunch between 12:30 and 13:30\" },\n  { role: \"user\", content: \"I have meetings Monday to Friday between 10:30 and 11:30\" }\n]);\n\nfor await (const chunk of stream.textStream) {\n  process.stdout.write(chunk);\n}\n```\n\n### Completion using `onFinish()`\n\nWhen streaming responses, the `onFinish()` callback runs after the LLM finishes generating its response and all tool executions are complete.\nIt provides the final `text`, execution `steps`, `finishReason`, token `usage` statistics, and other metadata useful for monitoring or logging.\n\n```typescript showLineNumbers copy\nconst stream = await testAgent.stream(\"Help me organize my day\", {\n  onFinish: ({ steps, text, finishReason, usage }) => {\n    console.log({ steps, text, finishReason, usage });\n  }\n});\n\nfor await (const chunk of stream.textStream) {\n  process.stdout.write(chunk);\n}\n```\n  </Tabs.Tab>\n</Tabs>\n\n> See [.generate()](../../reference/agents/generate.mdx) or [.stream()](../../reference/agents/stream.mdx) for more information.\n\n## Structured output\n\nAgents can return structured, type-safe data by defining the expected output using either [Zod](https://zod.dev/) or [JSON Schema](https://json-schema.org/). We recommend Zod for better TypeScript support and developer experience. The parsed result is available on `response.object`, allowing you to work directly with validated and typed data.\n\n### Using Zod\n\nDefine the `output` shape using [Zod](https://zod.dev/):\n\n```typescript showLineNumbers copy\nimport { z } from \"zod\";\n\nconst response = await testAgent.generate(\n  [\n    {\n      role: \"system\",\n      content: \"Provide a summary and keywords for the following text:\"\n    },\n    {\n      role: \"user\",\n      content: \"Monkey, Ice Cream, Boat\"\n    }\n  ],\n  {\n    structuredOutput: {\n      schema: z.object({\n        summary: z.string(),\n        keywords: z.array(z.string())\n      })\n    },\n  }\n);\n\nconsole.log(response.object);\n```\n\n### With Tool Calling\n\nUse the `model` property to ensure that your agent can execute multi-step LLM calls with tool calling.\n\n```typescript showLineNumbers copy\nimport { z } from \"zod\";\n\nconst response = await testAgentWithTools.generate(\n  [\n    {\n      role: \"system\",\n      content: \"Provide a summary and keywords for the following text:\"\n    },\n    {\n      role: \"user\",\n      content: \"Please use your test tool and let me know the results\"\n    }\n  ],\n  {\n    structuredOutput: {\n      schema: z.object({\n        summary: z.string(),\n        keywords: z.array(z.string())\n      }),\n      model: \"openai/gpt-4o\"\n    },\n  }\n);\n\nconsole.log(response.object);\nconsole.log(response.toolResults)\n```\n\n### Response format\n\nBy default `structuredOutput` will use `response_format` to pass the schema to the model provider. If the model provider does not natively support `response_format` it's possible that this will error or not give the desired results. To keep using the same model use `jsonPromptInjection` to bypass response format and inject a system prompt message to coerce the model to return structured output.\n\n```typescript showLineNumbers copy\nimport { z } from \"zod\";\n\nconst response = await testAgentThatDoesntSupportStructuredOutput.generate(\n  [\n    {\n      role: \"system\",\n      content: \"Provide a summary and keywords for the following text:\"\n    },\n    {\n      role: \"user\",\n      content: \"Monkey, Ice Cream, Boat\"\n    }\n  ],\n  {\n    structuredOutput: {\n      schema: z.object({\n        summary: z.string(),\n        keywords: z.array(z.string())\n      }),\n      jsonPromptInjection: true\n    },\n  }\n);\n\nconsole.log(response.object);\n```\n\n\n## Working with images\n\nAgents can analyze and describe images by processing both the visual content and any text within them. To enable image analysis, pass an object with `type: 'image'` and the image URL in the `content` array. You can combine image content with text prompts to guide the agent's analysis.\n\n```typescript showLineNumbers copy\nconst response = await testAgent.generate([\n  {\n    role: \"user\",\n    content: [\n      {\n        type: \"image\",\n        image: \"https://placebear.com/cache/395-205.jpg\",\n        mimeType: \"image/jpeg\"\n      },\n      {\n        type: \"text\",\n        text: \"Describe the image in detail, and extract all the text in the image.\"\n      }\n    ]\n  }\n]);\n\nconsole.log(response.text);\n```\n\nFor a detailed guide to creating and configuring tools, see the [Tools Overview](../tools-mcp/overview.mdx) page.\n\n### Using `maxSteps`\n\nThe `maxSteps` parameter controls the maximum number of sequential LLM calls an agent can make. Each step includes generating a response, executing any tool calls, and processing the result. Limiting steps helps prevent infinite loops, reduce latency, and control token usage for agents that use tools. The default is 1, but can be increased:\n\n```typescript showLineNumbers copy\nconst response = await testAgent.generate(\"Help me organize my day\", {\n  maxSteps: 5\n});\n\nconsole.log(response.text);\n```\n\n### Using `onStepFinish`\n\nYou can monitor the progress of multi-step operations using the `onStepFinish` callback. This is useful for debugging or providing progress updates to users.\n\n`onStepFinish` is only available when streaming or generating text without structured output.\n\n```typescript showLineNumbers copy\nconst response = await testAgent.generate(\"Help me organize my day\", {\n  onStepFinish: ({ text, toolCalls, toolResults, finishReason, usage }) => {\n    console.log({ text, toolCalls, toolResults, finishReason, usage });\n  }\n});\n```\n\n## Using tools\n\nAgents can use tools to go beyond language generation, enabling structured interactions with external APIs and services. Tools allow agents to access data and perform clearly defined operations in a reliable, repeatable way.\n\n```typescript filename=\"src/mastra/agents/test-agent.ts\" showLineNumbers\nexport const testAgent = new Agent({\n  // ...\n  tools: { testTool }\n});\n```\n\n> See [Using Tools](./using-tools.mdx) for more information.\n\n## Using `RuntimeContext`\n\nUse `RuntimeContext` to access request-specific values. This lets you conditionally adjust behavior based on the context of the request.\n\n```typescript filename=\"src/mastra/agents/test-agent.ts\" showLineNumbers\nexport type UserTier = {\n  \"user-tier\": \"enterprise\" | \"pro\";\n};\n\nexport const testAgent = new Agent({\n  // ...\n  model: ({ runtimeContext }) => {\n    const userTier = runtimeContext.get(\"user-tier\") as UserTier[\"user-tier\"];\n\n    return userTier === \"enterprise\"\n      ? openai(\"gpt-4o-mini\")\n      : openai(\"gpt-4.1-nano\");\n  }\n});\n```\n\n> See [Runtime Context](../server-db/runtime-context.mdx) for more information.\n\n## Testing with Mastra Playground\n\nUse the Mastra [Playground](../server-db/local-dev-playground.mdx) to test agents with different messages, inspect tool calls and responses, and debug agent behavior.\n\n## Related\n\n- [Using Tools](./using-tools.mdx)\n- [Agent Memory](./agent-memory.mdx)\n- [Runtime Context](../../examples/agents/runtime-context.mdx)\n- [Calling Agents](../../examples/agents/calling-agents.mdx)\n\n\n","path":null,"size_bytes":14527,"size_tokens":null},"docs/mastra/01-agents/21_example-memory-postgresql.md":{"content":"---\ntitle: \"Example: Memory with PostgreSQL | Memory | Mastra Docs\"\ndescription: Example for how to use Mastra's memory system with PostgreSQL storage and vector capabilities.\n---\n\n# Memory with Postgres\n[EN] Source: https://mastra.ai/en/docs/memory/storage/memory-with-pg\n\nThis example demonstrates how to use Mastra's memory system with PostgreSQL as the storage backend.\n\n## Prerequisites\n\nThis example uses the `openai` model and requires a PostgreSQL database with the `pgvector` extension. Make sure to add the following to your `.env` file:\n\n```bash filename=\".env\" copy\nOPENAI_API_KEY=<your-api-key>\nDATABASE_URL=<your-connection-string>\n```\n\nAnd install the following package:\n\n```bash copy\nnpm install @mastra/pg\n```\n\n## Adding memory to an agent\n\nTo add PostgreSQL memory to an agent use the `Memory` class and create a new `storage` key using `PostgresStore`. The `connectionString` can either be a remote location, or a local database connection.\n\n```typescript filename=\"src/mastra/agents/example-pg-agent.ts\" showLineNumbers copy\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { PostgresStore } from \"@mastra/pg\";\n\nexport const pgAgent = new Agent({\n  name: \"pg-agent\",\n  instructions: \"You are an AI agent with the ability to automatically recall memories from previous interactions.\",\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    storage: new PostgresStore({\n      connectionString: process.env.DATABASE_URL!\n    }),\n    options: {\n      threads: {\n        generateTitle: true\n      }\n    }\n  })\n});\n```\n\n## Local embeddings with fastembed\n\nEmbeddings are numeric vectors used by memoryâ€™s `semanticRecall` to retrieve related messages by meaning (not keywords). This setup uses `@mastra/fastembed` to generate vector embeddings.\n\nInstall `fastembed` to get started:\n\n```bash copy\nnpm install @mastra/fastembed\n```\n\nAdd the following to your agent:\n\n```typescript filename=\"src/mastra/agents/example-pg-agent.ts\" showLineNumbers copy\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { PostgresStore, PgVector } from \"@mastra/pg\";\nimport { fastembed } from \"@mastra/fastembed\";\n\nexport const pgAgent = new Agent({\n  name: \"pg-agent\",\n  instructions: \"You are an AI agent with the ability to automatically recall memories from previous interactions.\",\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    storage: new PostgresStore({\n      connectionString: process.env.DATABASE_URL!\n    }),\n    vector: new PgVector({\n      connectionString: process.env.DATABASE_URL!\n    }),\n    embedder: fastembed,\n    options: {\n      lastMessages: 10,\n      semanticRecall: {\n        topK: 3,\n        messageRange: 2\n      }\n    }\n  })\n});\n```\n\n## Usage example\n\nUse `memoryOptions` to scope recall for this request. Set `lastMessages: 5` to limit recency-based recall, and use `semanticRecall` to fetch the `topK: 3` most relevant messages, including `messageRange: 2` neighboring messages for context around each match.\n\n```typescript filename=\"src/test-pg-agent.ts\" showLineNumbers copy\nimport \"dotenv/config\";\n\nimport { mastra } from \"./mastra\";\n\nconst threadId = \"123\";\nconst resourceId = \"user-456\";\n\nconst agent = mastra.getAgent(\"pgAgent\");\n\nconst message = await agent.stream(\"My name is Mastra\", {\n  memory: {\n    thread: threadId,\n    resource: resourceId\n  }\n});\n\nawait message.textStream.pipeTo(new WritableStream());\n\nconst stream = await agent.stream(\"What's my name?\", {\n  memory: {\n    thread: threadId,\n    resource: resourceId\n  },\n  memoryOptions: {\n    lastMessages: 5,\n    semanticRecall: {\n      topK: 3,\n      messageRange: 2\n    }\n  }\n});\n\nfor await (const chunk of stream.textStream) {\n  process.stdout.write(chunk);\n}\n```\n\n## Related\n\n- [Calling Agents](../agents/calling-agents.mdx)\n\n\n","path":null,"size_bytes":3898,"size_tokens":null},"docs/mastra/01-agents/04_agent-networks.md":{"content":"---\ntitle: \"Agent Networks | Agents | Mastra Docs\"\ndescription: Learn how to coordinate multiple agents, workflows, and tools using agent networks for complex, non-deterministic task execution.\n---\n\n# Agent Networks\n[EN] Source: https://mastra.ai/en/docs/agents/networks\n\nAgent networks in Mastra coordinate multiple agents, workflows, and tools to handle tasks that aren't clearly defined upfront but can be inferred from the user's message or context. A top-level **routing agent** (a Mastra agent with other agents, workflows, and tools configured) uses an LLM to interpret the request and decide which primitives (sub-agents, workflows, or tools) to call, in what order, and with what data.\n\n## When to use networks\n\nUse networks for complex tasks that require coordination across multiple primitives. Unlike workflows, which follow a predefined sequence, networks rely on LLM reasoning to interpret the request and decide what to run.\n\n## Core principles\n\nMastra agent networks operate using these principles:\n\n- Memory is required when using `.network()` and is used to store task history and determine when a task is complete.\n- Primitives are selected based on their descriptions. Clear, specific descriptions improve routing. For workflows and tools, the input schema helps determine the right inputs at runtime.\n- If multiple primitives have overlapping functionality, the agent favors the more specific one, using a combination of schema and descriptions to decide which to run.\n\n## Creating an agent network\n\nAn agent network is built around a top-level routing agent that delegates tasks to agents, workflows, and tools defined in its configuration. Memory is configured on the routing agent using the `memory` option, and `instructions` define the agent's routing behavior.\n\n```typescript {22-23,26,29} filename=\"src/mastra/agents/routing-agent.ts\" showLineNumbers copy\n  import { openai } from \"@ai-sdk/openai\";\n  import { Agent } from \"@mastra/core/agent\";\n  import { Memory } from \"@mastra/memory\";\n  import { LibSQLStore } from \"@mastra/libsql\";\n\n  import { researchAgent } from \"./research-agent\";\n  import { writingAgent } from \"./writing-agent\";\n\n  import { cityWorkflow } from \"../workflows/city-workflow\";\n  import { weatherTool } from \"../tools/weather-tool\";\n\n  export const routingAgent = new Agent({\n    name: \"routing-agent\",\n    instructions: `\n      You are a network of writers and researchers.\n      The user will ask you to research a topic.\n      Always respond with a complete reportâ€”no bullet points.\n      Write in full paragraphs, like a blog post.\n      Do not answer with incomplete or uncertain information.`,\n    model: openai(\"gpt-4o-mini\"),\n    agents: {\n      researchAgent,\n      writingAgent\n    },\n    workflows: {\n      cityWorkflow\n    },\n    tools: {\n      weatherTool\n    },\n    memory: new Memory({\n      storage: new LibSQLStore({\n        url: \"file:../mastra.db\"\n      })\n    })\n  });\n  ```\n\n### Writing descriptions for network primitives\n\nWhen configuring a Mastra agent network, each primitive (agent, workflow, or tool) needs a clear description to help the routing agent decide which to use. The routing agent uses each primitive's description and schema to determine what it does and how to use it. Clear descriptions and well-defined input and output schemas improve routing accuracy.\n\n#### Agent descriptions\n\nEach agent in a network should include a clear `description` that explains what the agent does.\n\n```typescript filename=\"src/mastra/agents/research-agent.ts\" showLineNumbers\nexport const researchAgent = new Agent({\n  name: \"research-agent\",\n  description: `This agent gathers concise research insights in bullet-point form.\n    It's designed to extract key facts without generating full\n    responses or narrative content.`,\n  // ...\n});\n```\n```typescript filename=\"src/mastra/agents/writing-agent.ts\" showLineNumbers\nexport const writingAgent = new Agent({\n  name: \"writing-agent\",\n  description: `This agent turns researched material into well-structured\n    written content. It produces full-paragraph reports with no bullet points,\n    suitable for use in articles, summaries, or blog posts.`,\n  // ...\n});\n```\n\n#### Workflow descriptions\n\nWorkflows in a network should include a `description` to explain their purpose, along with `inputSchema` and `outputSchema` to describe the expected data.\n\n```typescript filename=\"src/mastra/workflows/city-workflow.ts\" showLineNumbers\nexport const cityWorkflow = createWorkflow({\n  id: \"city-workflow\",\n  description: `This workflow handles city-specific research tasks.\n    It first gathers factual information about the city, then synthesizes\n    that research into a full written report. Use it when the user input\n    includes a city to be researched.`,\n  inputSchema: z.object({\n    city: z.string()\n  }),\n  outputSchema: z.object({\n    text: z.string()\n  })\n  //...\n})\n```\n\n#### Tool descriptions\n\nTools in a network should include a `description` to explain their purpose, along with `inputSchema` and `outputSchema` to describe the expected data.\n\n```typescript filename=\"src/mastra/tools/weather-tool.ts\" showLineNumbers\nexport const weatherTool = createTool({\n  id: \"weather-tool\",\n  description: ` Retrieves current weather information using the wttr.in API.\n    Accepts a city or location name as input and returns a short weather summary.\n    Use this tool whenever up-to-date weather data is requested.\n  `,\n  inputSchema: z.object({\n    location: z.string()\n  }),\n  outputSchema: z.object({\n    weather: z.string()\n  }),\n  // ...\n});\n```\n\n## Calling agent networks\n\nCall a Mastra agent network using `.network()` with a user message. The method returns a stream of events that you can iterate over to track execution progress and retrieve the final result.\n\n### Agent example\n\nIn this example, the network interprets the message and would route the request to both the `researchAgent` and `writingAgent` to generate a complete response.\n\n```typescript showLineNumbers copy\nconst result = await routingAgent.network(\"Tell me three cool ways to use Mastra\");\n\nfor await (const chunk of result) {\n  console.log(chunk.type);\n  if (chunk.type === \"network-execution-event-step-finish\") {\n    console.log(chunk.payload.result);\n  }\n}\n```\n\n#### Agent output\n\nThe following `chunk.type` events are emitted during this request:\n\n```text\nrouting-agent-start\nrouting-agent-end\nagent-execution-start\nagent-execution-event-start\nagent-execution-event-step-start\nagent-execution-event-text-start\nagent-execution-event-text-delta\nagent-execution-event-text-end\nagent-execution-event-step-finish\nagent-execution-event-finish\nagent-execution-end\nnetwork-execution-event-step-finish\n```\n\n## Workflow example\n\nIn this example, the routing agent recognizes the city name in the message and runs the `cityWorkflow`. The workflow defines steps that call the `researchAgent` to gather facts, then the `writingAgent` to generate the final text.\n\n```typescript showLineNumbers copy\nconst result = await routingAgent.network(\"Tell me some historical facts about London\");\n\nfor await (const chunk of result) {\n  console.log(chunk.type);\n  if (chunk.type === \"network-execution-event-step-finish\") {\n    console.log(chunk.payload.result);\n  }\n}\n```\n\n#### Workflow output\n\nThe following `chunk.type` events are emitted during this request:\n\n```text\nrouting-agent-end\nworkflow-execution-start\nworkflow-execution-event-workflow-start\nworkflow-execution-event-workflow-step-start\nworkflow-execution-event-workflow-step-result\nworkflow-execution-event-workflow-finish\nworkflow-execution-end\nrouting-agent-start\nnetwork-execution-event-step-finish\n```\n\n### Tool example\n\nIn this example, the routing agent skips the `researchAgent`, `writingAgent`, and `cityWorkflow`, and calls the `weatherTool` directly to complete the task.\n\n```typescript showLineNumbers copy\nconst result = await routingAgent.network(\"What's the weather in London?\");\n\nfor await (const chunk of result) {\n  console.log(chunk.type);\n  if (chunk.type === \"network-execution-event-step-finish\") {\n    console.log(chunk.payload.result);\n  }\n}\n```\n\n#### Tool output\n\nThe following `chunk.type` events are emitted during this request:\n\n```text\nrouting-agent-start\nrouting-agent-end\ntool-execution-start\ntool-execution-end\nnetwork-execution-event-step-finish\n```\n\n## Related\n\n- [Agent Memory](./agent-memory.mdx)\n- [Workflows Overview](../workflows/overview.mdx)\n- [Runtime Context](../server-db/runtime-context.mdx)\n\n\n\n","path":null,"size_bytes":8485,"size_tokens":null},"docs/triggers/dev-prod-replit.md":{"content":"# Inngest + Mastra Integration Guide\n\nThis document explains:\n\n1. How Mastra and Inngest integrate together\n2. The difference between dev/prod environments for webhook-triggered workflows\n3. How to test workflows locally\n4. The complete production webhook flow architecture\n\n**NOTE:** This document describes how dev/prod runs the full routing.  The agent may need to know how to query the Mastra server to run directly agents, workflows, etc.  You should disregard this documentation in that case and consult the Mastra doc (docs/mastra/**).\n\n---\n\n## 1. CORE CONCEPTS\n\n### Two Critical Endpoints\n\nYour Mastra application exposes **two distinct Inngest endpoints**:\n\n1. **`/api/inngest`** - The Inngest orchestration endpoint\n   - WHERE: Inngest Cloud (or dev server) calls this to execute workflow steps\n   - WHAT: Registered by `inngestServe` in `src/mastra/index.ts`\n   - WHEN: Called by Inngest to orchestrate each step of your workflow\n\n2. **`/{connector}/webhook`** - Webhook receiver endpoints (e.g., `/linear/webhook`)\n   - WHERE: Your forwarding function calls this to trigger workflows\n   - WHAT: Registered by `registerApiRoute` in your trigger files\n   - WHEN: Receives webhook payloads and starts workflow execution\n\n### How `registerApiRoute` Works\n\nWhen you call `registerApiRoute(\"/linear/webhook\", {...})`:\n\n1. **Creates a webhook handler** at `/linear/webhook` that validates and processes payloads\n2. **Creates an Inngest function** with ID `api-linear` that:\n   - Listens for `event/api.webhooks.linear.action` events\n   - Contains a step called \"forward request to Mastra\"\n   - Forwards the request to `http://localhost:5000/linear/webhook`\n\nThis dual registration ensures webhook reliability through Inngest's retry and durability features.\n\n**Special Case - Slack/Telegram:**\nSlack and Telegram use paths like `/webhooks/slack/action` and `/webhooks/telegram/action`. Because `registerApiRoute` extracts the connector name from the first path segment, both resolve to the connector name \"webhooks\" instead of \"slack\" or \"telegram\". This means:\n\n- Both listen for: `event/api.webhooks.webhooks.action` (shared event name)\n- Function IDs: `api-webhooks` (same for both)\n- Test scripts must send `event/api.webhooks.webhooks.action` for Slack/Telegram\n\n---\n\n## 2. DEVELOPMENT SETUP\n\nWhen you run your application locally, you need **two servers**:\n\n```bash\n# Terminal 1: Mastra server (via \"Start application\" workflow)\nnpm start  # â†’ localhost:5000\n\n# Terminal 2: Inngest dev server (via \"Start inngest server\" workflow)\ninngest dev -u http://localhost:5000/api/inngest --port 3000\n# â†’ localhost:3000 (orchestrator and a web UI)\n```\n\nNOTE: The Replit agent can only do this by running the Replit Workflows with the restart_workflow tool for both.\n\n### How Webhooks Flow in DEV\n\n**Via Webhook Testing:**\n\nExternal Webhook          Mastra Container         Dev Inngest              Mastra Container\n(e.g., Linear)           localhost:5000           localhost:3000           localhost:5000\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nPOST /linear/webhook â”€â”€â–º Handler receives\n                         Validates payload\n                         Calls workflow.start() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                                                                                â”‚\n                                                  Registers execution â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                  Orchestrator (RAM)\n                                                       â”‚\n                                                       â”‚ Step 1 request\n                                                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º /api/inngest\n                                                       â”‚                    Execute Step 1\n                                                       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Returns result\n                                                       â”‚ (Stores in RAM)\n                                                       â”‚\n                                                       â”‚ Step 2 request\n                                                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º /api/inngest\n                                                       â”‚                    Execute Step 2\n                                                       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Returns result\n                                                       â”‚\n                                                       â–¼\n                                                  View in UI: Replit Playground\n\n**Key Points:**\n\n- Inngest dev server = temporary coordinator (stores everything in RAM)\n- Nothing persists after restart\n- Mastra Workflows are triggered by calling `workflow.start()` directly or via webhooks\n- Both methods go through the same Inngest orchestration flow\n\n---\n\n## 3. PRODUCTION SETUP\n\n**Important:** You **never** deploy yourself. The Replit Publish wizard handles all deployment configuration automatically. This section explains what happens behind the scenes so you can understand the production architecture.\n\n**What Runs in Production:**\n\n```bash\n# Deployed via: NODE_ENV=production node .mastra/output/index.mjs\n# No inngest dev server needed!\n# Replit automatically registers your app with Inngest Cloud (api.inngest.com)\n```\n\n### Complete Production Webhook Flow\n\n**Step-by-Step Production Architecture:**\n\n1. External webhook arrives\n   Linear servers â†’ Replit Webhook Service (Replit infrastructure)\n2. Replit transforms to Inngest event\n   Replit Webhook Service converts webhook to:\n   event/api.webhooks.linear.action â†’ sends to api.inngest.com\n3. Inngest receives event\n   Inngest Cloud looks up functions triggered by this event\n   Finds the forwarding function (id: \"api-linear\")\n4. Inngest executes forwarding function\n   Inngest Cloud â†’ Your Container\n   HTTP POST to your-domain/api/inngest\n   Requests execution of step \"forward request to Mastra\"\n5. Container forwards to webhook handler\n   Your Container executes:\n   fetch(`http://localhost:5000/{provider}/webhook`) with original payload\n6. Webhook handler validates and starts workflow\n   /linear/webhook endpoint:\n   - Validates payload (checks action, type, data)\n   - Calls workflow, e.g.`linearCreateIssueWorkflow.createRunAsync().start({inputData})`\n7. Workflow start signals Inngest\n   Your Container â†’ Inngest Cloud\n   Tells Inngest to orchestrate this workflow execution\n8. Inngest orchestrates Step 1\n   Inngest Cloud â†’ Your Container\n   HTTP POST to https://{your-domain}/api/inngest requesting Step 1\n9. Container executes Step 1\n   Your Container runs step code, returns result\n10. Inngest memoizes and schedules Step 2\n    Inngest Cloud stores Step 1 result in Postgres\n    Determines Step 2 is next\n11. Inngest orchestrates Step 2\n    Inngest Cloud â†’ Your Container\n    HTTP POST to https://{your-domain}/api/inngest\n    Includes Step 1 results + Step 2 request\n12. Container executes Step 2\n    Step 2 runs with Step 1's results available\n13. Workflow completes\n    Inngest Cloud persists final state\n    Workflow marked complete in Postgres\n\n**Visual Flow Diagram:**\n\nExternal Webhook     Replit Service        Inngest Cloud         Your Container\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nPOST /webhook    â”€â”€â–º Transforms to\n                     inngest event     â”€â”€â–º api.inngest.com\n                                           Finds function\n                                           \"api-linear\"\n                                                â”‚\n                                                â”‚ Execute forwarding step\n                                                â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º https://{your-domain}\n                                                â”‚                    /api/inngest\n                                                â”‚                    (forward to /linear/webhook)\n                                                â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n                                                â”‚\n                     â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n                     /linear/webhook receives   â”‚\n                     Validates payload          â”‚\n                     Calls workflow.start() â”€â”€â”€â”€â”¤\n                                                â”‚\n                                                â”‚ Register execution\n                                                â”‚ Orchestrate steps:\n                                                â”‚\n                                                â”‚ Step 1\n                                                â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º /api/inngest\n                                                â”‚                    Execute Step 1\n                                                â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Result\n                                                â”‚ (Save to Postgres)\n                                                â”‚\n                                                â”‚ Step 2\n                                                â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º /api/inngest\n                                                â”‚                    Execute Step 2\n                                                â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Result\n                                                â”‚\n                                                â–¼\n                                           Complete\n                                           (View at app.inngest.com)\n\n**Key Insight:** The `/api/inngest` endpoint serves two purposes:\n\n1. Executes the forwarding function (which calls `/linear/webhook`, which then calls `workflow.start()` to signal Inngest to orchestrate workflow steps).\n2. Receives workflow step execution requests (to run your actual workflow steps).\n\n---\n\n## 4. DEV vs PROD DIFFERENCES\n\n| Aspect | Development | Production |\n|--------|-------------|------------|\n| **Inngest runs on** | localhost:3000 (dev server) | api.inngest.com (cloud) |\n| **Webhook transformation** | Direct (no Replit layer) | Via Replit Webhook Service |\n| **Event pattern** | Direct calls to `/linear/webhook` | `event/api.webhooks.linear.action` |\n| **Storage** | RAM (lost on restart) | Postgres (durable) |\n| **Inngest dev server needed?** | âœ… YES (run manually) | âŒ NO (handled by Publish) |\n| **Web UI** | localhost:3000 | app.inngest.com |\n| **Authentication** | None | Auto-configured by Replit |\n| **Trigger methods** | `workflow.start()` or webhooks | Same - both methods work |\n| **Retry behavior** | 0 retries (fast debugging) | 3 retries (production reliability) |\n\nYour code = IDENTICAL in both environments.\n\nThe Mastra + Inngest SDK automatically detects the environment:\n\n- **Dev:** Sees dev server at localhost:3000 â†’ uses it for orchestration\n- **Prod:** No dev server â†’ uses api.inngest.com for orchestration\n\n---\n\n## 5. Testing the whole system (make it prod ready)\n\n### The ONLY Valid Test Method - Production Flow Simulation\n\n**CRITICAL:** You must test the complete production event flow to validate your integration using a test script. This is the exact flow that happens in production (using dev Inngest server instead of Inngest Cloud).\n\nExample:\n\n```typescript\nimport { inngest } from \"./src/mastra/inngest/client\";\n\n/**\n * This sends the EXACT event that Replit Webhook Service sends to Inngest Cloud.\n * It exercises the complete production flow:\n * 1. Inngest receives event/api.webhooks.{provider}.action\n * 2. Triggers the forwarding function (created by registerApiRoute)\n * 3. Forwarding function POSTs to /{provider}/webhook\n * 4. Webhook handler validates and starts workflow\n * 5. Inngest orchestrates step-by-step execution\n */\n// Configuration\nconst provider = \"linear\";  // The webhook provider (e.g., \"linear\", \"github\", \"stripe\")\n\n// Mock webhook payload - replace with your connector/name's schema. Keep it obviously mocked.\nconst mockWebhookPayload = {\n  action: \"create\",\n  type: \"Issue\",\n  data: {\n    id: \"mock-issue-999\",\n    title: \"MOCK: Test Issue Title\",\n    description: \"MOCK: This is fake test data for validation\",\n    number: 999,\n    priority: 1,\n    createdAt: \"2025-01-01T00:00:00Z\",\n    updatedAt: \"2025-01-01T00:00:00Z\"\n  },\n  createdAt: \"2025-01-01T00:00:00Z\",\n  organizationId: \"mock-org-123\"\n};\n\n/**\n * This sends the EXACT event that Replit Webhook Service sends to Inngest Cloud.\n * It exercises the complete production flow.\n * \n * SPECIAL CASE: Slack and Telegram use /webhooks/{provider}/action paths,\n * so they both listen for \"event/api.webhooks.webhooks.action\" instead.\n */\n// Determine the correct event name\nlet eventName: string;\nif (provider === \"slack\" || provider === \"telegram\") {\n  eventName = \"event/api.webhooks.webhooks.action\";\n} else {\n  eventName = `event/api.webhooks.${provider}.action`;\n}\n\nawait inngest.send({\n  name: eventName,  // Must match what registerApiRoute creates\n  data: {\n    method: \"POST\",\n    headers: { \"content-type\": \"application/json\" },\n    body: JSON.stringify(mockWebhookPayload)\n  }\n});\n\nconsole.log(`âœ… Event sent for ${provider}! Check execution at: http://localhost:3000`);\n```\n\nRun with:\n\n```bash\nnpx tsx <test-file-name>\n```\n\n### What This Tests (Complete Production Flow)\n\nThis is the ONLY test method that validates:\n\n- âœ… Inngest event routing (`event/api.webhooks.linear.action`)\n- âœ… The forwarding function created by `registerApiRoute`\n- âœ… HTTP forwarding to `/linear/webhook`\n- âœ… Webhook payload parsing and validation\n- âœ… Handler validation logic (action === \"create\", type === \"Issue\")\n- âœ… Workflow triggering via `workflow.start()`\n- âœ… Complete Inngest step-by-step orchestration\n- âœ… All error handling and retry logic\n\n### Verification Steps\n\nAfter running the test, check the console. We must add a lot of logging statements, so we should have a robust logging system.\n\n### Why Other Methods Don't Work\n\n**âŒ DON'T USE: Direct POST to `/linear/webhook`**\n\n```bash\n# This SKIPS the Inngest forwarding function\ncurl -X POST http://localhost:5000/linear/webhook ...\n```\n\nThis bypasses the `registerApiRoute` Inngest function, so you're not testing the production flow.\n\nâŒ DON'T USE: Direct workflow execution\n\n```typescript\n// This SKIPS webhook validation and event routing\nconst run = await linearIssueWorkflow.createRunAsync();\nawait run.start({ inputData: {...} });\n```\n\nThis skips the webhook handler validation logic and the Inngest event system.\n\nâœ… ONLY USE: The Inngest event send pattern shown above\n\nThis is the ONLY method that tests the complete production architecture from end-to-end.\n\n---\n\n## 6. TROUBLESHOOTING\n\n### Common Issues\n\n**Workflow doesn't start:**\n\n- âœ… Check both servers are running (`npm start` + `inngest dev`)\n- âœ… Verify workflow is registered in `src/mastra/index.ts`\n- âœ… Check inputData matches the workflow's inputSchema\n- âœ… Look for validation errors in console logs\n\n**Webhook returns 500 error:**\n\n- âœ… Verify payload structure matches expected format\n- âœ… Check webhook handler validation logic (action/type checks)\n- âœ… Review error logs in the \"Start application\" workflow\n\n**Steps fail with schema errors:**\n\n- âœ… Ensure outputSchema of step N matches inputSchema of step N+1\n- âœ… Check that all required fields are returned from each step\n- âœ… Review Inngest UI at localhost:3000 for detailed error messages\n\n**Changes not reflected:**\n\n- âœ… Restart both \"Start application\" and \"Start inngest server\" workflows\n- âœ… Clear Inngest dev server cache: Stop and restart \"Start inngest server\"\n\n### Debug Mode\n\nEnable detailed logging:\n\n```typescript\n// In src/mastra/index.ts, change logger level:\nlogger: new PinoLogger({\n  name: \"Mastra\",\n  level: \"debug\", // Changed from \"info\"\n}),\n```\n\nThis logs all requests, step executions, and data transformations.\n\n## Summary\n\n**Development:**\n\n- Two servers: Mastra (5000) + Inngest Dev (3000)\n- Test with direct `workflow.start()` or webhook simulation\n- Fast iteration, no persistence\n\n**Production:**\n\n- Single container running Mastra\n- Webhooks flow: External â†’ Replit Service â†’ Inngest Cloud â†’ Your Container\n- Durable execution with Postgres storage\n- Automatic retries and monitoring\n\n**Key Architecture:**\n\n- `/api/inngest` - Inngest's orchestration endpoint (handles step execution + forwarding)\n- `/{connector}/webhook` - Your webhook receivers (validate & trigger workflows)\n- `registerApiRoute` creates both the handler AND the Inngest forwarding function\n\nYour Mastra workflows work identically in both environments, with Inngest handling all the complexity of orchestration, retries, and state management.\n","path":null,"size_bytes":17407,"size_tokens":null},"docs/mastra/06-reference/111_run-cancel.md":{"content":"---\ntitle: \"Reference: Run.cancel() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Run.cancel()` method in workflows, which cancels a workflow run.\n---\n\n# Run.cancel()\n[EN] Source: https://mastra.ai/en/reference/workflows/run-methods/cancel\n\nThe `.cancel()` method cancels a workflow run, stopping execution and cleaning up resources.\n\n## Usage example\n\n```typescript showLineNumbers copy\nconst run = await workflow.createRunAsync();\n\nawait run.cancel();\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"No parameters\",\n      type: \"void\",\n      description: \"This method takes no parameters\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"result\",\n      type: \"Promise<void>\",\n      description: \"A promise that resolves when the workflow run has been cancelled\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript showLineNumbers copy\nconst run = await workflow.createRunAsync();\n\ntry {\n  const result = await run.start({ inputData: { value: \"initial data\" } });\n} catch (error) {\n  await run.cancel();\n}\n```\n\n## Related\n\n- [Workflows overview](../../../docs/workflows/overview.mdx#run-workflow)\n- [Workflow.createRunAsync()](../create-run.mdx)\n\n\n","path":null,"size_bytes":1247,"size_tokens":null},"docs/mastra/06-reference/21_cli-commands.md":{"content":"---\ntitle: \"Reference: CLI Commands\"\ndescription: Documentation for the Mastra CLI to develop, build, and start your project.\n---\n\nimport { Callout } from \"nextra/components\";\n\n# CLI Commands\n[EN] Source: https://mastra.ai/en/reference/cli/mastra\n\nYou can use the Command-Line Interface (CLI) provided by Mastra to develop, build, and start your Mastra project.\n\n## `mastra dev`\n\nStarts a server which exposes a [local dev playground](/docs/server-db/local-dev-playground) and REST endpoints for your agents, tools, and workflows. You can visit [http://localhost:4111/swagger-ui](http://localhost:4111/swagger-ui) for an overview of all available endpoints once `mastra dev` is running.\n\nYou can also [configure the server](/docs/server-db/local-dev-playground#configuration).\n\n### Flags\n\nThe command accepts [common flags][common-flags] and the following additional flags:\n\n#### `--https`\n\nEnable local HTTPS support. [Learn more](/docs/server-db/local-dev-playground#local-https).\n\n#### `--inspect`\n\nStart the development server in inspect mode, helpful for debugging. This can't be used together with `--inspect-brk`.\n\n#### `--inspect-brk`\n\nStart the development server in inspect mode and break at the beginning of the script. This can't be used together with `--inspect`.\n\n#### `--custom-args`\n\nComma-separated list of custom arguments to pass to the development server. You can pass arguments to the Node.js process, e.g. `--experimental-transform-types`.\n\n### Configs\n\nYou can set certain environment variables to modify the behavior of `mastra dev`.\n\n#### Disable build caching\n\nSet `MASTRA_DEV_NO_CACHE=1` to force a full rebuild rather than using the cached assets under `.mastra/`:\n\n```bash copy\nMASTRA_DEV_NO_CACHE=1 mastra dev\n```\n\nThis helps when you are debugging bundler plugins or suspect stale output.\n\n#### Limit parallelism\n\n`MASTRA_CONCURRENCY` caps how many expensive operations run in parallel (primarily build and evaluation steps). For example:\n\n```bash copy\nMASTRA_CONCURRENCY=4 mastra dev\n```\n\nLeave it unset to let the CLI pick a sensible default for the machine.\n\n#### Custom provider endpoints\n\nWhen using providers supported by the Vercel AI SDK you can redirect requests through proxies or internal gateways by setting a base URL. For OpenAI:\n\n```bash copy\nOPENAI_API_KEY=<your-api-key> \\\nOPENAI_BASE_URL=https://openrouter.example/v1 \\\nmastra dev\n```\n\nFor Anthropic:\n\n```bash copy\nANTHROPIC_API_KEY=<your-api-key> \\\nANTHROPIC_BASE_URL=https://anthropic.internal \\\nmastra dev\n```\n\nThese are forwarded by the AI SDK and work with any `openai()` or `anthropic()` calls.\n\n## `mastra build`\n\nThe `mastra build` command bundles your Mastra project into a production-ready Hono server. [Hono](https://hono.dev/) is a lightweight, type-safe web framework that makes it easy to deploy Mastra agents as HTTP endpoints with middleware support.\n\nUnder the hood Mastra's Rollup server locates your Mastra entry file and bundles it to a production-ready Hono server. During that bundling it tree-shakes your code and generates source maps for debugging.\n\nThe output in `.mastra` can be deployed to any cloud server using [`mastra start`](#mastra-start).\n\nIf you're deploying to a [serverless platform](/docs/deployment/serverless-platforms) you need to install the correct deployer in order to receive the correct output in `.mastra`.\n\nIt accepts [common flags][common-flags].\n\n### Configs\n\nYou can set certain environment variables to modify the behavior of `mastra build`.\n\n#### Limit parallelism\n\nFor CI or when running in resource constrained environments you can cap how many expensive tasks run at once by setting `MASTRA_CONCURRENCY`.\n\n```bash copy\nMASTRA_CONCURRENCY=2 mastra build\n```\n\n## `mastra start`\n\n<Callout type=\"info\">\nYou need to run `mastra build` before using `mastra start`.\n</Callout>\n\nStarts a local server to serve your built Mastra application in production mode. By default, [OTEL Tracing](/docs/observability/otel-tracing) is enabled.\n\n### Flags\n\nThe command accepts [common flags][common-flags] and the following additional flags:\n\n#### `--dir`\n\nThe path to your built Mastra output directory. Defaults to `.mastra/output`.\n\n#### `--no-telemetry`\n\nDisable the [OTEL Tracing](/docs/observability/otel-tracing).\n\n## `mastra lint`\n\nThe `mastra lint` command validates the structure and code of your Mastra project to ensure it follows best practices and is error-free.\n\nIt accepts [common flags][common-flags].\n\n## `mastra scorers`\n\nThe `mastra scorers` command provides management capabilities for evaluation scorers that measure the quality, accuracy, and performance of AI-generated outputs.\n\nRead the [Scorers overview](/docs/scorers/overview) to learn more.\n\n### `add`\n\nAdd a new scorer to your project. You can use an interactive prompt:\n\n```bash copy\nmastra scorers add\n```\n\nOr provide a scorer name directly:\n\n```bash copy\nmastra scorers add answer-relevancy\n```\n\nUse the [`list`](#list) command to get the correct ID.\n\n### `list`\n\nList all available scorer templates. Use the ID for the `add` command.\n\n## `mastra init`\n\nThe `mastra init` command initializes Mastra in an existing project. Use this command to scaffold the necessary folders and configuration without generating a new project from scratch.\n\n### Flags\n\nThe command accepts the following additional flags:\n\n#### `--default`\n\nCreates files inside `src` using OpenAI. It also populates the `src/mastra` folders with example code.\n\n#### `--dir`\n\nThe directory where Mastra files should be saved to. Defaults to `src`.\n\n#### `--components`\n\nComma-separated list of components to add. For each component a new folder will be created. Choose from: `\"agents\" | \"tools\" | \"workflows\" | \"scorers\"`. Defaults to `['agents', 'tools', 'workflows']`.\n\n#### `--llm`\n\nDefault model provider. Choose from: `\"openai\" | \"anthropic\" | \"groq\" | \"google\" | \"cerebras\" | \"mistral\"`.\n\n#### `--llm-api-key`\n\nThe API key for your chosen model provider. Will be written to an environment variables file (`.env`).\n\n#### `--example`\n\nIf enabled, example code is written to the list of components (e.g. example agent code).\n\n#### `--no-example`\n\nDo not include example code. Useful when using the `--default` flag.\n\n#### `--mcp`\n\nConfigure your code editor with Mastra's MCP server. Choose from: `\"cursor\" | \"cursor-global\" | \"windsurf\" | \"vscode\"`.\n\n## Common flags\n\n### `--dir`\n\n**Available in:** `dev`, `build`, `lint`\n\nThe path to your Mastra folder. Defaults to `src/mastra`.\n\n### `--debug`\n\n**Available in:** `dev`, `build`\n\nEnable verbose logging for Mastra's internals. Defaults to `false`.\n\n### `--env`\n\n**Available in:** `dev`, `start`\n\nCustom environment variables file to include. By default, includes `.env.development`, `.env.local`, and `.env`.\n\n### `--root`\n\n**Available in:** `dev`, `build`, `lint`\n\nPath to your root folder. Defaults to `process.cwd()`.\n\n### `--tools`\n\n**Available in:** `dev`, `build`, `lint`\n\nComma-separated list of tool paths to include. Defaults to `src/mastra/tools`.\n\n## Global flags\n\nUse these flags to get information about the `mastra` CLI.\n\n### `--version`\n\nPrints the Mastra CLI version and exits.\n\n### `--help`\n\nPrints help message and exits.\n\n## Telemetry\n\nBy default, Mastra collects anonymous information about your project like your OS, Mastra version or Node.js version. You can read the [source code](https://github.com/mastra-ai/mastra/blob/main/packages/cli/src/analytics/index.ts) to check what's collected.\n\nYou can opt out of the CLI analytics by setting an environment variable:\n\n```bash copy\nMASTRA_TELEMETRY_DISABLED=1\n```\n\nYou can also set this while using other `mastra` commands:\n\n```bash copy\nMASTRA_TELEMETRY_DISABLED=1 mastra dev\n```\n\n[common-flags]: #common-flags\n\n---\ntitle: Mastra Client Agents API\ndescription: Learn how to interact with Mastra AI agents, including generating responses, streaming interactions, and managing agent tools using the client-js SDK.\n---\n\n# Agents API\n[EN] Source: https://mastra.ai/en/reference/client-js/agents\n\nThe Agents API provides methods to interact with Mastra AI agents, including generating responses, streaming interactions, and managing agent tools.\n\n## Getting All Agents\n\nRetrieve a list of all available agents:\n\n```typescript\nconst agents = await mastraClient.getAgents();\n```\n\n## Working with a Specific Agent\n\nGet an instance of a specific agent:\n\n```typescript\nconst agent = mastraClient.getAgent(\"agent-id\");\n```\n\n## Agent Methods\n\n### Get Agent Details\n\nRetrieve detailed information about an agent:\n\n```typescript\nconst details = await agent.details();\n```\n\n### Generate Response\n\nGenerate a response from the agent:\n\n```typescript\nconst response = await agent.generate({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Hello, how are you?\",\n    },\n  ],\n  threadId: \"thread-1\", // Optional: Thread ID for conversation context\n  resourceId: \"resource-1\", // Optional: Resource ID\n  output: {}, // Optional: Output configuration\n});\n```\n\n### Stream Response\n\nStream responses from the agent for real-time interactions:\n\n```typescript\nconst response = await agent.stream({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Tell me a story\",\n    },\n  ],\n});\n\n// Process data stream with the processDataStream util\nresponse.processDataStream({\n  onChunk: async(chunk) => {\n    console.log(chunk);\n  },\n});\n\n\n// You can also read from response body directly\nconst reader = response.body.getReader();\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  console.log(new TextDecoder().decode(value));\n}\n```\n\n### Client tools\n\nClient-side tools allow you to execute custom functions on the client side when the agent requests them.\n\n#### Basic Usage\n\n```typescript\nimport { createTool } from '@mastra/client-js';\nimport { z } from 'zod';\n\nconst colorChangeTool = createTool({\n  id: 'changeColor',\n  description: 'Changes the background color',\n  inputSchema: z.object({\n    color: z.string(),\n  }),\n  execute: async ({ context }) => {\n    document.body.style.backgroundColor = context.color;\n    return { success: true };\n  }\n})\n\n\n// Use with generate\nconst response = await agent.generate({\n  messages: 'Change the background to blue',\n  clientTools: {colorChangeTool},\n});\n\n// Use with stream\nconst response = await agent.stream({\n  messages: 'Change the background to green',\n  clientTools: {colorChangeTool},\n});\n\nresponse.processDataStream({\n  onChunk: async (chunk) => {\n    if (chunk.type === 'text-delta') {\n      console.log(chunk.payload.text);\n    } else if (chunk.type === 'tool-call') {\n      console.log(`calling tool ${chunk.payload.toolName} with args ${JSON.stringify(chunk.payload.args, null, 2)}`);\n    }\n  },\n});\n```\n\n### Get Agent Tool\n\nRetrieve information about a specific tool available to the agent:\n\n```typescript\nconst tool = await agent.getTool(\"tool-id\");\n```\n\n### Get Agent Evaluations\n\nGet evaluation results for the agent:\n\n```typescript\n// Get CI evaluations\nconst evals = await agent.evals();\n\n// Get live evaluations\nconst liveEvals = await agent.liveEvals();\n```\n\n\n### Stream\n\n\nStream responses using the enhanced API with improved method signatures. This method provides enhanced capabilities and format flexibility, with support for Mastra's native format.\n\n```typescript\nconst response = await agent.stream(\n  \"Tell me a story\",\n  {\n    threadId: \"thread-1\",\n    clientTools: { colorChangeTool },\n  }\n);\n\n// Process the stream\nresponse.processDataStream({\n  onChunk: async (chunk) => {\n    if (chunk.type === 'text-delta') {\n      console.log(chunk.payload.text);\n    }\n  },\n});\n```\n\n#### AI SDK compatible format\n\nTo stream AI SDK-formatted parts on the client from an `agent.stream(...)` response, wrap `response.processDataStream` into a `ReadableStream<ChunkType>` and use `toAISdkFormat`:\n\n```typescript filename=\"client-ai-sdk-transform.ts\" copy\nimport { createUIMessageStream } from 'ai';\nimport { toAISdkFormat } from '@mastra/ai-sdk';\nimport type { ChunkType, MastraModelOutput } from '@mastra/core/stream';\n\nconst response = await agent.stream({ messages: 'Tell me a story' });\n\nconst chunkStream: ReadableStream<ChunkType> = new ReadableStream<ChunkType>({\n  start(controller) {\n    response.processDataStream({\n      onChunk: async (chunk) => controller.enqueue(chunk as ChunkType),\n    }).finally(() => controller.close());\n  },\n});\n\nconst uiMessageStream = createUIMessageStream({\n  execute: async ({ writer }) => {\n    for await (const part of toAISdkFormat(chunkStream as unknown as MastraModelOutput, { from: 'agent' })) {\n      writer.write(part);\n    }\n  },\n});\n\nfor await (const part of uiMessageStream) {\n  console.log(part);\n}\n```\n\n### Generate\n\nGenerate a response using the enhanced API with improved method signatures and AI SDK v5 compatibility:\n\n```typescript\nconst response = await agent.generate(\n  \"Hello, how are you?\",\n  {\n    threadId: \"thread-1\",\n    resourceId: \"resource-1\",\n  }\n);\n```\n\n\n---\ntitle: Mastra Client Error Handling\ndescription: Learn about the built-in retry mechanism and error handling capabilities in the Mastra client-js SDK.\n---\n\n# Error Handling\n[EN] Source: https://mastra.ai/en/reference/client-js/error-handling\n\nThe Mastra Client SDK includes built-in retry mechanism and error handling capabilities.\n\n## Error Handling\n\nAll API methods can throw errors that you can catch and handle:\n\n```typescript\ntry {\n  const agent = mastraClient.getAgent(\"agent-id\");\n  const response = await agent.generate({\n    messages: [{ role: \"user\", content: \"Hello\" }],\n  });\n} catch (error) {\n  console.error(\"An error occurred:\", error.message);\n}\n```\n\n\n---\ntitle: Mastra Client Logs API\ndescription: Learn how to access and query system logs and debugging information in Mastra using the client-js SDK.\n---\n\n# Logs API\n[EN] Source: https://mastra.ai/en/reference/client-js/logs\n\nThe Logs API provides methods to access and query system logs and debugging information in Mastra.\n\n## Getting Logs\n\nRetrieve system logs with optional filtering:\n\n```typescript\nconst logs = await mastraClient.getLogs({\n  transportId: \"transport-1\",\n});\n```\n\n## Getting Logs for a Specific Run\n\nRetrieve logs for a specific execution run:\n\n```typescript\nconst runLogs = await mastraClient.getLogForRun({\n  runId: \"run-1\",\n  transportId: \"transport-1\",\n});\n```\n\n\n---\ntitle: MastraClient\ndescription: Learn how to interact with Mastra using the client-js SDK.\n---\n\n# Mastra Client SDK\n[EN] Source: https://mastra.ai/en/reference/client-js/mastra-client\n\nThe Mastra Client SDK provides a simple and type-safe interface for interacting with your [Mastra Server](/docs/deployment/server-deployment.mdx) from your client environment.\n\n## Usage example\n\n```typescript filename=\"lib/mastra/mastra-client.ts\" showLineNumbers copy\nimport { MastraClient } from \"@mastra/client-js\";\n\nexport const mastraClient = new MastraClient({\n  baseUrl: \"http://localhost:4111/\",\n});\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"baseUrl\",\n      type: \"string\",\n      description: \"The base URL for the Mastra API. All requests will be sent relative to this URL.\",\n      isOptional: false,\n    },\n    {\n      name: \"retries\",\n      type: \"number\",\n      description: \"The number of times a request will be retried on failure before throwing an error.\",\n      isOptional: true,\n      defaultValue: \"3\",\n    },\n    {\n      name: \"backoffMs\",\n      type: \"number\",\n      description: \"The initial delay in milliseconds before retrying a failed request. This value is doubled with each retry (exponential backoff).\",\n      isOptional: true,\n      defaultValue: \"300\",\n    },\n    {\n      name: \"maxBackoffMs\",\n      type: \"number\",\n      description: \"The maximum backoff time in milliseconds. Prevents retries from waiting too long between attempts.\",\n      isOptional: true,\n      defaultValue: \"5000\",\n    },\n    {\n      name: \"headers\",\n      type: \"Record<string, string>\",\n      description: \"An object containing custom HTTP headers to include with every request.\",\n      isOptional: true,\n    },\n    {\n      name: \"credentials\",\n      type: '\"omit\" | \"same-origin\" | \"include\"',\n      description: \"Credentials mode for requests. See https://developer.mozilla.org/en-US/docs/Web/API/Request/credentials for more info.\",\n      isOptional: true,\n    },\n  ]}\n/>\n\n## Methods\n\n<PropertiesTable\n  content={[\n    {\n      name: \"getAgents()\",\n      type: \"Promise<Record<string, GetAgentResponse>>\",\n      description: \"Returns all available agent instances.\",\n    },\n    {\n      name: \"getAgent(agentId)\",\n      type: \"Agent\",\n      description: \"Retrieves a specific agent instance by ID.\",\n    },\n    {\n      name: \"getMemoryThreads(params)\",\n      type: \"Promise<StorageThreadType[]>\",\n      description: \"Retrieves memory threads for the specified resource and agent. Requires a `resourceId` and an `agentId`.\",\n    },\n    {\n      name: \"createMemoryThread(params)\",\n      type: \"Promise<MemoryThread>\",\n      description: \"Creates a new memory thread with the given parameters.\",\n    },\n    {\n      name: \"getMemoryThread(threadId)\",\n      type: \"Promise<MemoryThread>\",\n      description: \"Fetches a specific memory thread by ID.\",\n    },\n    {\n      name: \"saveMessageToMemory(params)\",\n      type: \"Promise<void>\",\n      description: \"Saves one or more messages to the memory system.\",\n    },\n    {\n      name: \"getMemoryStatus()\",\n      type: \"Promise<MemoryStatus>\",\n      description: \"Returns the current status of the memory system.\",\n    },\n    {\n      name: \"getTools()\",\n      type: \"Record<string, Tool>\",\n      description: \"Returns all available tools.\",\n    },\n    {\n      name: \"getTool(toolId)\",\n      type: \"Tool\",\n      description: \"Retrieves a specific tool instance by ID.\",\n    },\n    {\n      name: \"getWorkflows()\",\n      type: \"Record<string, Workflow>\",\n      description: \"Returns all available workflow instances.\",\n    },\n    {\n      name: \"getWorkflow(workflowId)\",\n      type: \"Workflow\",\n      description: \"Retrieves a specific workflow instance by ID.\",\n    },\n    {\n      name: \"getVector(vectorName)\",\n      type: \"MastraVector\",\n      description: \"Returns a vector store instance by name.\",\n    },\n    {\n      name: \"getLogs(params)\",\n      type: \"Promise<LogEntry[]>\",\n      description: \"Fetches system logs matching the provided filters.\",\n    },\n    {\n      name: \"getLog(params)\",\n      type: \"Promise<LogEntry>\",\n      description: \"Retrieves a specific log entry by ID or filter.\",\n    },\n    {\n      name: \"getLogTransports()\",\n      type: \"string[]\",\n      description: \"Returns the list of configured log transport types.\",\n    },\n    {\n      name: \"getAITrace(traceId)\",\n      type: \"Promise<AITraceRecord>\",\n      description: \"Retrieves a specific AI trace by ID, including all its spans and details.\",\n    },\n    {\n      name: \"getAITraces(params)\",\n      type: \"Promise<GetAITracesResponse>\",\n      description: \"Retrieves paginated list of AI trace root spans with optional filtering. Use getAITrace() to get complete traces with all spans.\",\n    },\n  ]}\n/>\n\n\n\n---\ntitle: Mastra Client Memory API\ndescription: Learn how to manage conversation threads and message history in Mastra using the client-js SDK.\n---\n\n# Memory API\n[EN] Source: https://mastra.ai/en/reference/client-js/memory\n\nThe Memory API provides methods to manage conversation threads and message history in Mastra.\n\n### Get All Threads\n\nRetrieve all memory threads for a specific resource:\n\n```typescript\nconst threads = await mastraClient.getMemoryThreads({\n  resourceId: \"resource-1\",\n  agentId: \"agent-1\",\n});\n```\n\n### Create a New Thread\n\nCreate a new memory thread:\n\n```typescript\nconst thread = await mastraClient.createMemoryThread({\n  title: \"New Conversation\",\n  metadata: { category: \"support\" },\n  resourceId: \"resource-1\",\n  agentId: \"agent-1\",\n});\n```\n\n### Working with a Specific Thread\n\nGet an instance of a specific memory thread:\n\n```typescript\nconst thread = mastraClient.getMemoryThread(\"thread-id\", \"agent-id\");\n```\n\n## Thread Methods\n\n### Get Thread Details\n\nRetrieve details about a specific thread:\n\n```typescript\nconst details = await thread.get();\n```\n\n### Update Thread\n\nUpdate thread properties:\n\n```typescript\nconst updated = await thread.update({\n  title: \"Updated Title\",\n  metadata: { status: \"resolved\" },\n  resourceId: \"resource-1\",\n});\n```\n\n### Delete Thread\n\nDelete a thread and its messages:\n\n```typescript\nawait thread.delete();\n```\n\n## Message Operations\n\n### Save Messages\n\nSave messages to memory:\n\n```typescript\nconst savedMessages = await mastraClient.saveMessageToMemory({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Hello!\",\n      id: \"1\",\n      threadId: \"thread-1\",\n      createdAt: new Date(),\n      type: \"text\",\n    },\n  ],\n  agentId: \"agent-1\",\n});\n```\n\n### Retrieve Thread Messages\n\nGet messages associated with a memory thread:\n\n```typescript\n// Get all messages in the thread\nconst { messages } = await thread.getMessages();\n\n// Limit the number of messages retrieved\nconst { messages } = await thread.getMessages({ limit: 10 });\n```\n\n### Delete a Message\n\nDelete a specific message from a thread:\n\n```typescript\nconst result = await thread.deleteMessage(\"message-id\");\n// Returns: { success: true, message: \"Message deleted successfully\" }\n```\n\n### Delete Multiple Messages\n\nDelete multiple messages from a thread in a single operation:\n\n```typescript\nconst result = await thread.deleteMessages([\"message-1\", \"message-2\", \"message-3\"]);\n// Returns: { success: true, message: \"3 messages deleted successfully\" }\n```\n\n### Get Memory Status\n\nCheck the status of the memory system:\n\n```typescript\nconst status = await mastraClient.getMemoryStatus(\"agent-id\");\n```\n\n\n---\ntitle: Mastra Client Observability API\ndescription: Learn how to retrieve AI traces, monitor application performance, and score traces using the client-js SDK.\n---\n\n# Observability API\n[EN] Source: https://mastra.ai/en/reference/client-js/observability\n\nThe Observability API provides methods to retrieve AI traces, monitor your application's performance, and score traces for evaluation. This helps you understand how your AI agents and workflows are performing.\n\n## Getting a Specific AI Trace\n\nRetrieve a specific AI trace by its ID, including all its spans and details:\n\n```typescript\nconst trace = await mastraClient.getAITrace(\"trace-id-123\");\n```\n\n## Getting AI Traces with Filtering\n\nRetrieve a paginated list of AI trace root spans with optional filtering:\n\n```typescript\nconst traces = await mastraClient.getAITraces({\n  pagination: {\n    page: 1,\n    perPage: 20,\n    dateRange: {\n      start: new Date('2024-01-01'),\n      end: new Date('2024-01-31')\n    }\n  },\n  filters: {\n    name: \"weather-agent\", // Filter by trace name\n    spanType: \"agent\", // Filter by span type\n    entityId: \"weather-agent-id\", // Filter by entity ID\n    entityType: \"agent\" // Filter by entity type\n  }\n});\n\nconsole.log(`Found ${traces.spans.length} root spans`);\nconsole.log(`Total pages: ${traces.pagination.totalPages}`);\n\n// To get the complete trace with all spans, use getAITrace\nconst completeTrace = await mastraClient.getAITrace(traces.spans[0].traceId);\n```\n\n## Scoring Traces\n\nScore specific traces using registered scorers for evaluation:\n\n```typescript\nconst result = await mastraClient.score({\n  scorerName: \"answer-relevancy\",\n  targets: [\n    { traceId: \"trace-1\", spanId: \"span-1\" }, // Score specific span\n    { traceId: \"trace-2\" }, // Score specific span which defaults to the parent span\n  ]\n});\n```\n\n## Getting Scores by Span\n\nRetrieve scores for a specific span within a trace:\n\n```typescript\nconst scores = await mastraClient.getScoresBySpan({\n  traceId: \"trace-123\",\n  spanId: \"span-456\",\n  page: 1,\n  perPage: 20\n});\n```\n## Related\n\n- [Agents API](./agents) - Learn about agent interactions that generate traces\n- [Workflows API](./workflows) - Understand workflow execution monitoring  \n\n\n---\ntitle: Mastra Client Telemetry API\ndescription: Learn how to retrieve and analyze traces from your Mastra application for monitoring and debugging using the client-js SDK.\n---\n\n# Telemetry API\n[EN] Source: https://mastra.ai/en/reference/client-js/telemetry\n\nThe Telemetry API provides methods to retrieve and analyze traces from your Mastra application. This helps you monitor and debug your application's behavior and performance.\n\n## Getting Traces\n\nRetrieve traces with optional filtering and pagination:\n\n```typescript\nconst telemetry = await mastraClient.getTelemetry({\n  name: \"trace-name\", // Optional: Filter by trace name\n  scope: \"scope-name\", // Optional: Filter by scope\n  page: 1, // Optional: Page number for pagination\n  perPage: 10, // Optional: Number of items per page\n  attribute: {\n    // Optional: Filter by custom attributes\n    key: \"value\",\n  },\n});\n```\n\n\n---\ntitle: Mastra Client Tools API\ndescription: Learn how to interact with and execute tools available in the Mastra platform using the client-js SDK.\n---\n\n# Tools API\n[EN] Source: https://mastra.ai/en/reference/client-js/tools\n\nThe Tools API provides methods to interact with and execute tools available in the Mastra platform.\n\n## Getting All Tools\n\nRetrieve a list of all available tools:\n\n```typescript\nconst tools = await mastraClient.getTools();\n```\n\n## Working with a Specific Tool\n\nGet an instance of a specific tool:\n\n```typescript\nconst tool = mastraClient.getTool(\"tool-id\");\n```\n\n## Tool Methods\n\n### Get Tool Details\n\nRetrieve detailed information about a tool:\n\n```typescript\nconst details = await tool.details();\n```\n\n### Execute Tool\n\nExecute a tool with specific arguments:\n\n```typescript\nconst result = await tool.execute({\n  args: {\n    param1: \"value1\",\n    param2: \"value2\",\n  },\n  threadId: \"thread-1\", // Optional: Thread context\n  resourceId: \"resource-1\", // Optional: Resource identifier\n});\n```\n\n\n---\ntitle: Mastra Client Vectors API\ndescription: Learn how to work with vector embeddings for semantic search and similarity matching in Mastra using the client-js SDK.\n---\n\n# Vectors API\n[EN] Source: https://mastra.ai/en/reference/client-js/vectors\n\nThe Vectors API provides methods to work with vector embeddings for semantic search and similarity matching in Mastra.\n\n## Working with Vectors\n\nGet an instance of a vector store:\n\n```typescript\nconst vector = mastraClient.getVector(\"vector-name\");\n```\n\n## Vector Methods\n\n### Get Vector Index Details\n\nRetrieve information about a specific vector index:\n\n```typescript\nconst details = await vector.details(\"index-name\");\n```\n\n### Create Vector Index\n\nCreate a new vector index:\n\n```typescript\nconst result = await vector.createIndex({\n  indexName: \"new-index\",\n  dimension: 128,\n  metric: \"cosine\", // 'cosine', 'euclidean', or 'dotproduct'\n});\n```\n\n### Upsert Vectors\n\nAdd or update vectors in an index:\n\n```typescript\nconst ids = await vector.upsert({\n  indexName: \"my-index\",\n  vectors: [\n    [0.1, 0.2, 0.3], // First vector\n    [0.4, 0.5, 0.6], // Second vector\n  ],\n  metadata: [{ label: \"first\" }, { label: \"second\" }],\n  ids: [\"id1\", \"id2\"], // Optional: Custom IDs\n});\n```\n\n### Query Vectors\n\nSearch for similar vectors:\n\n```typescript\nconst results = await vector.query({\n  indexName: \"my-index\",\n  queryVector: [0.1, 0.2, 0.3],\n  topK: 10,\n  filter: { label: \"first\" }, // Optional: Metadata filter\n  includeVector: true, // Optional: Include vectors in results\n});\n```\n\n### Get All Indexes\n\nList all available indexes:\n\n```typescript\nconst indexes = await vector.getIndexes();\n```\n\n### Delete Index\n\nDelete a vector index:\n\n```typescript\nconst result = await vector.delete(\"index-name\");\n```\n\n\n---\ntitle: Mastra Client Workflows (Legacy) API\ndescription: Learn how to interact with and execute automated legacy workflows in Mastra using the client-js SDK.\n---\n\n# Workflows (Legacy) API\n[EN] Source: https://mastra.ai/en/reference/client-js/workflows-legacy\n\nThe Workflows (Legacy) API provides methods to interact with and execute automated legacy workflows in Mastra.\n\n## Getting All Legacy Workflows\n\nRetrieve a list of all available legacy workflows:\n\n```typescript\nconst workflows = await mastraClient.getLegacyWorkflows();\n```\n\n## Working with a Specific Legacy Workflow\n\nGet an instance of a specific legacy workflow:\n\n```typescript\nconst workflow = mastraClient.getLegacyWorkflow(\"workflow-id\");\n```\n\n## Legacy Workflow Methods\n\n### Get Legacy Workflow Details\n\nRetrieve detailed information about a legacy workflow:\n\n```typescript\nconst details = await workflow.details();\n```\n\n### Start Legacy Workflow run asynchronously\n\nStart a legacy workflow run with triggerData and await full run results:\n\n```typescript\nconst { runId } = workflow.createRun();\n\nconst result = await workflow.startAsync({\n  runId,\n  triggerData: {\n    param1: \"value1\",\n    param2: \"value2\",\n  },\n});\n```\n\n### Resume Legacy Workflow run asynchronously\n\nResume a suspended legacy workflow step and await full run result:\n\n```typescript\nconst { runId } = createRun({ runId: prevRunId });\n\nconst result = await workflow.resumeAsync({\n  runId,\n  stepId: \"step-id\",\n  contextData: { key: \"value\" },\n});\n```\n\n### Watch Legacy Workflow\n\nWatch legacy workflow transitions\n\n```typescript\ntry {\n  // Get workflow instance\n  const workflow = mastraClient.getLegacyWorkflow(\"workflow-id\");\n\n  // Create a workflow run\n  const { runId } = workflow.createRun();\n\n  // Watch workflow run\n  workflow.watch({ runId }, (record) => {\n    // Every new record is the latest transition state of the workflow run\n\n    console.log({\n      activePaths: record.activePaths,\n      results: record.results,\n      timestamp: record.timestamp,\n      runId: record.runId,\n    });\n  });\n\n  // Start workflow run\n  workflow.start({\n    runId,\n    triggerData: {\n      city: \"New York\",\n    },\n  });\n} catch (e) {\n  console.error(e);\n}\n```\n\n### Resume Legacy Workflow\n\nResume legacy workflow run and watch legacy workflow step transitions\n\n```typescript\ntry {\n  //To resume a workflow run, when a step is suspended\n  const { run } = createRun({ runId: prevRunId });\n\n  //Watch run\n  workflow.watch({ runId }, (record) => {\n    // Every new record is the latest transition state of the workflow run\n\n    console.log({\n      activePaths: record.activePaths,\n      results: record.results,\n      timestamp: record.timestamp,\n      runId: record.runId,\n    });\n  });\n\n  //resume run\n  workflow.resume({\n    runId,\n    stepId: \"step-id\",\n    contextData: { key: \"value\" },\n  });\n} catch (e) {\n  console.error(e);\n}\n```\n\n### Legacy Workflow run result\n\nA legacy workflow run result yields the following:\n\n| Field         | Type                                                                           | Description                                                        |\n| ------------- | ------------------------------------------------------------------------------ | ------------------------------------------------------------------ |\n| `activePaths` | `Record<string, { status: string; suspendPayload?: any; stepPath: string[] }>` | Currently active paths in the workflow with their execution status |\n| `results`     | `LegacyWorkflowRunResult<any, any, any>['results']`                            | Results from the workflow execution                                |\n| `timestamp`   | `number`                                                                       | Unix timestamp of when this transition occurred                    |\n| `runId`       | `string`                                                                       | Unique identifier for this workflow run instance                   |\n\n\n---\ntitle: Mastra Client Workflows API\ndescription: Learn how to interact with and execute automated workflows in Mastra using the client-js SDK.\n---\n\n# Workflows API\n[EN] Source: https://mastra.ai/en/reference/client-js/workflows\n\nThe Workflows API provides methods to interact with and execute automated workflows in Mastra.\n\n## Getting All Workflows\n\nRetrieve a list of all available workflows:\n\n```typescript\nconst workflows = await mastraClient.getWorkflows();\n```\n\n## Working with a Specific Workflow\n\nGet an instance of a specific workflow as defined by the const name:\n\n```typescript filename=\"src/mastra/workflows/test-workflow.ts\"\nexport const testWorkflow = createWorkflow({\n  id: 'city-workflow'\n})\n```\n\n```typescript\nconst workflow = mastraClient.getWorkflow(\"testWorkflow\");\n```\n\n## Workflow Methods\n\n### Get Workflow Details\n\nRetrieve detailed information about a workflow:\n\n```typescript\nconst details = await workflow.details();\n```\n\n### Start workflow run asynchronously\n\nStart a workflow run with inputData and await full run results:\n\n```typescript\nconst run = await workflow.createRunAsync();\n\nconst result = await run.startAsync({\n  inputData: {\n    city: \"New York\",\n  },\n});\n```\n\n### Resume Workflow run asynchronously\n\nResume a suspended workflow step and await full run result:\n\n```typescript\nconst run = await workflow.createRunAsync();\n\nconst result = await run.resumeAsync({\n  step: \"step-id\",\n  resumeData: { key: \"value\" },\n});\n```\n\n### Watch Workflow\n\nWatch workflow transitions:\n\n```typescript\ntry {\n  const workflow = mastraClient.getWorkflow(\"testWorkflow\");\n\n  const run = await workflow.createRunAsync();\n\n  run.watch((record) => {\n    console.log(record);\n  });\n\n  const result = await run.start({\n    inputData: {\n      city: \"New York\",\n    },\n  });\n} catch (e) {\n  console.error(e);\n}\n```\n\n### Resume Workflow\n\nResume workflow run and watch workflow step transitions:\n\n```typescript\ntry {\n  const workflow = mastraClient.getWorkflow(\"testWorkflow\");\n\n  const run = await workflow.createRunAsync({ runId: prevRunId });\n\n  run.watch((record) => {\n    console.log(record);\n  });\n\n  run.resume({\n    step: \"step-id\",\n    resumeData: { key: \"value\" },\n  });\n} catch (e) {\n  console.error(e);\n}\n```\n\n### Stream Workflow\n\nStream workflow execution for real-time updates:\n\n```typescript\ntry {\n  const workflow = mastraClient.getWorkflow(\"testWorkflow\");\n\n  const run = await workflow.createRunAsync();\n\n  const stream = await run.stream({\n    inputData: {\n      city: 'New York',\n    },\n  });\n\n  for await (const chunk of stream) {\n    console.log(JSON.stringify(chunk, null, 2));\n  }\n} catch (e) {\n  console.error('Workflow error:', e);\n}\n```\n\n### Get Workflow Run result\n\nGet the result of a workflow run:\n\n```typescript\ntry  {\n  const workflow = mastraClient.getWorkflow(\"testWorkflow\");\n\n  const run = await workflow.createRunAsync();\n\n  // start the workflow run\n  const startResult = await run.start({\n    inputData: {\n      city: \"New York\",\n    },\n  });\n\n  const result = await workflow.runExecutionResult(run.runId);\n\n  console.log(result);\n} catch (e) {\n  console.error(e);\n}\n```\n\nThis is useful when dealing with long running workflows. You can use this to poll the result of the workflow run.\n\n### Workflow run result\n\nA workflow run result yields the following:\n\n| Field            | Type                                                                                                                                                                                                                                               | Description                                      |\n| ---------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------ |\n| `payload`        | `{currentStep?: {id: string, status: string, output?: Record<string, any>, payload?: Record<string, any>}, workflowState: {status: string, steps: Record<string, {status: string, output?: Record<string, any>, payload?: Record<string, any>}>}}` | The current step and workflow state of the run   |\n| `eventTimestamp` | `Date`                                                                                                                                                                                                                                             | The timestamp of the event                       |\n| `runId`          | `string`                                                                                                                                                                                                                                           | Unique identifier for this workflow run instance |\n\n\n","path":null,"size_bytes":36391,"size_tokens":null},"docs/mastra/06-reference/35_mastra-get-logs-by-run-id.md":{"content":"---\ntitle: \"Reference: Mastra.getLogsByRunId() | Core | Mastra Docs\"\ndescription: \"Documentation for the `Mastra.getLogsByRunId()` method in Mastra, which retrieves logs for a specific run ID and transport ID.\"\n---\n\n# Mastra.getLogsByRunId()\n[EN] Source: https://mastra.ai/en/reference/core/getLogsByRunId\n\nThe `.getLogsByRunId()` method is used to retrieve logs for a specific run ID and transport ID. This method requires a configured logger that supports the `getLogsByRunId` operation.\n\n## Usage example\n\n```typescript copy\nmastra.getLogsByRunId({ runId: \"123\", transportId: \"456\" });\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"runId\",\n      type: \"string\",\n      description: \"The run ID to retrieve logs for.\",\n    },\n    {\n      name: \"transportId\",\n      type: \"string\",\n      description: \"The transport ID to retrieve logs from.\",\n    },\n    {\n      name: \"fromDate\",\n      type: \"Date\",\n      description: \"Optional start date for filtering logs. e.g., new Date('2024-01-01').\",\n      optional: true,\n    },\n    {\n      name: \"toDate\",\n      type: \"Date\",\n      description: \"Optional end date for filtering logs. e.g., new Date('2024-01-31').\",\n      optional: true,\n    },\n    {\n      name: \"logLevel\",\n      type: \"LogLevel\",\n      description: \"Optional log level to filter by.\",\n      optional: true,\n    },\n    {\n      name: \"filters\",\n      type: \"Record<string, any>\",\n      description: \"Optional additional filters to apply to the log query.\",\n      optional: true,\n    },\n    {\n      name: \"page\",\n      type: \"number\",\n      description: \"Optional page number for pagination.\",\n      optional: true,\n    },\n    {\n      name: \"perPage\",\n      type: \"number\",\n      description: \"Optional number of logs per page for pagination.\",\n      optional: true,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"logs\",\n      type: \"Promise<any>\",\n      description: \"A promise that resolves to the logs for the specified run ID and transport ID.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Logging overview](../../docs/observability/logging.mdx)\n- [Logger reference](../../reference/observability/logger.mdx)\n\n\n","path":null,"size_bytes":2165,"size_tokens":null},"docs/mastra/06-reference/91_workflow-branch.md":{"content":"---\ntitle: \"Reference: Workflow.branch() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow.branch()` method in workflows, which creates conditional branches between steps.\n---\n\n# Workflow.branch()\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow-methods/branch\n\nThe `.branch()` method creates conditional branches between workflow steps, allowing for different paths to be taken based on the result of a previous step.\n\n## Usage example\n\n```typescript copy\nworkflow.branch([\n  [async ({ context }) => true, step1],\n  [async ({ context }) => false, step2],\n]);\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"steps\",\n      type: \"[() => boolean, Step]\",\n      description:\n        \"An array of tuples, each containing a condition function and a step to execute if the condition is true\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"NewWorkflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Conditional Branching Logic](../../../docs/workflows/control-flow#conditional-logic-with-branch)\n- [Conditional Branching Example](../../../examples/workflows/conditional-branching)\n\n\n","path":null,"size_bytes":1263,"size_tokens":null},"docs/mastra/01-agents/10_memory-overview.md":{"content":"---\ntitle: \"Memory Overview | Memory | Mastra Docs\"\ndescription: \"Learn how Mastra's memory system works with working memory, conversation history, and semantic recall.\"\n---\n\nimport { Steps } from \"nextra/components\";\n\n# Memory overview\n[EN] Source: https://mastra.ai/en/docs/memory/overview\n\nMemory in Mastra helps agents manage context across conversations by condensing relevant information into the language model's context window.\n\nMastra supports three types of memory: working memory, conversation history, and semantic recall. It uses a two-tier scoping system where memory can be isolated per conversation thread (thread-scoped) or shared across all conversations for the same user (resource-scoped).\n\nMastra's memory system uses [storage providers](#memory-storage-adapters) to persist conversation threads, messages, and working memory across application restarts.\n\n## Getting started\n\nFirst install the required dependencies:\n\n```bash copy\nnpm install @mastra/core @mastra/memory @mastra/libsql\n```\n\nThen add a storage adapter to the main Mastra instance. Any agent with memory enabled will use this shared storage to store and recall interactions.\n\n```typescript {6-8} filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { LibSQLStore } from \"@mastra/libsql\";\n\nexport const mastra = new Mastra({\n  // ...\n  storage: new LibSQLStore({\n    url: \":memory:\"\n  })\n});\n```\n\nNow, enable memory by passing a `Memory` instance to the agent's `memory` parameter:\n\n```typescript {3-5} filename=\"src/mastra/agents/test-agent.ts\" showLineNumbers copy\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\n\nexport const testAgent = new Agent({\n  // ...\n  memory: new Memory()\n});\n```\n\nThat memory instance has options you can configure for working memory, conversation history, and semantic recall.\n\n## Different types of memory\n\nMastra supports three types of memory: working memory, conversation history, and semantic recall. \n\n[**Working memory**](./working-memory.mdx) stores persistent user-specific details such as names, preferences, goals, and other structured data. (Compare this to ChatGPT where you can ask it to tell you about yourself). This is implemented as a block of Markdown text that the agent is able to update over time (or alternately, as a Zod schema)\n\n[**Conversation history**](./conversation-history.mdx) captures recent messages from the current conversation, providing short-term continuity and maintaining dialogue flow.\n\n[**Semantic recall**](./semantic-recall.mdx) retrieves older messages from past conversations based on semantic relevance. Matches are retrieved using vector search and can include surrounding context for better comprehension.\n\nMastra combines all memory types into a single context window. If the total exceeds the modelâ€™s token limit, use [memory processors](./memory-processors.mdx) to trim or filter messages before sending them to the model.\n\n## Scoping memory with threads and resources\n\nAll memory types are [thread-scoped](./working-memory.mdx#thread-scoped-memory-default) by default, meaning they apply only to a single conversation. [Resource-scoped](./working-memory.mdx#resource-scoped-memory) configuration allows working memory and semantic recall to persist across all threads that use the same user or entity.\n\n## Memory Storage Adapters\n\nTo persist and recall information between conversations, memory requires a storage adapter.\n\nSupported options include [LibSQL](/docs/memory/storage/memory-with-libsql), [MongoDB](/docs/memory/storage/memory-with-mongodb), [Postgres](/docs/memory/storage/memory-with-pg), and [Upstash](/docs/memory/storage/memory-with-upstash)\n\nWe use LibSQL out of the box because it is file-based or in-memory, so it is easy to install and works well with the playground.\n\n## Dedicated storage\n\nAgents can be configured with their own dedicated storage, keeping tasks, conversations, and recalled information separate across agents.\n\n### Adding storage to agents\n\nTo assign dedicated storage to an agent, install and import the required dependency and pass a `storage` instance to the `Memory` constructor:\n\n```typescript {3, 9-11} filename=\"src/mastra/agents/test-agent.ts\" showLineNumbers copy\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { LibSQLStore } from \"@mastra/libsql\";\n\nexport const testAgent = new Agent({\n  // ...\n  memory: new Memory({\n    // ...\n    storage: new LibSQLStore({\n      url: \"file:agent-memory.db\"\n    })\n  // ...\n  })\n});\n```\n\n## Viewing retrieved messages\n\nIf tracing is enabled in your Mastra deployment and memory is configured either with `lastMessages` and/or `semanticRecall`, the agentâ€™s trace output will show all messages retrieved for contextâ€”including both recent conversation history and messages recalled via semantic recall.\n\nThis is helpful for debugging, understanding agent decisions, and verifying that the agent is retrieving the right information for each request.\n\nFor more details on enabling and configuring tracing, see [Tracing](../observability/tracing).\n\n## Local development with LibSQL\n\nFor local development with `LibSQLStore`, you can inspect stored memory using the [SQLite Viewer](https://marketplace.visualstudio.com/items?itemName=qwtel.sqlite-viewer) extension in VS Code.\n\n![SQLite Viewer](/image/memory/memory-sqlite-viewer.jpg)\n\n## Next Steps\n\nNow that you understand the core concepts, continue to [semantic recall](./semantic-recall.mdx) to learn how to add RAG memory to your Mastra agents.\n\nAlternatively you can visit the [configuration reference](../../reference/memory/Memory.mdx) for available options.\n\n\n","path":null,"size_bytes":5710,"size_tokens":null},"docs/mastra/01-agents/14_memory-processors.md":{"content":"---\ntitle: \"Memory Processors | Memory | Mastra Docs\"\ndescription: \"Learn how to use memory processors in Mastra to filter, trim, and transform messages before they're sent to the language model to manage context window limits.\"\n---\n\n# Memory Processors\n[EN] Source: https://mastra.ai/en/docs/memory/memory-processors\n\nMemory Processors allow you to modify the list of messages retrieved from memory _before_ they are added to the agent's context window and sent to the LLM. This is useful for managing context size, filtering content, and optimizing performance.\n\nProcessors operate on the messages retrieved based on your memory configuration (e.g., `lastMessages`, `semanticRecall`). They do **not** affect the new incoming user message.\n\n## Built-in Processors\n\nMastra provides built-in processors:\n\n### `TokenLimiter`\n\nThis processor is used to prevent errors caused by exceeding the LLM's context window limit. It counts the tokens in the retrieved memory messages and removes the oldest messages until the total count is below the specified `limit`.\n\n```typescript copy showLineNumbers {9-12}\nimport { Memory } from \"@mastra/memory\";\nimport { TokenLimiter } from \"@mastra/memory/processors\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\n\nconst agent = new Agent({\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    processors: [\n      // Ensure the total tokens from memory don't exceed ~127k\n      new TokenLimiter(127000),\n    ],\n  }),\n});\n```\n\nThe `TokenLimiter` uses the `o200k_base` encoding by default (suitable for GPT-4o). You can specify other encodings if needed for different models:\n\n```typescript copy showLineNumbers {6-9}\n// Import the encoding you need (e.g., for older OpenAI models)\nimport cl100k_base from \"js-tiktoken/ranks/cl100k_base\";\n\nconst memoryForOlderModel = new Memory({\n  processors: [\n    new TokenLimiter({\n      limit: 16000, // Example limit for a 16k context model\n      encoding: cl100k_base,\n    }),\n  ],\n});\n```\n\nSee the [OpenAI cookbook](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken#encodings) or [`js-tiktoken` repo](https://github.com/dqbd/tiktoken) for more on encodings.\n\n### `ToolCallFilter`\n\nThis processor removes tool calls from the memory messages sent to the LLM. It saves tokens by excluding potentially verbose tool interactions from the context, which is useful if the details aren't needed for future interactions. It's also useful if you always want your agent to call a specific tool again and not rely on previous tool results in memory.\n\n```typescript copy showLineNumbers {5-14}\nimport { Memory } from \"@mastra/memory\";\nimport { ToolCallFilter, TokenLimiter } from \"@mastra/memory/processors\";\n\nconst memoryFilteringTools = new Memory({\n  processors: [\n    // Example 1: Remove all tool calls/results\n    new ToolCallFilter(),\n\n    // Example 2: Remove only noisy image generation tool calls/results\n    new ToolCallFilter({ exclude: [\"generateImageTool\"] }),\n\n    // Always place TokenLimiter last\n    new TokenLimiter(127000),\n  ],\n});\n```\n\n## Applying Multiple Processors\n\nYou can chain multiple processors. They execute in the order they appear in the `processors` array. The output of one processor becomes the input for the next.\n\n**Order matters!** It's generally best practice to place `TokenLimiter` **last** in the chain. This ensures it operates on the final set of messages after other filtering has occurred, providing the most accurate token limit enforcement.\n\n```typescript copy showLineNumbers {7-14}\nimport { Memory } from \"@mastra/memory\";\nimport { ToolCallFilter, TokenLimiter } from \"@mastra/memory/processors\";\n// Assume a hypothetical 'PIIFilter' custom processor exists\n// import { PIIFilter } from './custom-processors';\n\nconst memoryWithMultipleProcessors = new Memory({\n  processors: [\n    // 1. Filter specific tool calls first\n    new ToolCallFilter({ exclude: [\"verboseDebugTool\"] }),\n    // 2. Apply custom filtering (e.g., remove hypothetical PII - use with caution)\n    // new PIIFilter(),\n    // 3. Apply token limiting as the final step\n    new TokenLimiter(127000),\n  ],\n});\n```\n\n## Creating Custom Processors\n\nYou can create custom logic by extending the base `MemoryProcessor` class.\n\n```typescript copy showLineNumbers {5-20,24-27}\nimport { Memory } from \"@mastra/memory\";\nimport { CoreMessage, MemoryProcessorOpts } from \"@mastra/core\";\nimport { MemoryProcessor } from \"@mastra/core/memory\";\n\nclass ConversationOnlyFilter extends MemoryProcessor {\n  constructor() {\n    // Provide a name for easier debugging if needed\n    super({ name: \"ConversationOnlyFilter\" });\n  }\n\n  process(\n    messages: CoreMessage[],\n    _opts: MemoryProcessorOpts = {}, // Options passed during memory retrieval, rarely needed here\n  ): CoreMessage[] {\n    // Filter messages based on role\n    return messages.filter(\n      (msg) => msg.role === \"user\" || msg.role === \"assistant\",\n    );\n  }\n}\n\n// Use the custom processor\nconst memoryWithCustomFilter = new Memory({\n  processors: [\n    new ConversationOnlyFilter(),\n    new TokenLimiter(127000), // Still apply token limiting\n  ],\n});\n```\n\nWhen creating custom processors avoid mutating the input `messages` array or its objects directly.\n\n\n","path":null,"size_bytes":5245,"size_tokens":null},"docs/mastra/01-agents/15_memory-threads-resources.md":{"content":"---\ntitle: \"Memory Threads and Resources | Memory | Mastra Docs\"\ndescription: \"Learn how Mastra's memory system works with working memory, conversation history, and semantic recall.\"\n---\n\nimport { Callout } from \"nextra/components\";\n\n# Memory threads and resources\n[EN] Source: https://mastra.ai/en/docs/memory/threads-and-resources\n\nMastra organizes memory into threads, which are records that group related interactions, using two identifiers:\n\n1. **`thread`**: A globally unique ID representing the conversation (e.g., `support_123`). Must be unique across all resources.\n2. **`resource`**: The user or entity that owns the thread (e.g., `user_123`, `org_456`).\n\nThe `resource` is especially important for [resource-scoped memory](./working-memory.mdx#resource-scoped-memory), which allows memory to persist across all threads associated with the same user or entity.\n\n```typescript {4} showLineNumbers\nconst stream = await agent.stream(\"message for agent\", {\n  memory: {\n    thread: \"user-123\",\n    resource: \"test-123\"\n  }\n});\n```\n\n<Callout type=\"warning\">\nEven with memory configured, agents wonâ€™t store or recall information unless both `thread` and `resource` are provided.\n</Callout>\n\n> Mastra Playground sets `thread` and `resource` IDs automatically. In your own application, you must provide them manually as part of each `.generate()` or `.stream()` call.\n\n### Thread title generation\n\nMastra can automatically generate descriptive thread titles based on the user's first message. Enable this by setting `generateTitle` to `true`. This improves organization and makes it easier to display conversations in your UI.\n\n```typescript {3-7} showLineNumbers\nexport const testAgent = new Agent({\n  memory: new Memory({\n    options: {\n      threads: {\n        generateTitle: true,\n      }\n    },\n  })\n});\n```\n\n> Title generation runs asynchronously after the agent responds and does not affect response time. See the [full configuration reference](../../reference/memory/Memory.mdx#thread-title-generation) for details and examples.\n\n#### Optimizing title generation\n\nTitles are generated using your agent's model by default. To optimize cost or behavior, provide a smaller `model` and custom `instructions`. This keeps title generation separate from main conversation logic.\n\n```typescript {5-9} showLineNumbers\nexport const testAgent = new Agent({\n  // ...\n  memory: new Memory({\n    options: {\n      threads: {\n        generateTitle: {\n          model: openai(\"gpt-4.1-nano\"),\n          instructions: \"Generate a concise title based on the user's first message\",\n        },\n      },\n    }\n  })\n});\n```\n\n#### Dynamic model selection and instructions\n\nYou can configure thread title generation dynamically by passing functions to `model` and `instructions`. These functions receive the `runtimeContext` object, allowing you to adapt title generation based on user-specific values.\n\n```typescript {7-16} showLineNumbers\nexport const testAgent = new Agent({\n  // ...\n  memory: new Memory({\n    options: {\n      threads: {\n        generateTitle: {\n          model: ({ runtimeContext }) => {\n            const userTier = runtimeContext.get(\"userTier\");\n            return userTier === \"premium\" ? openai(\"gpt-4.1\") : openai(\"gpt-4.1-nano\");\n          },\n          instructions: ({ runtimeContext }) => {\n            const language = runtimeContext.get(\"userLanguage\") || \"English\";\n            return `Generate a concise, engaging title in ${language} based on the user's first message.`;\n          }\n        }\n      }\n    }\n  })\n});\n```\n\n\n","path":null,"size_bytes":3544,"size_tokens":null},"docs/mastra/03-workflows/00_agents-and-tools.md":{"content":"---\ntitle: \"Agents and Tools | Workflows | Mastra Docs\"\ndescription: \"Learn how to call agents and tools from workflow steps and choose between execute functions and step composition.\"\n---\n\n# Agents and Tools\n[EN] Source: https://mastra.ai/en/docs/workflows/agents-and-tools\n\nWorkflow steps can call agents to leverage LLM reasoning or call tools for type-safe logic. You can either invoke them from within a step's `execute` function or compose them directly as steps using `createStep()`.\n\n## Using agents in workflows\n\nUse agents in workflow steps when you need reasoning, language generation, or other LLM-based tasks. Call from a step's `execute` function for more control over the agent call (e.g., track conversation history or return structured output). Compose agents as steps when you don't need to modify how the agent is invoked.\n\n### Calling agents\n\nCall agents inside a step's `execute` function using `.generate()` or `.stream()`. This lets you modify the agent call and handle the response before passing it to the next step.\n\n```typescript {7-12} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nconst step1 = createStep({\n  // ...\n  execute: async ({ inputData, mastra }) => {\n    const { message } = inputData;\n\n    const testAgent = mastra.getAgent(\"testAgent\");\n    const response = await testAgent.generate(`Convert this message into bullet points: ${message}`, {\n      memory: {\n        thread: \"user-123\",\n        resource: \"test-123\"\n      }\n    });\n\n    return {\n      list: response.text\n    };\n  }\n});\n```\n\n> See [Calling Agents](../../examples/agents/calling-agents.mdx) for more examples.\n\n### Agents as steps\n\nCompose an agent as a step using `createStep()` when you don't need to modify the agent call. Use `.map()` to transform the previous step's output into a `prompt` the agent can use.\n\n![Agent as step](/image/workflows/workflows-agent-tools-agent-step.jpg)\n\n```typescript {1,3,8-13} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { testAgent } from \"../agents/test-agent\";\n\nconst step1 = createStep(testAgent);\n\nexport const testWorkflow = createWorkflow({\n  // ...\n})\n  .map(async ({ inputData }) => {\n    const { message } = inputData;\n    return {\n      prompt: `Convert this message into bullet points: ${message}`\n    };\n  })\n  .then(step1)\n  .then(step2)\n  .commit();\n```\n\n  > See [Input Data Mapping](./input-data-mapping.mdx) for more information.\n\nMastra agents use a default schema that expects a `prompt` string as input and returns a `text` string as output:\n\n```json\n{\n  inputSchema: {\n    prompt: string\n  },\n  outputSchema: {\n    text: string\n  }\n}\n```\n\n\n## Using tools in workflows\n\nUse tools in workflow steps to leverage existing tool logic. Call from a step's `execute` function when you need to prepare context or process responses. Compose tools as steps when you don't need to modify how the tool is used.\n\n### Calling tools\n\nCall tools inside a step's `execute` function using `.execute()`. This gives you more control over the tool's input context, or process its response before passing it to the next step.\n\n```typescript {8-13,16} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { testTool } from \"../tools/test-tool\";\n\nconst step2 = createStep({\n  // ...\n  execute: async ({ inputData, runtimeContext }) => {\n    const { formatted } = inputData;\n\n    const response = await testTool.execute({\n      context: {\n        text: formatted\n      },\n      runtimeContext\n    });\n\n    return {\n      emphasized: response.emphasized\n    };\n  }\n});\n```\n\n> See [Calling Tools](../../examples/tools/calling-tools.mdx) for more examples.\n\n### Tools as steps\n\nCompose a tool as a step using `createStep()` when the previous step's output matches the tool's input context. You can use `.map()` to transform the previous step's output if they don't.\n\n![Tool as step](/image/workflows/workflows-agent-tools-tool-step.jpg)\n\n```typescript {1,3,9-14} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { testTool } from \"../tools/test-tool\";\n\nconst step2 = createStep(testTool);\n\nexport const testWorkflow = createWorkflow({\n  // ...\n})\n  .then(step1)\n  .map(async ({ inputData }) => {\n    const { formatted } = inputData;\n    return {\n      text: formatted\n    };\n  })\n  .then(step2)\n  .commit();\n```\n\n> See [Input Data Mapping](./input-data-mapping.mdx) for more information.\n\n## Related\n\n- [Using Agents](../agents/overview.mdx)\n- [Using Tools](../tools-mcp/overview.mdx)\n\n\n\n","path":null,"size_bytes":4531,"size_tokens":null},"docs/mastra/06-reference/16_agent-model-output.md":{"content":"---\ntitle: \"Reference: MastraModelOutput | Agents | Mastra Docs\"\ndescription: \"Complete reference for MastraModelOutput - the stream object returned by agent.stream() with streaming and promise-based access to model outputs.\"\n---\n\nimport { Callout } from \"nextra/components\";\nimport { PropertiesTable } from \"@/components/properties-table\";\n\n# MastraModelOutput\n[EN] Source: https://mastra.ai/en/reference/streaming/agents/MastraModelOutput\n\nThe `MastraModelOutput` class is returned by [.stream()](./stream.mdx) and provides both streaming and promise-based access to model outputs. It supports structured output generation, tool calls, reasoning, and comprehensive usage tracking.\n\n```typescript\n// MastraModelOutput is returned by agent.stream()\nconst stream = await agent.stream(\"Hello world\");\n```\n\nFor setup and basic usage, see the [.stream()](./stream.mdx) method documentation.\n\n## Streaming Properties\n\nThese properties provide real-time access to model outputs as they're generated:\n\n<PropertiesTable\n  content={[\n    {\n      name: \"fullStream\",\n      type: \"ReadableStream<ChunkType<OUTPUT>>\",\n      description: \"Complete stream of all chunk types including text, tool calls, reasoning, metadata, and control chunks. Provides granular access to every aspect of the model's response.\",\n      properties: [{\n        type: \"ReadableStream\",\n        parameters: [\n          { name: \"ChunkType\", type: \"ChunkType<OUTPUT>\", description: \"All possible chunk types that can be emitted during streaming\" }\n        ]\n      }]\n    },\n    {\n      name: \"textStream\",\n      type: \"ReadableStream<string>\",\n      description: \"Stream of incremental text content only. Filters out all metadata, tool calls, and control chunks to provide just the text being generated.\"\n    },\n    {\n      name: \"objectStream\",\n      type: \"ReadableStream<PartialSchemaOutput<OUTPUT>>\",\n      description: \"Stream of progressive structured object updates when using output schemas. Emits partial objects as they're built up, allowing real-time visualization of structured data generation.\",\n      properties: [{\n        type: \"ReadableStream\",\n        parameters: [\n          { name: \"PartialSchemaOutput\", type: \"PartialSchemaOutput<OUTPUT>\", description: \"Partially completed object matching the defined schema\" }\n        ]\n      }]\n    },\n    {\n      name: \"elementStream\",\n      type: \"ReadableStream<InferSchemaOutput<OUTPUT> extends (infer T)[] ? T : never>\",\n      description: \"Stream of individual array elements when the output schema defines an array type. Each element is emitted as it's completed rather than waiting for the entire array.\"\n    }\n  ]}\n/>\n\n## Promise-based Properties\n\nThese properties resolve to final values after the stream completes:\n\n<PropertiesTable\n  content={[\n    {\n      name: \"text\",\n      type: \"Promise<string>\",\n      description: \"The complete concatenated text response from the model. Resolves when text generation is finished.\"\n    },\n    {\n      name: \"object\",\n      type: \"Promise<InferSchemaOutput<OUTPUT>>\",\n      description: \"The complete structured object response when using output schemas. Validated against the schema before resolving. Rejects if validation fails.\",\n      properties: [{\n        type: \"Promise\",\n        parameters: [\n          { name: \"InferSchemaOutput\", type: \"InferSchemaOutput<OUTPUT>\", description: \"Fully typed object matching the exact schema definition\" }\n        ]\n      }]\n    },\n    {\n      name: \"reasoning\",\n      type: \"Promise<string>\",\n      description: \"Complete reasoning text for models that support reasoning (like OpenAI's o1 series). Returns empty string for models without reasoning capability.\"\n    },\n    {\n      name: \"reasoningText\",\n      type: \"Promise<string | undefined>\",\n      description: \"Alternative access to reasoning content. May be undefined for models that don't support reasoning, while 'reasoning' returns empty string.\"\n    },\n    {\n      name: \"toolCalls\",\n      type: \"Promise<ToolCallChunk[]>\",\n      description: \"Array of all tool call chunks made during execution. Each chunk contains tool metadata and execution details.\",\n      properties: [{\n        type: \"ToolCallChunk\",\n        parameters: [\n          { name: \"type\", type: \"'tool-call'\", description: \"Chunk type identifier\" },\n          { name: \"runId\", type: \"string\", description: \"Execution run identifier\" },\n          { name: \"from\", type: \"ChunkFrom\", description: \"Source of the chunk (AGENT, WORKFLOW, etc.)\" },\n          { name: \"payload\", type: \"ToolCallPayload\", description: \"Tool call data including toolCallId, toolName, args, and execution details\" }\n        ]\n      }]\n    },\n    {\n      name: \"toolResults\",\n      type: \"Promise<ToolResultChunk[]>\",\n      description: \"Array of all tool result chunks corresponding to the tool calls. Contains execution results and error information.\",\n      properties: [{\n        type: \"ToolResultChunk\",\n        parameters: [\n          { name: \"type\", type: \"'tool-result'\", description: \"Chunk type identifier\" },\n          { name: \"runId\", type: \"string\", description: \"Execution run identifier\" },\n          { name: \"from\", type: \"ChunkFrom\", description: \"Source of the chunk (AGENT, WORKFLOW, etc.)\" },\n          { name: \"payload\", type: \"ToolResultPayload\", description: \"Tool result data including toolCallId, toolName, result, and error status\" }\n        ]\n      }]\n    },\n    {\n      name: \"usage\",\n      type: \"Promise<LanguageModelUsage>\",\n      description: \"Token usage statistics including input tokens, output tokens, total tokens, and reasoning tokens (for reasoning models).\",\n      properties: [{\n        type: \"Record\",\n        parameters: [\n          { name: \"inputTokens\", type: \"number\", description: \"Tokens consumed by the input prompt\" },\n          { name: \"outputTokens\", type: \"number\", description: \"Tokens generated in the response\" },\n          { name: \"totalTokens\", type: \"number\", description: \"Sum of input and output tokens\" },\n          { name: \"reasoningTokens\", type: \"number\", isOptional: true, description: \"Hidden reasoning tokens (for reasoning models)\" },\n          { name: \"cachedInputTokens\", type: \"number\", isOptional: true, description: \"Number of input tokens that were a cache hit\" }\n        ]\n      }]\n    },\n    {\n      name: \"finishReason\",\n      type: \"Promise<string | undefined>\",\n      description: \"Reason why generation stopped (e.g., 'stop', 'length', 'tool_calls', 'content_filter'). Undefined if the stream hasn't finished.\",\n      properties: [{\n        type: \"enum\",\n        parameters: [\n          { name: \"stop\", type: \"'stop'\", description: \"Model finished naturally\" },\n          { name: \"length\", type: \"'length'\", description: \"Hit maximum token limit\" },\n          { name: \"tool_calls\", type: \"'tool_calls'\", description: \"Model called tools\" },\n          { name: \"content_filter\", type: \"'content_filter'\", description: \"Content was filtered\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n## Error Properties\n\n<PropertiesTable\n  content={[\n    {\n      name: \"error\",\n      type: \"string | Error | { message: string; stack: string; } | undefined\",\n      description: \"Error information if the stream encountered an error. Undefined if no errors occurred. Can be a string message, Error object, or serialized error with stack trace.\"\n    }\n  ]}\n/>\n\n## Methods\n\n<PropertiesTable\n  content={[\n    {\n      name: \"getFullOutput\",\n      type: \"() => Promise<FullOutput>\",\n      description: \"Returns a comprehensive output object containing all results: text, structured object, tool calls, usage statistics, reasoning, and metadata. Convenient single method to access all stream results.\",\n      properties: [{\n        type: \"FullOutput\",\n        parameters: [\n          { name: \"text\", type: \"string\", description: \"Complete text response\" },\n          { name: \"object\", type: \"InferSchemaOutput<OUTPUT>\", isOptional: true, description: \"Structured output if schema was provided\" },\n          { name: \"toolCalls\", type: \"ToolCallChunk[]\", description: \"All tool call chunks made\" },\n          { name: \"toolResults\", type: \"ToolResultChunk[]\", description: \"All tool result chunks\" },\n          { name: \"usage\", type: \"Record<string, number>\", description: \"Token usage statistics\" },\n          { name: \"reasoning\", type: \"string\", isOptional: true, description: \"Reasoning text if available\" },\n          { name: \"finishReason\", type: \"string\", isOptional: true, description: \"Why generation finished\" }\n        ]\n      }]\n    },\n    {\n      name: \"consumeStream\",\n      type: \"(options?: ConsumeStreamOptions) => Promise<void>\",\n      description: \"Manually consume the entire stream without processing chunks. Useful when you only need the final promise-based results and want to trigger stream consumption.\",\n      properties: [{\n        type: \"ConsumeStreamOptions\",\n        parameters: [\n          { name: \"onError\", type: \"(error: Error) => void\", isOptional: true, description: \"Callback for handling stream errors\" }\n        ]\n      }]\n    }\n  ]}\n/>\n\n## Usage Examples\n\n### Basic Text Streaming\n\n```typescript\nconst stream = await agent.stream(\"Write a haiku\");\n\n// Stream text as it's generated\nfor await (const text of stream.textStream) {\n  process.stdout.write(text);\n}\n\n// Or get the complete text\nconst fullText = await stream.text;\nconsole.log(fullText);\n```\n\n### Structured Output Streaming\n\n```typescript\nconst stream = await agent.stream(\"Generate user data\", {\n  structuredOutput: {\n    schema: z.object({\n      name: z.string(),\n      age: z.number(),\n      email: z.string()\n    })\n  },\n});\n\n// Stream partial objects\nfor await (const partial of stream.objectStream) {\n  console.log(\"Progress:\", partial); // { name: \"John\" }, { name: \"John\", age: 30 }, ...\n}\n\n// Get final validated object\nconst user = await stream.object;\nconsole.log(\"Final:\", user); // { name: \"John\", age: 30, email: \"john@example.com\" }\n```\n```\n\n### Tool Calls and Results\n\n```typescript\nconst stream = await agent.stream(\"What's the weather in NYC?\", {\n  tools: { weather: weatherTool }\n});\n\n// Monitor tool calls\nconst toolCalls = await stream.toolCalls;\nconst toolResults = await stream.toolResults;\n\nconsole.log(\"Tools called:\", toolCalls);\nconsole.log(\"Results:\", toolResults);\n```\n\n### Complete Output Access\n\n```typescript\nconst stream = await agent.stream(\"Analyze this data\");\n\nconst output = await stream.getFullOutput();\nconsole.log({\n  text: output.text,\n  usage: output.usage,\n  reasoning: output.reasoning,\n  finishReason: output.finishReason\n});\n```\n\n### Full Stream Processing\n\n```typescript\nconst stream = await agent.stream(\"Complex task\");\n\nfor await (const chunk of stream.fullStream) {\n  switch (chunk.type) {\n    case 'text-delta':\n      process.stdout.write(chunk.payload.text);\n      break;\n    case 'tool-call':\n      console.log(`Calling ${chunk.payload.toolName}...`);\n      break;\n    case 'reasoning-delta':\n      console.log(`Reasoning: ${chunk.payload.text}`);\n      break;\n    case 'finish':\n      console.log(`Done! Reason: ${chunk.payload.stepResult.reason}`);\n      break;\n  }\n}\n```\n\n### Error Handling\n\n```typescript\nconst stream = await agent.stream(\"Analyze this data\");\n\ntry {\n  // Option 1: Handle errors in consumeStream\n  await stream.consumeStream({\n    onError: (error) => {\n      console.error(\"Stream error:\", error);\n    }\n  });\n\n  const result = await stream.text;\n} catch (error) {\n  console.error(\"Failed to get result:\", error);\n}\n\n// Option 2: Check error property\nconst result = await stream.getFullOutput();\nif (stream.error) {\n  console.error(\"Stream had errors:\", stream.error);\n}\n```\n\n## Related Types\n\n- [.stream()](./stream.mdx) - Method that returns MastraModelOutput\n- [ChunkType](../ChunkType.mdx) - All possible chunk types in the full stream\n\n\n","path":null,"size_bytes":11777,"size_tokens":null},"docs/mastra/06-reference/98_workflow-parallel.md":{"content":"---\ntitle: \"Reference: Workflow.parallel() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow.parallel()` method in workflows, which executes multiple steps in parallel.\n---\n\n# Workflow.parallel()\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow-methods/parallel\n\nThe `.parallel()` method executes multiple steps in parallel.\n\n## Usage example\n\n```typescript copy\nworkflow.parallel([step1, step2]);\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"steps\",\n      type: \"Step[]\",\n      description: \"The step instances to execute in parallel\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"Workflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Parallel Workflow Example](../../../examples/workflows/parallel-steps.mdx)\n\n\n","path":null,"size_bytes":906,"size_tokens":null},"docs/mastra/06-reference/52_memory-delete-messages.md":{"content":"---\ntitle: \"Reference: Memory.deleteMessages() | Memory | Mastra Docs\"\ndescription: \"Documentation for the `Memory.deleteMessages()` method in Mastra, which deletes multiple messages by their IDs.\"\n---\n\n# Memory.deleteMessages()\n[EN] Source: https://mastra.ai/en/reference/memory/deleteMessages\n\nThe `.deleteMessages()` method deletes multiple messages by their IDs.\n\n## Usage Example\n\n```typescript copy\nawait memory?.deleteMessages([\"671ae63f-3a91-4082-a907-fe7de78e10ec\"]);\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"messageIds\",\n      type: \"string[]\",\n      description: \"Array of message IDs to delete\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"void\",\n      type: \"Promise<void>\",\n      description: \"A promise that resolves when all messages are deleted\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript filename=\"src/test-memory.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\nimport { UIMessageWithMetadata } from \"@mastra/core/agent\";\n\nconst agent = mastra.getAgent(\"agent\");\nconst memory = await agent.getMemory();\n\nconst { uiMessages } = await memory!.query({ threadId: \"thread-123\" });\n\nconst messageIds = uiMessages.map((message: UIMessageWithMetadata) => message.id);\nawait memory?.deleteMessages([...messageIds]);\n```\n\n## Related\n\n- [Memory Class Reference](/reference/memory/Memory.mdx)\n- [query](/reference/memory/query.mdx)\n- [Getting Started with Memory](/docs/memory/overview.mdx)\n\n\n","path":null,"size_bytes":1512,"size_tokens":null},"docs/mastra/01-agents/23_example-working-memory-schema.md":{"content":"---\ntitle: \"Example: Working Memory with Schema | Memory | Mastra Docs\"\ndescription: Example showing how to use Zod schema to structure and validate working memory data.\n---\n\n# Working Memory with Schema\n[EN] Source: https://mastra.ai/en/examples/memory/working-memory-schema\n\nUse Zod schema to define the structure of information stored in working memory. Schema provides type safety and validation for the data that agents extract and persist across conversations.\n\nIt works with both streamed responses using `.stream()` and generated responses using `.generate()`, and requires a storage provider such as PostgreSQL, LibSQL, or Redis to persist data between sessions.\n\nThis example shows how to manage a todo list using a working memory schema.\n\n## Prerequisites\n\nThis example uses the `openai` model. Make sure to add `OPENAI_API_KEY` to your `.env` file.\n\n```bash filename=\".env\" copy\nOPENAI_API_KEY=<your-api-key>\n```\n\nAnd install the following package:\n\n```bash copy\nnpm install @mastra/libsql\n```\n\n## Adding memory to an agent\n\nTo add LibSQL memory to an agent, use the `Memory` class and pass a `storage` instance using `LibSQLStore`. The `url` can point to a remote location or local file.\n\n### Working memory with `schema`\n\nEnable working memory by setting `workingMemory.enabled` to `true`. This allows the agent to remember structured information between interactions.\n\nProviding a `schema` defines the shape in which the agent should remember information. In this example, it separates tasks into active and completed lists.\n\nThreads group related messages into conversations. When `generateTitle` is enabled, each thread is automatically given a descriptive name based on its content.\n\n```typescript filename=\"src/mastra/agents/example-working-memory-schema-agent.ts\" showLineNumbers copy\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { LibSQLStore } from \"@mastra/libsql\";\nimport { z } from \"zod\";\n\nexport const workingMemorySchemaAgent = new Agent({\n  name: \"working-memory-schema-agent\",\n  instructions: `\n    You are a todo list AI agent.\n    Always show the current list when starting a conversation.\n    For each task, include: title with index number, due date, description, status, and estimated time.\n    Use emojis for each field.\n    Support subtasks with bullet points.\n    Ask for time estimates to help with timeboxing.\n  `,\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    storage: new LibSQLStore({\n      url: \"file:working-memory-schema.db\"\n    }),\n    options: {\n      workingMemory: {\n        enabled: true,\n        schema: z.object({\n          items: z.array(\n            z.object({\n              title: z.string(),\n              due: z.string().optional(),\n              description: z.string(),\n              status: z.enum([\"active\", \"completed\"]).default(\"active\"),\n              estimatedTime: z.string().optional(),\n            })\n          )\n        })\n      },\n      threads: {\n        generateTitle: true\n      }\n    }\n  })\n});\n```\n\n## Usage examples\n\nThis example shows how to interact with an agent that uses a working memory schema to manage structured information. The agent updates and persists the todo list across multiple interactions within the same thread.\n\n### Streaming a response using `.stream()`\n\nThis example sends a message to the agent with a new task. The response is streamed and includes the updated todo list.\n\n```typescript filename=\"src/test-working-memory-schema-agent.ts\" showLineNumbers copy\nimport \"dotenv/config\";\n\nimport { mastra } from \"./mastra\";\n\nconst threadId = \"123\";\nconst resourceId = \"user-456\";\n\nconst agent = mastra.getAgent(\"workingMemorySchemaAgent\");\n\nconst stream = await agent.stream(\"Add a task: Build a new feature for our app. It should take about 2 hours and needs to be done by next Friday.\", {\n  memory: {\n    thread: threadId,\n    resource: resourceId\n  }\n});\n\nfor await (const chunk of stream.textStream) {\n  process.stdout.write(chunk);\n}\n```\n\n### Generating a response using `.generate()`\n\nThis example sends a message to the agent with a new task. The response is returned as a single message and includes the updated todo list.\n\n```typescript filename=\"src/test-working-memory-schema-agent.ts\" showLineNumbers copy\nimport \"dotenv/config\";\n\nimport { mastra } from \"./mastra\";\n\nconst threadId = \"123\";\nconst resourceId = \"user-456\";\n\nconst agent = mastra.getAgent(\"workingMemorySchemaAgent\");\n\nconst response = await agent.generate(\"Add a task: Build a new feature for our app. It should take about 2 hours and needs to be done by next Friday.\", {\n  memory: {\n    thread: threadId,\n    resource: resourceId\n  }\n});\n\nconsole.log(response.text);\n```\n\n## Example output\n\nThe output demonstrates how the agent formats and returns the updated todo list using the structure defined by the zod schema.\n\n```text\n# Todo List\n## Active Items\n1. ðŸ› ï¸ **Task:** Build a new feature for our app\n   - ðŸ“… **Due:** Next Friday\n   - ðŸ“ **Description:** Develop and integrate a new feature into the existing application.\n   - â³ **Status:** Not Started\n   - â²ï¸ **Estimated Time:** 2 hours\n\n## Completed Items\n- None yet\n```\n\n## Example storage object\n\nWorking memory stores data in `.json` format, which would look similar to the below:\n\n```json\n{\n  // ...\n  \"toolInvocations\": [\n    {\n      // ...\n      \"args\": {\n        \"memory\": {\n          \"items\": [\n            {\n              \"title\": \"Build a new feature for our app\",\n              \"due\": \"Next Friday\",\n              \"description\": \"\",\n              \"status\": \"active\",\n              \"estimatedTime\": \"2 hours\"\n            }\n          ]\n        }\n      },\n    }\n  ],\n}\n```\n\n## Related\n\n- [Calling Agents](../agents/calling-agents.mdx#from-the-command-line)\n- [Agent Memory](../../docs/agents/agent-memory.mdx)\n- [Serverless Deployment](../../docs/deployment/server-deployment.mdx#libsqlstore)\n\n\n","path":null,"size_bytes":5943,"size_tokens":null},"docs/mastra/01-agents/24_example-working-memory-template.md":{"content":"---\ntitle: \"Example: Working Memory with Template | Memory | Mastra Docs\"\ndescription: Example showing how to use Markdown template to structure working memory data.\n---\n\n# Working Memory with Template\n[EN] Source: https://mastra.ai/en/examples/memory/working-memory-template\n\nUse template to define the structure of information stored in working memory. Template helps agents extract and persist consistent, structured data across conversations.\n\nIt works with both streamed responses using `.stream()` and generated responses using `.generate()`, and requires a storage provider such as PostgreSQL, LibSQL, or Redis to persist data between sessions.\n\nThis example shows how to manage a todo list using a working memory template.\n\n## Prerequisites\n\nThis example uses the `openai` model. Make sure to add `OPENAI_API_KEY` to your `.env` file.\n\n```bash filename=\".env\" copy\nOPENAI_API_KEY=<your-api-key>\n```\n\nAnd install the following package:\n\n```bash copy\nnpm install @mastra/libsql\n```\n\n## Adding memory to an agent\n\nTo add LibSQL memory to an agent, use the `Memory` class and pass a `storage` instance using `LibSQLStore`. The `url` can point to a remote location or local file.\n\n### Working memory with `template`\n\nEnable working memory by setting `workingMemory.enabled` to `true`. This allows the agent to remember structured information between interactions.\n\nProviding a `template` helps define the structure of what should be remembered. In this example, the template organizes tasks into active and completed items using Markdown formatting.\n\nThreads group related messages into conversations. When `generateTitle` is enabled, each thread is automatically given a descriptive name based on its content.\n\n```typescript filename=\"src/mastra/agents/example-working-memory-template-agent.ts\" showLineNumbers copy\nimport { Memory } from \"@mastra/memory\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { LibSQLStore } from \"@mastra/libsql\";\n\nexport const workingMemoryTemplateAgent = new Agent({\n  name: \"working-memory-template-agent\",\n  instructions: `\n    You are a todo list AI agent.\n    Always show the current list when starting a conversation.\n    For each task, include: title with index number, due date, description, status, and estimated time.\n    Use emojis for each field.\n    Support subtasks with bullet points.\n    Ask for time estimates to help with timeboxing.\n  `,\n  model: openai(\"gpt-4o\"),\n  memory: new Memory({\n    storage: new LibSQLStore({\n      url: \"file:working-memory-template.db\"\n    }),\n    options: {\n      workingMemory: {\n        enabled: true,\n        template: `\n          # Todo List\n          ## Active Items\n          - Task 1: Example task\n            - Due: Feb 7 2028\n            - Description: This is an example task\n            - Status: Not Started\n            - Estimated Time: 2 hours\n\n          ## Completed Items\n          - None yet`\n      },\n      threads: {\n        generateTitle: true\n      }\n    }\n  })\n});\n```\n\n## Usage examples\n\nThis example shows how to interact with an agent that uses a working memory template to manage structured information. The agent updates and persists the todo list across multiple interactions within the same thread.\n\n### Streaming a response using `.stream()`\n\nThis example sends a message to the agent with a new task. The response is streamed and includes the updated todo list.\n\n```typescript filename=\"src/test-working-memory-template-agent.ts\" showLineNumbers copy\nimport \"dotenv/config\";\n\nimport { mastra } from \"./mastra\";\n\nconst threadId = \"123\";\nconst resourceId = \"user-456\";\n\nconst agent = mastra.getAgent(\"workingMemoryTemplateAgent\");\n\nconst stream = await agent.stream(\"Add a task: Build a new feature for our app. It should take about 2 hours and needs to be done by next Friday.\", {\n  memory: {\n    thread: threadId,\n    resource: resourceId\n  }\n});\n\nfor await (const chunk of stream.textStream) {\n  process.stdout.write(chunk);\n}\n```\n\n### Generating a response using `.generate()`\n\nThis example sends a message to the agent with a new task. The response is returned as a single message and includes the updated todo list.\n\n```typescript filename=\"src/test-working-memory-template-agent.ts\" showLineNumbers copy\nimport \"dotenv/config\";\n\nimport { mastra } from \"./mastra\";\n\nconst threadId = \"123\";\nconst resourceId = \"user-456\";\n\nconst agent = mastra.getAgent(\"workingMemoryTemplateAgent\");\n\nconst response = await agent.generate(\"Add a task: Build a new feature for our app. It should take about 2 hours and needs to be done by next Friday.\", {\n  memory: {\n    thread: threadId,\n    resource: resourceId\n  }\n});\n\nconsole.log(response.text);\n```\n\n## Example output\n\nThe output demonstrates how the agent formats and returns the updated todo list using the structure defined in the working memory template.\n\n```text\n# Todo List\n## Active Items\n1. ðŸ› ï¸ **Task:** Build a new feature for our app\n   - ðŸ“… **Due:** Next Friday\n   - ðŸ“ **Description:** Develop and integrate a new feature into the existing application.\n   - â³ **Status:** Not Started\n   - â²ï¸ **Estimated Time:** 2 hours\n\n## Completed Items\n- None yet\n```\n\n## Example storage object\n\nWorking memory stores data in `.json` format, which would look similar to the below:\n\n```json\n{\n  // ...\n  \"toolInvocations\": [\n    {\n      // ...\n      \"args\": {\n        \"memory\": \"# Todo List\\n## Active Items\\n- Task 1: Build a new feature for our app\\n  - Due: Next Friday\\n  - Description: Build a new feature for our app\\n  - Status: Not Started\\n  - Estimated Time: 2 hours\\n\\n## Completed Items\\n- None yet\"\n      },\n    }\n  ],\n}\n```\n\n## Related\n\n- [Calling Agents](../agents/calling-agents.mdx#from-the-command-line)\n- [Agent Memory](../../docs/agents/agent-memory.mdx)\n- [Serverless Deployment](../../docs/deployment/server-deployment.mdx#libsqlstore)\n\n\n","path":null,"size_bytes":5874,"size_tokens":null},"docs/mastra/04-models/00_models-overview.md":{"content":"---\ntitle: \"Models\"\ndescription: \"Access 47+ AI providers and 803+ models through Mastra's model router.\"\n---\n\n{/* This file is auto-generated by generate-model-docs.ts - DO NOT EDIT MANUALLY */}\n\nimport { CardGrid, CardGridItem } from \"@/components/cards/card-grid\";\nimport { Tab, Tabs } from \"@/components/tabs\";\nimport { Callout } from \"nextra/components\";\nimport { NetlifyLogo } from \"@/components/logos/NetlifyLogo\";\n\n# Model Providers\n[EN] Source: https://mastra.ai/en/models\n\nMastra provides a unified interface for working with LLMs across multiple providers, giving you access to 803 models from 47 providers through a single API.\n\n## Features\n\n- **One API for any model** - Access any model without having to install and manage additional provider dependencies.\n\n- **Access the newest AI** - Use new models the moment they're released, no matter which provider they come from. Avoid vendor lock-in with Mastra's provider-agnostic interface.  \n\n- [**Mix and match models**](#mix-and-match-models) - Use different models for different tasks. For example, run GPT-4o-mini for large-context processing, then switch to Claude Opus 4.1 for reasoning tasks.  \n\n- [**Model fallbacks**](#model-fallbacks) - If a provider experiences an outage, Mastra can automatically switch to another provider at the application level, minimizing latency compared to API gateways.\n\n## Basic usage\n\nWhether you're using OpenAI, Anthropic, Google, or a gateway like OpenRouter, specify the model as `\"provider/model-name\"` and Mastra handles the rest.\n\nMastra reads the relevant environment variable (e.g. `ANTHROPIC_API_KEY`) and routes requests to the provider. If an API key is missing, you'll get a clear runtime error showing exactly which variable to set.\n\n<Tabs items={[\"OpenAI\", \"Anthropic\", \"Google Gemini\", \"xAI\", \"OpenRouter\"]}>\n  <Tab>\n    ```typescript copy showLineNumbers\n    import { Agent } from \"@mastra/core\";\n\n    const agent = new Agent({\n      name: \"my-agent\",\n      instructions: \"You are a helpful assistant\",\n      model: \"openai/gpt-5\"\n    })\n    ```\n  </Tab>\n  <Tab>\n    ```typescript copy showLineNumbers\n    import { Agent } from \"@mastra/core\";\n\n    const agent = new Agent({\n      name: \"my-agent\", \n      instructions: \"You are a helpful assistant\",\n      model: \"anthropic/claude-4-5-sonnet\"\n    })\n    ```\n  </Tab>\n  <Tab>\n    ```typescript copy showLineNumbers\n    import { Agent } from \"@mastra/core\";\n\n    const agent = new Agent({\n      name: \"my-agent\",\n      instructions: \"You are a helpful assistant\",\n      model: \"google/gemini-2.5-flash\"\n    })\n    ```\n  </Tab>\n  <Tab>\n    ```typescript copy showLineNumbers\n    import { Agent } from \"@mastra/core\";\n\n    const agent = new Agent({\n      name: \"my-agent\",\n      instructions: \"You are a helpful assistant\",\n      model: \"xai/grok-4\"\n    })\n    ```\n  </Tab>\n  <Tab>\n    ```typescript copy showLineNumbers\n    import { Agent } from \"@mastra/core\";\n\n    const agent = new Agent({\n      name: \"my-agent\",\n      instructions: \"You are a helpful assistant\", \n      model: \"openrouter/anthropic/claude-haiku-4-5\"\n    })\n    ```\n  </Tab>\n</Tabs>\n\n\n## Model directory\n\nBrowse the directory of available models using the navigation on the left, or explore below.\n\n<CardGrid>\n    <CardGridItem\n      title=\"Gateways\"\n      href=\"./models/gateways\"\n    >\n      <div className=\"space-y-3\">\n        <div className=\"flex flex-col gap-2\">\n          <div className=\"flex items-center gap-2 text-sm\">\n            <img src=\"https://models.dev/logos/openrouter.svg\" alt=\"OpenRouter\" className=\"w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200\" />\n            <span>OpenRouter</span>\n          </div>\n          <div className=\"flex items-center gap-2 text-sm\">\n            <NetlifyLogo className=\"w-4 h-4\" />\n            <span>Netlify</span>\n          </div>\n          <div className=\"flex items-center gap-2 text-sm\">\n            <img src=\"https://models.dev/logos/vercel.svg\" alt=\"Vercel\" className=\"w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200\" />\n            <span>Vercel</span>\n          </div>\n        </div>\n\n      </div>\n    </CardGridItem>\n    <CardGridItem\n      title=\"Providers\"\n      href=\"./models/providers\"\n    >\n      <div className=\"space-y-3\">\n        <div className=\"flex flex-col gap-2\">\n          <div className=\"flex items-center gap-2 text-sm\">\n            <img src=\"https://models.dev/logos/openai.svg\" alt=\"OpenAI\" className=\"w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200\" />\n            <span>OpenAI</span>\n          </div>\n          <div className=\"flex items-center gap-2 text-sm\">\n            <img src=\"https://models.dev/logos/anthropic.svg\" alt=\"Anthropic\" className=\"w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200\" />\n            <span>Anthropic</span>\n          </div>\n          <div className=\"flex items-center gap-2 text-sm\">\n            <img src=\"https://models.dev/logos/google.svg\" alt=\"Google\" className=\"w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200\" />\n            <span>Google</span>\n          </div>\n        </div>\n        <div className=\"text-sm text-gray-600 dark:text-gray-400 mt-3\">+ 41 more</div>\n      </div>\n    </CardGridItem>\n</CardGrid>\n\nYou can also discover models directly in your editor. Mastra provides full autocomplete for the `model` field - just start typing, and your IDE will show available options.\n\nAlternatively, browse and test models in the [Playground](/docs/server-db/local-dev-playground) UI.\n\n<Callout type=\"info\">\nIn development, we auto-refresh your local model list every hour, ensuring your TypeScript autocomplete and Playground stay up-to-date with the latest models. To disable, set `MASTRA_AUTO_REFRESH_PROVIDERS=false`. Auto-refresh is disabled by default in production.\n</Callout>\n\n\n## Mix and match models\n\nSome models are faster but less capable, while others offer larger context windows or stronger reasoning skills. Use different models from the same provider, or mix and match across providers to fit each task.\n\n```typescript showLineNumbers\nimport { Agent } from \"@mastra/core\";\n\n// Use a cost-effective model for document processing\nconst documentProcessor = new Agent({\n  name: \"document-processor\",\n  instructions: \"Extract and summarize key information from documents\",\n  model: \"openai/gpt-4o-mini\" \n})\n\n// Use a powerful reasoning model for complex analysis\nconst reasoningAgent = new Agent({\n  name: \"reasoning-agent\", \n  instructions: \"Analyze data and provide strategic recommendations\",\n  model: \"anthropic/claude-opus-4-1\"\n})\n```\n## Dynamic model selection\n\nSince models are just strings, you can select them dynamically based on [runtime context](/docs/server-db/runtime-context), variables, or any other logic.\n\n```typescript showLineNumbers\nconst agent = new Agent({\n  name: \"dynamic-assistant\",\n  model: ({ runtimeContext }) => {\n    const provider = runtimeContext.get(\"provider-id\");\n    const model = runtimeContext.get(\"model-id\");\n    return `${provider}/${model}`;\n  },\n});\n```\n\nThis enables powerful patterns:\n\n- A/B testing - Compare model performance in production.\n- User-selectable models - Let users choose their preferred model in your app.\n- Multi-tenant applications - Each customer can bring their own API keys and model preferences.\n\n## Provider-specific options\n\nDifferent model providers expose their own configuration options. With OpenAI, you might adjust the `reasoningEffort`. With Anthropic, you might tune `cacheControl`. Mastra lets you set these specific `providerOptions` either at the agent level or per message.\n\n```typescript showLineNumbers\n// Agent level (apply to all future messages)\nconst planner = new Agent({\n  instructions: {\n    role: \"system\",\n    content: \"You are a helpful assistant.\",\n    providerOptions: {\n      openai: { reasoningEffort: \"low\" }\n    }\n  },\n  model: \"openai/o3-pro\",\n});\n\nconst lowEffort = \n  await planner.generate(\"Plan a simple 3 item dinner menu\");\n\n// Message level (apply only to this message)\nconst highEffort = await planner.generate([\n  {\n    role: \"user\",\n    content: \"Plan a simple 3 item dinner menu for a celiac\",\n    providerOptions: {\n      openai: { reasoningEffort: \"high\" }\n    }\n  }\n]);\n```\n\n## Custom headers\n\nIf you need to specify custom headers, such as an organization ID or other provider-specific fields, use this syntax.\n\n\n```typescript showLineNumbers\nconst agent = new Agent({\n  name: \"custom-agent\",\n  model: {\n    id: \"openai/gpt-4-turbo\",\n    apiKey: process.env.OPENAI_API_KEY,\n    headers: {\n      \"OpenAI-Organization\": \"org-abc123\"\n    }\n  }\n});\n```\n<Callout type=\"info\">\nConfiguration differs by provider. See the provider pages in the left navigation for details on custom headers.\n</Callout>\n\n## Model fallbacks\n\nRelying on a single model creates a single point of failure for your application. Model fallbacks provide automatic failover between models and providers. If the primary model becomes unavailable, requests are retried against the next configured fallback until one succeeds.\n\n\n```typescript showLineNumbers\nimport { Agent } from '@mastra/core';\n\nconst agent = new Agent({\n  name: 'resilient-assistant',\n  instructions: 'You are a helpful assistant.',\n  model: [\n    {\n      model: \"openai/gpt-5\",\n      maxRetries: 3,\n    },\n    {\n      model: \"anthropic/claude-4-5-sonnet\",\n      maxRetries: 2,\n    },\n    {\n      model: \"google/gemini-2.5-pro\",\n      maxRetries: 2,\n    },\n  ],\n});\n```\nMastra tries your primary model first. If it encounters a 500 error, rate limit, or timeout, it automatically switches to your first fallback. If that fails too, it moves to the next. Each model gets its own retry count before moving on.\n\nYour users never experience the disruption - the response comes back with the same format, just from a different model. The error context is preserved as the system moves through your fallback chain, ensuring clean error propagation while maintaining streaming compatibility.\n\n## Use AI SDK with Mastra\n\nMastra supports AI SDK provider modules, should you need to use them directly.\n\n\n```typescript showLineNumbers\nimport { groq } from '@ai-sdk/groq';\nimport { Agent } from \"@mastra/core\";\n\nconst agent = new Agent({\n  name: \"my-agent\",\n  model: groq('gemma2-9b-it')\n})\n```\nYou can use an AI SDK model (e.g. `groq('gemma2-9b-it')`) anywhere that accepts a `\"provider/model\"` string, including within model router fallbacks and [scorers](/docs/scorers/overview).\n\n","path":null,"size_bytes":10497,"size_tokens":null},"docs/mastra/03-workflows/02_error-handling.md":{"content":"---\ntitle: \"Error Handling in Workflows | Workflows | Mastra Docs\"\ndescription: \"Learn how to handle errors in Mastra workflows using step retries, conditional branching, and monitoring.\"\n---\n\n# Error Handling\n[EN] Source: https://mastra.ai/en/docs/workflows/error-handling\n\nMastra provides a built-in retry mechanism for workflows or steps that fail due to transient errors. This is particularly useful for steps that interact with external services or resources that might experience temporary unavailability.\n\n## Workflow-level using `retryConfig`\n\nYou can configure retries at the workflow level, which applies to all steps in the workflow:\n\n```typescript {8-11} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\n\nexport const testWorkflow = createWorkflow({\n  // ...\n  retryConfig: {\n    attempts: 5,\n    delay: 2000\n  }\n})\n  .then(step1)\n  .commit();\n```\n\n## Step-level using `retries`\n\nYou can configure retries for individual steps using the `retries` property. This overrides the workflow-level retry configuration for that specific step:\n\n```typescript {17} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({\n  // ...\n  execute: async () => {\n    const response = await // ...\n\n    if (!response.ok) {\n      throw new Error('Error');\n    }\n\n    return {\n      value: \"\"\n    };\n  },\n  retries: 3\n});\n```\n\n## Conditional branching\n\nYou can create alternative workflow paths based on the success or failure of previous steps using conditional logic:\n\n```typescript {15,19,33-34} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({\n  // ...\n  execute: async () => {\n    try {\n      const response = await // ...\n\n      if (!response.ok) {\n        throw new Error('error');\n      }\n\n      return {\n        status: \"ok\"\n      };\n    } catch (error) {\n      return {\n        status: \"error\"\n      };\n    }\n  }\n});\n\nconst step2 = createStep({...});\nconst fallback = createStep({...});\n\nexport const testWorkflow = createWorkflow({\n  // ...\n})\n  .then(step1)\n  .branch([\n    [async ({ inputData: { status } }) => status === \"ok\", step2],\n    [async ({ inputData: { status } }) => status === \"error\", fallback]\n  ])\n  .commit();\n```\n\n## Check previous step results\n\nUse `getStepResult()` to inspect a previous stepâ€™s results.\n\n```typescript {10} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\n\nconst step2 = createStep({\n  // ...\n  execute: async ({ getStepResult }) => {\n\n    const step1Result = getStepResult(step1);\n\n    return {\n      value: \"\"\n    };\n  }\n});\n```\n\n## Exiting early with `bail()`\n\nUse `bail()` in a step to exit early with a successful result. This returns the provided payload as the step output and ends workflow execution.\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({\n  id: 'step1',\n  execute: async ({ bail }) => {\n    return bail({ result: 'bailed' });\n  },\n  inputSchema: z.object({ value: z.string() }),\n  outputSchema: z.object({ result: z.string() }),\n});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .commit();\n```\n\n## Exiting early with `Error()`\n\nUse `throw new Error()` in a step to exit with an error.\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({\n  id: 'step1',\n  execute: async () => {\n    throw new Error('error');\n  },\n  inputSchema: z.object({ value: z.string() }),\n  outputSchema: z.object({ result: z.string() }),\n});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .commit();\n```\n\n## Monitor errors with `watch()`\n\nYou can monitor workflows for errors using the `watch` method:\n\n```typescript {11} filename=\"src/test-workflow.ts\" showLineNumbers copy\nimport { mastra } from \"../src/mastra\";\n\nconst workflow = mastra.getWorkflow(\"testWorkflow\");\nconst run = await workflow.createRunAsync();\n\nrun.watch((event) => {\n  const {\n    payload: { currentStep }\n  } = event;\n\n  console.log(currentStep?.payload?.status);\n});\n\n```\n\n##Â Monitor errors with `stream()`\n\nYou can monitor workflows for errors using `stream`:\n\n```typescript {11} filename=\"src/test-workflow.ts\" showLineNumbers copy\nimport { mastra } from \"../src/mastra\";\n\nconst workflow = mastra.getWorkflow(\"testWorkflow\");\n\nconst run = await workflow.createRunAsync();\n\nconst stream = await run.stream({\n  inputData: {\n    value: \"initial data\"\n  }\n});\n\nfor await (const chunk of stream.stream) {\n  console.log(chunk.payload.output.stats);\n}\n\n```\n\n## Related\n\n- [Control Flow](./control-flow.mdx)\n- [Conditional Branching](./control-flow.mdx#conditional-logic-with-branch)\n- [Running Workflows](../../examples/workflows/running-workflows.mdx)\n\n\n","path":null,"size_bytes":5382,"size_tokens":null},"docs/mastra/06-reference/53_memory-get-thread-by-id.md":{"content":"---\ntitle: \"Reference: Memory.getThreadById() | Memory | Mastra Docs\"\ndescription: \"Documentation for the `Memory.getThreadById()` method in Mastra, which retrieves a specific thread by its ID.\"\n---\n\n# Memory.getThreadById()\n[EN] Source: https://mastra.ai/en/reference/memory/getThreadById\n\nThe `.getThreadById()` method retrieves a specific thread by its ID.\n\n## Usage Example\n\n```typescript\nawait memory?.getThreadById({ threadId: \"thread-123\" });\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"threadId\",\n      type: \"string\",\n      description: \"The ID of the thread to be retrieved.\",\n      isOptional: false,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"thread\",\n      type: \"Promise<StorageThreadType | null>\",\n      description: \"A promise that resolves to the thread associated with the given ID, or null if not found.\",\n    },\n  ]}\n/>\n\n### Related\n\n- [Memory Class Reference](/reference/memory/Memory.mdx)\n- [Getting Started with Memory](/docs/memory/overview.mdx) (Covers threads concept)\n- [createThread](/reference/memory/createThread.mdx)\n- [getThreadsByResourceId](/reference/memory/getThreadsByResourceId.mdx)\n\n\n","path":null,"size_bytes":1182,"size_tokens":null},"docs/mastra/02-tools/00_using-tools.md":{"content":"---\ntitle: \"Using Tools | Tools & MCP | Mastra Docs\"\ndescription: Understand what tools are in Mastra, how to add them to agents, and best practices for designing effective tools.\n---\n\nimport { Steps } from \"nextra/components\";\n\n# Using Tools\n[EN] Source: https://mastra.ai/en/docs/tools-mcp/overview\n\nTools are functions that agents can execute to perform specific tasks or access external information. They extend an agent's capabilities beyond simple text generation, allowing interaction with APIs, databases, or other systems.\n\nEach tool typically defines:\n\n- **Inputs:** What information the tool needs to run (defined with an `inputSchema`, often using Zod).\n- **Outputs:** The structure of the data the tool returns (defined with an `outputSchema`).\n- **Execution Logic:** The code that performs the tool's action.\n- **Description:** Text that helps the agent understand what the tool does and when to use it.\n\n## Creating Tools\n\nIn Mastra, you create tools using the [`createTool`](/reference/tools/create-tool) function from the `@mastra/core/tools` package.\n\n```typescript filename=\"src/mastra/tools/weatherInfo.ts\" copy\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nconst getWeatherInfo = async (city: string) => {\n  // Replace with an actual API call to a weather service\n  console.log(`Fetching weather for ${city}...`);\n  // Example data structure\n  return { temperature: 20, conditions: \"Sunny\" };\n};\n\nexport const weatherTool = createTool({\n  id: \"Get Weather Information\",\n  description: `Fetches the current weather information for a given city`,\n  inputSchema: z.object({\n    city: z.string().describe(\"City name\"),\n  }),\n  outputSchema: z.object({\n    temperature: z.number(),\n    conditions: z.string(),\n  }),\n  execute: async ({ context: { city } }) => {\n    console.log(\"Using tool to fetch weather information for\", city);\n    return await getWeatherInfo(city);\n  },\n});\n```\n\nThis example defines a `weatherTool` with an input schema for the city, an output schema for the weather data, and an `execute` function that contains the tool's logic.\n\nWhen creating tools, keep tool descriptions simple and focused on **what** the tool does and **when** to use it, emphasizing its primary use case. Technical details belong in the parameter schemas, guiding the agent on _how_ to use the tool correctly with descriptive names, clear descriptions, and explanations of default values.\n\n## Adding Tools to an Agent\n\nTo make tools available to an agent, you configure them in the agent's definition. Mentioning available tools and their general purpose in the agent's system prompt can also improve tool usage. For detailed steps and examples, see the guide on [Using Tools and MCP with Agents](/docs/agents/using-tools-and-mcp#add-tools-to-an-agent).\n\n## Using `RuntimeContext`\n\nUse [RuntimeContext](../server-db/runtime-context.mdx) to access request-specific values. This lets you conditionally adjust behavior based on the context of the request.\n\n```typescript filename=\"src/mastra/tools/test-tool.ts\" showLineNumbers\nexport type UserTier = {\n  \"user-tier\": \"enterprise\" | \"pro\";\n};\n\nconst advancedTools = () => {\n  // ...\n};\n\nconst baseTools =  () => {\n  // ...\n};\n\nexport const testTool = createTool({\n  // ...\n  execute: async ({ runtimeContext }) => {\n    const userTier = runtimeContext.get(\"user-tier\") as UserTier[\"user-tier\"];\n\n    return userTier === \"enterprise\"\n      ? advancedTools\n      : baseTools;\n  }\n});\n```\n\n> See [Runtime Context](../server-db/runtime-context.mdx) for more information.\n\n## Testing with Mastra Playground\n\nUse the Mastra [Playground](../server-db/local-dev-playground.mdx) to test tools with different inputs, inspect execution results, and verify tool behavior.\n\n\n\n---\ntitle: Voice in Mastra | Mastra Docs\ndescription: Overview of voice capabilities in Mastra, including text-to-speech, speech-to-text, and real-time speech-to-speech interactions.\n---\n\nimport { Tabs } from \"nextra/components\";\nimport { AudioPlayback } from \"@/components/audio-playback\";\n\n# Voice in Mastra\n[EN] Source: https://mastra.ai/en/docs/voice/overview\n\nMastra's Voice system provides a unified interface for voice interactions, enabling text-to-speech (TTS), speech-to-text (STT), and real-time speech-to-speech (STS) capabilities in your applications.\n\n## Adding Voice to Agents\n\nTo learn how to integrate voice capabilities into your agents, check out the [Adding Voice to Agents](../agents/adding-voice.mdx) documentation. This section covers how to use both single and multiple voice providers, as well as real-time interactions.\n\n```typescript\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { OpenAIVoice } from \"@mastra/voice-openai\";\n\n// Initialize OpenAI voice for TTS\n\nconst voiceAgent = new Agent({\n  name: \"Voice Agent\",\n  instructions:\n    \"You are a voice assistant that can help users with their tasks.\",\n  model: openai(\"gpt-4o\"),\n  voice: new OpenAIVoice(),\n});\n```\n\nYou can then use the following voice capabilities:\n\n### Text to Speech (TTS)\n\nTurn your agent's responses into natural-sounding speech using Mastra's TTS capabilities.\nChoose from multiple providers like OpenAI, ElevenLabs, and more.\n\nFor detailed configuration options and advanced features, check out our [Text-to-Speech guide](./text-to-speech).\n\n{/*\nLLM CONTEXT: This Tabs component demonstrates Text-to-Speech (TTS) implementation across different voice providers.\nEach tab shows how to set up and use a specific TTS provider (OpenAI, Azure, ElevenLabs, etc.) with Mastra agents.\nThe tabs help users compare different TTS providers and choose the one that best fits their needs.\nEach tab includes complete code examples showing agent setup, text generation, and audio playback.\nThe providers include OpenAI, Azure, ElevenLabs, PlayAI, Google, Cloudflare, Deepgram, Speechify, Sarvam, and Murf.\n*/}\n\n<Tabs items={[\"OpenAI\", \"Azure\", \"ElevenLabs\", \"PlayAI\", \"Google\", \"Cloudflare\", \"Deepgram\", \"Speechify\", \"Sarvam\", \"Murf\"]}>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { OpenAIVoice } from \"@mastra/voice-openai\";\nimport { playAudio } from \"@mastra/node-audio\";\n\nconst voiceAgent = new Agent({\nname: \"Voice Agent\",\ninstructions: \"You are a voice assistant that can help users with their tasks.\",\nmodel: openai(\"gpt-4o\"),\nvoice: new OpenAIVoice(),\n});\n\nconst { text } = await voiceAgent.generate('What color is the sky?');\n\n// Convert text to speech to an Audio Stream\nconst audioStream = await voiceAgent.voice.speak(text, {\nspeaker: \"default\", // Optional: specify a speaker\nresponseFormat: \"wav\", // Optional: specify a response format\n});\n\nplayAudio(audioStream);\n\n````\n\nVisit the [OpenAI Voice Reference](/reference/voice/openai) for more information on the OpenAI voice provider.\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { AzureVoice } from \"@mastra/voice-azure\";\nimport { playAudio } from \"@mastra/node-audio\";\n\nconst voiceAgent = new Agent({\n  name: \"Voice Agent\",\n  instructions: \"You are a voice assistant that can help users with their tasks.\",\n  model: openai(\"gpt-4o\"),\n  voice: new AzureVoice(),\n});\n\nconst { text } = await voiceAgent.generate('What color is the sky?');\n\n// Convert text to speech to an Audio Stream\nconst audioStream = await voiceAgent.voice.speak(text, {\n  speaker: \"en-US-JennyNeural\", // Optional: specify a speaker\n});\n\nplayAudio(audioStream);\n````\n\nVisit the [Azure Voice Reference](/reference/voice/azure) for more information on the Azure voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { ElevenLabsVoice } from \"@mastra/voice-elevenlabs\";\nimport { playAudio } from \"@mastra/node-audio\";\n\nconst voiceAgent = new Agent({\nname: \"Voice Agent\",\ninstructions: \"You are a voice assistant that can help users with their tasks.\",\nmodel: openai(\"gpt-4o\"),\nvoice: new ElevenLabsVoice(),\n});\n\nconst { text } = await voiceAgent.generate('What color is the sky?');\n\n// Convert text to speech to an Audio Stream\nconst audioStream = await voiceAgent.voice.speak(text, {\nspeaker: \"default\", // Optional: specify a speaker\n});\n\nplayAudio(audioStream);\n\n````\n\nVisit the [ElevenLabs Voice Reference](/reference/voice/elevenlabs) for more information on the ElevenLabs voice provider.\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { PlayAIVoice } from \"@mastra/voice-playai\";\nimport { playAudio } from \"@mastra/node-audio\";\n\nconst voiceAgent = new Agent({\n  name: \"Voice Agent\",\n  instructions: \"You are a voice assistant that can help users with their tasks.\",\n  model: openai(\"gpt-4o\"),\n  voice: new PlayAIVoice(),\n});\n\nconst { text } = await voiceAgent.generate('What color is the sky?');\n\n// Convert text to speech to an Audio Stream\nconst audioStream = await voiceAgent.voice.speak(text, {\n  speaker: \"default\", // Optional: specify a speaker\n});\n\nplayAudio(audioStream);\n````\n\nVisit the [PlayAI Voice Reference](/reference/voice/playai) for more information on the PlayAI voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { GoogleVoice } from \"@mastra/voice-google\";\nimport { playAudio } from \"@mastra/node-audio\";\n\nconst voiceAgent = new Agent({\nname: \"Voice Agent\",\ninstructions: \"You are a voice assistant that can help users with their tasks.\",\nmodel: openai(\"gpt-4o\"),\nvoice: new GoogleVoice(),\n});\n\nconst { text } = await voiceAgent.generate('What color is the sky?');\n\n// Convert text to speech to an Audio Stream\nconst audioStream = await voiceAgent.voice.speak(text, {\nspeaker: \"en-US-Studio-O\", // Optional: specify a speaker\n});\n\nplayAudio(audioStream);\n\n````\n\nVisit the [Google Voice Reference](/reference/voice/google) for more information on the Google voice provider.\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { CloudflareVoice } from \"@mastra/voice-cloudflare\";\nimport { playAudio } from \"@mastra/node-audio\";\n\nconst voiceAgent = new Agent({\n  name: \"Voice Agent\",\n  instructions: \"You are a voice assistant that can help users with their tasks.\",\n  model: openai(\"gpt-4o\"),\n  voice: new CloudflareVoice(),\n});\n\nconst { text } = await voiceAgent.generate('What color is the sky?');\n\n// Convert text to speech to an Audio Stream\nconst audioStream = await voiceAgent.voice.speak(text, {\n  speaker: \"default\", // Optional: specify a speaker\n});\n\nplayAudio(audioStream);\n````\n\nVisit the [Cloudflare Voice Reference](/reference/voice/cloudflare) for more information on the Cloudflare voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { DeepgramVoice } from \"@mastra/voice-deepgram\";\nimport { playAudio } from \"@mastra/node-audio\";\n\nconst voiceAgent = new Agent({\nname: \"Voice Agent\",\ninstructions: \"You are a voice assistant that can help users with their tasks.\",\nmodel: openai(\"gpt-4o\"),\nvoice: new DeepgramVoice(),\n});\n\nconst { text } = await voiceAgent.generate('What color is the sky?');\n\n// Convert text to speech to an Audio Stream\nconst audioStream = await voiceAgent.voice.speak(text, {\nspeaker: \"aura-english-us\", // Optional: specify a speaker\n});\n\nplayAudio(audioStream);\n\n````\n\nVisit the [Deepgram Voice Reference](/reference/voice/deepgram) for more information on the Deepgram voice provider.\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { SpeechifyVoice } from \"@mastra/voice-speechify\";\nimport { playAudio } from \"@mastra/node-audio\";\n\nconst voiceAgent = new Agent({\n  name: \"Voice Agent\",\n  instructions: \"You are a voice assistant that can help users with their tasks.\",\n  model: openai(\"gpt-4o\"),\n  voice: new SpeechifyVoice(),\n});\n\nconst { text } = await voiceAgent.generate('What color is the sky?');\n\n// Convert text to speech to an Audio Stream\nconst audioStream = await voiceAgent.voice.speak(text, {\n  speaker: \"matthew\", // Optional: specify a speaker\n});\n\nplayAudio(audioStream);\n````\n\nVisit the [Speechify Voice Reference](/reference/voice/speechify) for more information on the Speechify voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { SarvamVoice } from \"@mastra/voice-sarvam\";\nimport { playAudio } from \"@mastra/node-audio\";\n\nconst voiceAgent = new Agent({\nname: \"Voice Agent\",\ninstructions: \"You are a voice assistant that can help users with their tasks.\",\nmodel: openai(\"gpt-4o\"),\nvoice: new SarvamVoice(),\n});\n\nconst { text } = await voiceAgent.generate('What color is the sky?');\n\n// Convert text to speech to an Audio Stream\nconst audioStream = await voiceAgent.voice.speak(text, {\nspeaker: \"default\", // Optional: specify a speaker\n});\n\nplayAudio(audioStream);\n\n````\n\nVisit the [Sarvam Voice Reference](/reference/voice/sarvam) for more information on the Sarvam voice provider.\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { MurfVoice } from \"@mastra/voice-murf\";\nimport { playAudio } from \"@mastra/node-audio\";\n\nconst voiceAgent = new Agent({\n  name: \"Voice Agent\",\n  instructions: \"You are a voice assistant that can help users with their tasks.\",\n  model: openai(\"gpt-4o\"),\n  voice: new MurfVoice(),\n});\n\nconst { text } = await voiceAgent.generate('What color is the sky?');\n\n// Convert text to speech to an Audio Stream\nconst audioStream = await voiceAgent.voice.speak(text, {\n  speaker: \"default\", // Optional: specify a speaker\n});\n\nplayAudio(audioStream);\n````\n\nVisit the [Murf Voice Reference](/reference/voice/murf) for more information on the Murf voice provider.\n\n  </Tabs.Tab>\n</Tabs>\n\n### Speech to Text (STT)\n\nTranscribe spoken content using various providers like OpenAI, ElevenLabs, and more. For detailed configuration options and more, check out [Speech to Text](./speech-to-text).\n\nYou can download a sample audio file from [here](https://github.com/mastra-ai/realtime-voice-demo/raw/refs/heads/main/how_can_i_help_you.mp3).\n\n<br />\n<AudioPlayback audio=\"https://github.com/mastra-ai/realtime-voice-demo/raw/refs/heads/main/how_can_i_help_you.mp3\" />\n\n{/*\nLLM CONTEXT: This Tabs component demonstrates Speech-to-Text (STT) implementation across different voice providers.\nEach tab shows how to set up and use a specific STT provider for transcribing audio to text.\nThe tabs help users understand how to implement speech recognition with different providers.\nEach tab includes code examples showing audio file handling, transcription, and response generation.\n*/}\n\n<Tabs items={[\"OpenAI\", \"Azure\", \"ElevenLabs\", \"Google\", \"Cloudflare\", \"Deepgram\", \"Sarvam\"]}>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { OpenAIVoice } from \"@mastra/voice-openai\";\nimport { createReadStream } from 'fs';\n\nconst voiceAgent = new Agent({\nname: \"Voice Agent\",\ninstructions: \"You are a voice assistant that can help users with their tasks.\",\nmodel: openai(\"gpt-4o\"),\nvoice: new OpenAIVoice(),\n});\n\n// Use an audio file from a URL\nconst audioStream = await createReadStream(\"./how_can_i_help_you.mp3\");\n\n// Convert audio to text\nconst transcript = await voiceAgent.voice.listen(audioStream);\nconsole.log(`User said: ${transcript}`);\n\n// Generate a response based on the transcript\nconst { text } = await voiceAgent.generate(transcript);\n\n````\n\nVisit the [OpenAI Voice Reference](/reference/voice/openai) for more information on the OpenAI voice provider.\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { createReadStream } from 'fs';\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { AzureVoice } from \"@mastra/voice-azure\";\nimport { createReadStream } from 'fs';\n\nconst voiceAgent = new Agent({\n  name: \"Voice Agent\",\n  instructions: \"You are a voice assistant that can help users with their tasks.\",\n  model: openai(\"gpt-4o\"),\n  voice: new AzureVoice(),\n});\n\n// Use an audio file from a URL\nconst audioStream = await createReadStream(\"./how_can_i_help_you.mp3\");\n\n// Convert audio to text\nconst transcript = await voiceAgent.voice.listen(audioStream);\nconsole.log(`User said: ${transcript}`);\n\n// Generate a response based on the transcript\nconst { text } = await voiceAgent.generate(transcript);\n````\n\nVisit the [Azure Voice Reference](/reference/voice/azure) for more information on the Azure voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { ElevenLabsVoice } from \"@mastra/voice-elevenlabs\";\nimport { createReadStream } from 'fs';\n\nconst voiceAgent = new Agent({\nname: \"Voice Agent\",\ninstructions: \"You are a voice assistant that can help users with their tasks.\",\nmodel: openai(\"gpt-4o\"),\nvoice: new ElevenLabsVoice(),\n});\n\n// Use an audio file from a URL\nconst audioStream = await createReadStream(\"./how_can_i_help_you.mp3\");\n\n// Convert audio to text\nconst transcript = await voiceAgent.voice.listen(audioStream);\nconsole.log(`User said: ${transcript}`);\n\n// Generate a response based on the transcript\nconst { text } = await voiceAgent.generate(transcript);\n\n````\n\nVisit the [ElevenLabs Voice Reference](/reference/voice/elevenlabs) for more information on the ElevenLabs voice provider.\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { GoogleVoice } from \"@mastra/voice-google\";\nimport { createReadStream } from 'fs';\n\nconst voiceAgent = new Agent({\n  name: \"Voice Agent\",\n  instructions: \"You are a voice assistant that can help users with their tasks.\",\n  model: openai(\"gpt-4o\"),\n  voice: new GoogleVoice(),\n});\n\n// Use an audio file from a URL\nconst audioStream = await createReadStream(\"./how_can_i_help_you.mp3\");\n\n// Convert audio to text\nconst transcript = await voiceAgent.voice.listen(audioStream);\nconsole.log(`User said: ${transcript}`);\n\n// Generate a response based on the transcript\nconst { text } = await voiceAgent.generate(transcript);\n````\n\nVisit the [Google Voice Reference](/reference/voice/google) for more information on the Google voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { CloudflareVoice } from \"@mastra/voice-cloudflare\";\nimport { createReadStream } from 'fs';\n\nconst voiceAgent = new Agent({\nname: \"Voice Agent\",\ninstructions: \"You are a voice assistant that can help users with their tasks.\",\nmodel: openai(\"gpt-4o\"),\nvoice: new CloudflareVoice(),\n});\n\n// Use an audio file from a URL\nconst audioStream = await createReadStream(\"./how_can_i_help_you.mp3\");\n\n// Convert audio to text\nconst transcript = await voiceAgent.voice.listen(audioStream);\nconsole.log(`User said: ${transcript}`);\n\n// Generate a response based on the transcript\nconst { text } = await voiceAgent.generate(transcript);\n\n````\n\nVisit the [Cloudflare Voice Reference](/reference/voice/cloudflare) for more information on the Cloudflare voice provider.\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { DeepgramVoice } from \"@mastra/voice-deepgram\";\nimport { createReadStream } from 'fs';\n\nconst voiceAgent = new Agent({\n  name: \"Voice Agent\",\n  instructions: \"You are a voice assistant that can help users with their tasks.\",\n  model: openai(\"gpt-4o\"),\n  voice: new DeepgramVoice(),\n});\n\n// Use an audio file from a URL\nconst audioStream = await createReadStream(\"./how_can_i_help_you.mp3\");\n\n// Convert audio to text\nconst transcript = await voiceAgent.voice.listen(audioStream);\nconsole.log(`User said: ${transcript}`);\n\n// Generate a response based on the transcript\nconst { text } = await voiceAgent.generate(transcript);\n````\n\nVisit the [Deepgram Voice Reference](/reference/voice/deepgram) for more information on the Deepgram voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { SarvamVoice } from \"@mastra/voice-sarvam\";\nimport { createReadStream } from 'fs';\n\nconst voiceAgent = new Agent({\nname: \"Voice Agent\",\ninstructions: \"You are a voice assistant that can help users with their tasks.\",\nmodel: openai(\"gpt-4o\"),\nvoice: new SarvamVoice(),\n});\n\n// Use an audio file from a URL\nconst audioStream = await createReadStream(\"./how_can_i_help_you.mp3\");\n\n// Convert audio to text\nconst transcript = await voiceAgent.voice.listen(audioStream);\nconsole.log(`User said: ${transcript}`);\n\n// Generate a response based on the transcript\nconst { text } = await voiceAgent.generate(transcript);\n\n````\n\nVisit the [Sarvam Voice Reference](/reference/voice/sarvam) for more information on the Sarvam voice provider.\n  </Tabs.Tab>\n</Tabs>\n\n### Speech to Speech (STS)\n\nCreate conversational experiences with speech-to-speech capabilities. The unified API enables real-time voice interactions between users and AI agents.\nFor detailed configuration options and advanced features, check out [Speech to Speech](./speech-to-speech).\n\n{/*\n  LLM CONTEXT: This Tabs component demonstrates Speech-to-Speech (STS) implementation for real-time voice interactions.\n  Currently only shows OpenAI's realtime voice implementation for bidirectional voice conversations.\n  The tab shows how to set up real-time voice communication with event handling for audio responses.\n  This enables conversational AI experiences with continuous audio streaming.\n*/}\n<Tabs items={[\"OpenAI\", \"Google Gemini Live\"]}>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { playAudio, getMicrophoneStream } from '@mastra/node-audio';\nimport { OpenAIRealtimeVoice } from \"@mastra/voice-openai-realtime\";\n\nconst voiceAgent = new Agent({\n  name: \"Voice Agent\",\n  instructions: \"You are a voice assistant that can help users with their tasks.\",\n  model: openai(\"gpt-4o\"),\n  voice: new OpenAIRealtimeVoice(),\n});\n\n// Listen for agent audio responses\nvoiceAgent.voice.on('speaker', ({ audio }) => {\n  playAudio(audio);\n});\n\n// Initiate the conversation\nawait voiceAgent.voice.speak('How can I help you today?');\n\n// Send continuous audio from the microphone\nconst micStream = getMicrophoneStream();\nawait voiceAgent.voice.send(micStream);\n````\n\nVisit the [OpenAI Voice Reference](/reference/voice/openai-realtime) for more information on the OpenAI voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\nimport { Agent } from '@mastra/core/agent';\nimport { openai } from '@ai-sdk/openai';\nimport { playAudio, getMicrophoneStream } from '@mastra/node-audio';\nimport { GeminiLiveVoice } from \"@mastra/voice-google-gemini-live\";\n\nconst voiceAgent = new Agent({\n  name: \"Voice Agent\",\n  instructions: \"You are a voice assistant that can help users with their tasks.\",\n  model: openai(\"gpt-4o\"),\n  voice: new GeminiLiveVoice({\n    // Live API mode\n    apiKey: process.env.GOOGLE_API_KEY,\n    model: 'gemini-2.0-flash-exp',\n    speaker: 'Puck',\n    debug: true,\n    // Vertex AI alternative:\n    // vertexAI: true,\n    // project: 'your-gcp-project',\n    // location: 'us-central1',\n    // serviceAccountKeyFile: '/path/to/service-account.json',\n  }),\n});\n\n// Connect before using speak/send\nawait voiceAgent.voice.connect();\n\n// Listen for agent audio responses\nvoiceAgent.voice.on('speaker', ({ audio }) => {\n  playAudio(audio);\n});\n\n// Listen for text responses and transcriptions\nvoiceAgent.voice.on('writing', ({ text, role }) => {\n  console.log(`${role}: ${text}`);\n});\n\n// Initiate the conversation\nawait voiceAgent.voice.speak('How can I help you today?');\n\n// Send continuous audio from the microphone\nconst micStream = getMicrophoneStream();\nawait voiceAgent.voice.send(micStream);\n```\n\nVisit the [Google Gemini Live Reference](/reference/voice/google-gemini-live) for more information on the Google Gemini Live voice provider.\n\n  </Tabs.Tab>\n</Tabs>\n\n## Voice Configuration\n\nEach voice provider can be configured with different models and options. Below are the detailed configuration options for all supported providers:\n\n{/*\nLLM CONTEXT: This Tabs component shows detailed configuration options for all supported voice providers.\nEach tab demonstrates how to configure a specific voice provider with all available options and settings.\nThe tabs help users understand the full configuration capabilities of each provider including models, languages, and advanced settings.\nEach tab shows both speech and listening model configurations where applicable.\n*/}\n\n<Tabs items={[\"OpenAI\", \"Azure\", \"ElevenLabs\", \"PlayAI\", \"Google\", \"Cloudflare\", \"Deepgram\", \"Speechify\", \"Sarvam\", \"Murf\", \"OpenAI Realtime\", \"Google Gemini Live\"]}>\n  <Tabs.Tab>\n```typescript\n// OpenAI Voice Configuration\nconst voice = new OpenAIVoice({\n  speechModel: {\n    name: \"gpt-3.5-turbo\", // Example model name\n    apiKey: process.env.OPENAI_API_KEY,\n    language: \"en-US\", // Language code\n    voiceType: \"neural\", // Type of voice model\n  },\n  listeningModel: {\n    name: \"whisper-1\", // Example model name\n    apiKey: process.env.OPENAI_API_KEY,\n    language: \"en-US\", // Language code\n    format: \"wav\", // Audio format\n  },\n  speaker: \"alloy\", // Example speaker name\n});\n```\n\nVisit the [OpenAI Voice Reference](/reference/voice/openai) for more information on the OpenAI voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\n// Azure Voice Configuration\nconst voice = new AzureVoice({\n  speechModel: {\n    name: \"en-US-JennyNeural\", // Example model name\n    apiKey: process.env.AZURE_SPEECH_KEY,\n    region: process.env.AZURE_SPEECH_REGION,\n    language: \"en-US\", // Language code\n    style: \"cheerful\", // Voice style\n    pitch: \"+0Hz\", // Pitch adjustment\n    rate: \"1.0\", // Speech rate\n  },\n  listeningModel: {\n    name: \"en-US\", // Example model name\n    apiKey: process.env.AZURE_SPEECH_KEY,\n    region: process.env.AZURE_SPEECH_REGION,\n    format: \"simple\", // Output format\n  },\n});\n```\n\nVisit the [Azure Voice Reference](/reference/voice/azure) for more information on the Azure voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\n// ElevenLabs Voice Configuration\nconst voice = new ElevenLabsVoice({\n  speechModel: {\n    voiceId: \"your-voice-id\", // Example voice ID\n    model: \"eleven_multilingual_v2\", // Example model name\n    apiKey: process.env.ELEVENLABS_API_KEY,\n    language: \"en\", // Language code\n    emotion: \"neutral\", // Emotion setting\n  },\n  // ElevenLabs may not have a separate listening model\n});\n```\n\nVisit the [ElevenLabs Voice Reference](/reference/voice/elevenlabs) for more information on the ElevenLabs voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\n// PlayAI Voice Configuration\nconst voice = new PlayAIVoice({\n  speechModel: {\n    name: \"playai-voice\", // Example model name\n    speaker: \"emma\", // Example speaker name\n    apiKey: process.env.PLAYAI_API_KEY,\n    language: \"en-US\", // Language code\n    speed: 1.0, // Speech speed\n  },\n  // PlayAI may not have a separate listening model\n});\n```\n\nVisit the [PlayAI Voice Reference](/reference/voice/playai) for more information on the PlayAI voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\n// Google Voice Configuration\nconst voice = new GoogleVoice({\n  speechModel: {\n    name: \"en-US-Studio-O\", // Example model name\n    apiKey: process.env.GOOGLE_API_KEY,\n    languageCode: \"en-US\", // Language code\n    gender: \"FEMALE\", // Voice gender\n    speakingRate: 1.0, // Speaking rate\n  },\n  listeningModel: {\n    name: \"en-US\", // Example model name\n    sampleRateHertz: 16000, // Sample rate\n  },\n});\n```\n\nVisit the [Google Voice Reference](/reference/voice/google) for more information on the Google voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\n// Cloudflare Voice Configuration\nconst voice = new CloudflareVoice({\n  speechModel: {\n    name: \"cloudflare-voice\", // Example model name\n    accountId: process.env.CLOUDFLARE_ACCOUNT_ID,\n    apiToken: process.env.CLOUDFLARE_API_TOKEN,\n    language: \"en-US\", // Language code\n    format: \"mp3\", // Audio format\n  },\n  // Cloudflare may not have a separate listening model\n});\n```\n\nVisit the [Cloudflare Voice Reference](/reference/voice/cloudflare) for more information on the Cloudflare voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\n// Deepgram Voice Configuration\nconst voice = new DeepgramVoice({\n  speechModel: {\n    name: \"nova-2\", // Example model name\n    speaker: \"aura-english-us\", // Example speaker name\n    apiKey: process.env.DEEPGRAM_API_KEY,\n    language: \"en-US\", // Language code\n    tone: \"formal\", // Tone setting\n  },\n  listeningModel: {\n    name: \"nova-2\", // Example model name\n    format: \"flac\", // Audio format\n  },\n});\n```\n\nVisit the [Deepgram Voice Reference](/reference/voice/deepgram) for more information on the Deepgram voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\n// Speechify Voice Configuration\nconst voice = new SpeechifyVoice({\n  speechModel: {\n    name: \"speechify-voice\", // Example model name\n    speaker: \"matthew\", // Example speaker name\n    apiKey: process.env.SPEECHIFY_API_KEY,\n    language: \"en-US\", // Language code\n    speed: 1.0, // Speech speed\n  },\n  // Speechify may not have a separate listening model\n});\n```\n\nVisit the [Speechify Voice Reference](/reference/voice/speechify) for more information on the Speechify voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\n// Sarvam Voice Configuration\nconst voice = new SarvamVoice({\n  speechModel: {\n    name: \"sarvam-voice\", // Example model name\n    apiKey: process.env.SARVAM_API_KEY,\n    language: \"en-IN\", // Language code\n    style: \"conversational\", // Style setting\n  },\n  // Sarvam may not have a separate listening model\n});\n```\n\nVisit the [Sarvam Voice Reference](/reference/voice/sarvam) for more information on the Sarvam voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\n// Murf Voice Configuration\nconst voice = new MurfVoice({\n  speechModel: {\n    name: \"murf-voice\", // Example model name\n    apiKey: process.env.MURF_API_KEY,\n    language: \"en-US\", // Language code\n    emotion: \"happy\", // Emotion setting\n  },\n  // Murf may not have a separate listening model\n});\n```\n\nVisit the [Murf Voice Reference](/reference/voice/murf) for more information on the Murf voice provider.\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\n// OpenAI Realtime Voice Configuration\nconst voice = new OpenAIRealtimeVoice({\n  speechModel: {\n    name: \"gpt-3.5-turbo\", // Example model name\n    apiKey: process.env.OPENAI_API_KEY,\n    language: \"en-US\", // Language code\n  },\n  listeningModel: {\n    name: \"whisper-1\", // Example model name\n    apiKey: process.env.OPENAI_API_KEY,\n    format: \"ogg\", // Audio format\n  },\n  speaker: \"alloy\", // Example speaker name\n});\n```\n\nFor more information on the OpenAI Realtime voice provider, refer to the [OpenAI Realtime Voice Reference](/reference/voice/openai-realtime).\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n```typescript\n// Google Gemini Live Voice Configuration\nconst voice = new GeminiLiveVoice({\n  speechModel: {\n    name: \"gemini-2.0-flash-exp\", // Example model name\n    apiKey: process.env.GOOGLE_API_KEY,\n  },\n  speaker: \"Puck\", // Example speaker name\n  // Google Gemini Live is a realtime bidirectional API without separate speech and listening models\n});\n```\n\nVisit the [Google Gemini Live Reference](/reference/voice/google-gemini-live) for more information on the Google Gemini Live voice provider.\n\n  </Tabs.Tab>\n</Tabs>\n\n### Using Multiple Voice Providers\n\nThis example demonstrates how to create and use two different voice providers in Mastra: OpenAI for speech-to-text (STT) and PlayAI for text-to-speech (TTS).\n\nStart by creating instances of the voice providers with any necessary configuration.\n\n```typescript\nimport { OpenAIVoice } from \"@mastra/voice-openai\";\nimport { PlayAIVoice } from \"@mastra/voice-playai\";\nimport { CompositeVoice } from \"@mastra/core/voice\";\nimport { playAudio, getMicrophoneStream } from \"@mastra/node-audio\";\n\n// Initialize OpenAI voice for STT\nconst input = new OpenAIVoice({\n  listeningModel: {\n    name: \"whisper-1\",\n    apiKey: process.env.OPENAI_API_KEY,\n  },\n});\n\n// Initialize PlayAI voice for TTS\nconst output = new PlayAIVoice({\n  speechModel: {\n    name: \"playai-voice\",\n    apiKey: process.env.PLAYAI_API_KEY,\n  },\n});\n\n// Combine the providers using CompositeVoice\nconst voice = new CompositeVoice({\n  input,\n  output,\n});\n\n// Implement voice interactions using the combined voice provider\nconst audioStream = getMicrophoneStream(); // Assume this function gets audio input\nconst transcript = await voice.listen(audioStream);\n\n// Log the transcribed text\nconsole.log(\"Transcribed text:\", transcript);\n\n// Convert text to speech\nconst responseAudio = await voice.speak(`You said: ${transcript}`, {\n  speaker: \"default\", // Optional: specify a speaker,\n  responseFormat: \"wav\", // Optional: specify a response format\n});\n\n// Play the audio response\nplayAudio(responseAudio);\n```\n\nFor more information on the CompositeVoice, refer to the [CompositeVoice Reference](/reference/voice/composite-voice).\n\n## More Resources\n\n- [CompositeVoice](../../reference/voice/composite-voice.mdx)\n- [MastraVoice](../../reference/voice/mastra-voice.mdx)\n- [OpenAI Voice](../../reference/voice/openai.mdx)\n- [OpenAI Realtime Voice](../../reference/voice/openai-realtime.mdx)\n- [Azure Voice](../../reference/voice/azure.mdx)\n- [Google Voice](../../reference/voice/google.mdx)\n- [Google Gemini Live Voice](../../reference/voice/google-gemini-live.mdx)\n- [Deepgram Voice](../../reference/voice/deepgram.mdx)\n- [PlayAI Voice](../../reference/voice/playai.mdx)\n- [Voice Examples](../../examples/voice/text-to-speech.mdx)\n\n\n---\ntitle: Speech-to-Speech Capabilities in Mastra | Mastra Docs\ndescription: Overview of speech-to-speech capabilities in Mastra, including real-time interactions and event-driven architecture.\n---\n\n# Speech-to-Speech Capabilities in Mastra\n[EN] Source: https://mastra.ai/en/docs/voice/speech-to-speech\n\n## Introduction\n\nSpeech-to-Speech (STS) in Mastra provides a standardized interface for real-time interactions across multiple providers.  \nSTS enables continuous bidirectional audio communication through listening to events from Realtime models. Unlike separate TTS and STT operations, STS maintains an open connection that processes speech continuously in both directions.\n\n## Configuration\n\n- **`apiKey`**: Your OpenAI API key. Falls back to the `OPENAI_API_KEY` environment variable.\n- **`model`**: The model ID to use for real-time voice interactions (e.g., `gpt-4o-mini-realtime`).\n- **`speaker`**: The default voice ID for speech synthesis. This allows you to specify which voice to use for the speech output.\n\n```typescript\nconst voice = new OpenAIRealtimeVoice({\n  apiKey: \"your-openai-api-key\",\n  model: \"gpt-4o-mini-realtime\",\n  speaker: \"alloy\", // Default voice\n});\n\n// If using default settings the configuration can be simplified to:\nconst voice = new OpenAIRealtimeVoice();\n```\n\n## Using STS\n\n```typescript\nimport { Agent } from \"@mastra/core/agent\";\nimport { OpenAIRealtimeVoice } from \"@mastra/voice-openai-realtime\";\nimport { playAudio, getMicrophoneStream } from \"@mastra/node-audio\";\n\nconst agent = new Agent({\n  name: \"Agent\",\n  instructions: `You are a helpful assistant with real-time voice capabilities.`,\n  model: openai(\"gpt-4o\"),\n  voice: new OpenAIRealtimeVoice(),\n});\n\n// Connect to the voice service\nawait agent.voice.connect();\n\n// Listen for agent audio responses\nagent.voice.on(\"speaker\", ({ audio }) => {\n  playAudio(audio);\n});\n\n// Initiate the conversation\nawait agent.voice.speak(\"How can I help you today?\");\n\n// Send continuous audio from the microphone\nconst micStream = getMicrophoneStream();\nawait agent.voice.send(micStream);\n```\n\nFor integrating Speech-to-Speech capabilities with agents, refer to the [Adding Voice to Agents](../agents/adding-voice.mdx) documentation.\n\n## Google Gemini Live (Realtime)\n\n```typescript\nimport { Agent } from \"@mastra/core/agent\";\nimport { GeminiLiveVoice } from \"@mastra/voice-google-gemini-live\";\nimport { playAudio, getMicrophoneStream } from \"@mastra/node-audio\";\n\nconst agent = new Agent({\n  name: 'Agent',\n  instructions: 'You are a helpful assistant with real-time voice capabilities.',\n  // Model used for text generation; voice provider handles realtime audio\n  model: openai(\"gpt-4o\"),\n  voice: new GeminiLiveVoice({\n    apiKey: process.env.GOOGLE_API_KEY,\n    model: 'gemini-2.0-flash-exp',\n    speaker: 'Puck',\n    debug: true,\n    // Vertex AI option:\n    // vertexAI: true, \n    // project: 'your-gcp-project', \n    // location: 'us-central1',\n    // serviceAccountKeyFile: '/path/to/service-account.json',\n  }),\n});\n\nawait agent.voice.connect();\n\nagent.voice.on('speaker', ({ audio }) => {\n  playAudio(audio);\n});\n\nagent.voice.on('writing', ({ role, text }) => {\n  console.log(`${role}: ${text}`);\n});\n\nawait agent.voice.speak('How can I help you today?');\n\nconst micStream = getMicrophoneStream();\nawait agent.voice.send(micStream);\n```\n\nNote:\n- Live API requires `GOOGLE_API_KEY`. Vertex AI requires project/location and service account credentials.\n- Events: `speaker` (audio stream), `writing` (text), `turnComplete`, `usage`, and `error`.\n\n\n---\ntitle: Speech-to-Text (STT) in Mastra | Mastra Docs\ndescription: Overview of Speech-to-Text capabilities in Mastra, including configuration, usage, and integration with voice providers.\n---\n\n# Speech-to-Text (STT)\n[EN] Source: https://mastra.ai/en/docs/voice/speech-to-text\n\nSpeech-to-Text (STT) in Mastra provides a standardized interface for converting audio input into text across multiple service providers.\nSTT helps create voice-enabled applications that can respond to human speech, enabling hands-free interaction, accessibility for users with disabilities, and more natural human-computer interfaces.\n\n## Configuration\n\nTo use STT in Mastra, you need to provide a `listeningModel` when initializing the voice provider. This includes parameters such as:\n\n- **`name`**: The specific STT model to use.\n- **`apiKey`**: Your API key for authentication.\n- **Provider-specific options**: Additional options that may be required or supported by the specific voice provider.\n\n**Note**: All of these parameters are optional. You can use the default settings provided by the voice provider, which will depend on the specific provider you are using.\n\n```typescript\nconst voice = new OpenAIVoice({\n  listeningModel: {\n    name: \"whisper-1\",\n    apiKey: process.env.OPENAI_API_KEY,\n  },\n});\n\n// If using default settings the configuration can be simplified to:\nconst voice = new OpenAIVoice();\n```\n\n## Available Providers\n\nMastra supports several Speech-to-Text providers, each with their own capabilities and strengths:\n\n- [**OpenAI**](/reference/voice/openai/) - High-accuracy transcription with Whisper models\n- [**Azure**](/reference/voice/azure/) - Microsoft's speech recognition with enterprise-grade reliability\n- [**ElevenLabs**](/reference/voice/elevenlabs/) - Advanced speech recognition with support for multiple languages\n- [**Google**](/reference/voice/google/) - Google's speech recognition with extensive language support\n- [**Cloudflare**](/reference/voice/cloudflare/) - Edge-optimized speech recognition for low-latency applications\n- [**Deepgram**](/reference/voice/deepgram/) - AI-powered speech recognition with high accuracy for various accents\n- [**Sarvam**](/reference/voice/sarvam/) - Specialized in Indic languages and accents\n\nEach provider is implemented as a separate package that you can install as needed:\n\n```bash\npnpm add @mastra/voice-openai  # Example for OpenAI\n```\n\n## Using the Listen Method\n\nThe primary method for STT is the `listen()` method, which converts spoken audio into text. Here's how to use it:\n\n```typescript\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { OpenAIVoice } from \"@mastra/voice-openai\";\nimport { getMicrophoneStream } from \"@mastra/node-audio\";\n\nconst voice = new OpenAIVoice();\n\nconst agent = new Agent({\n  name: \"Voice Agent\",\n  instructions:\n    \"You are a voice assistant that provides recommendations based on user input.\",\n  model: openai(\"gpt-4o\"),\n  voice,\n});\n\nconst audioStream = getMicrophoneStream(); // Assume this function gets audio input\n\nconst transcript = await agent.voice.listen(audioStream, {\n  filetype: \"m4a\", // Optional: specify the audio file type\n});\n\nconsole.log(`User said: ${transcript}`);\n\nconst { text } = await agent.generate(\n  `Based on what the user said, provide them a recommendation: ${transcript}`,\n);\n\nconsole.log(`Recommendation: ${text}`);\n```\n\nCheck out the [Adding Voice to Agents](../agents/adding-voice.mdx) documentation to learn how to use STT in an agent.\n\n\n---\ntitle: Text-to-Speech (TTS) in Mastra | Mastra Docs\ndescription: Overview of Text-to-Speech capabilities in Mastra, including configuration, usage, and integration with voice providers.\n---\n\n# Text-to-Speech (TTS)\n[EN] Source: https://mastra.ai/en/docs/voice/text-to-speech\n\nText-to-Speech (TTS) in Mastra offers a unified API for synthesizing spoken audio from text using various providers.\nBy incorporating TTS into your applications, you can enhance user experience with natural voice interactions, improve accessibility for users with visual impairments, and create more engaging multimodal interfaces.\n\nTTS is a core component of any voice application. Combined with STT (Speech-to-Text), it forms the foundation of voice interaction systems. Newer models support STS ([Speech-to-Speech](./speech-to-speech)) which can be used for real-time interactions but come at high cost ($).\n\n## Configuration\n\nTo use TTS in Mastra, you need to provide a `speechModel` when initializing the voice provider. This includes parameters such as:\n\n- **`name`**: The specific TTS model to use.\n- **`apiKey`**: Your API key for authentication.\n- **Provider-specific options**: Additional options that may be required or supported by the specific voice provider.\n\nThe **`speaker`** option allows you to select different voices for speech synthesis. Each provider offers a variety of voice options with distinct characteristics for **Voice diversity**, **Quality**, **Voice personality**, and **Multilingual support**\n\n**Note**: All of these parameters are optional. You can use the default settings provided by the voice provider, which will depend on the specific provider you are using.\n\n```typescript\nconst voice = new OpenAIVoice({\n  speechModel: {\n    name: \"tts-1-hd\",\n    apiKey: process.env.OPENAI_API_KEY,\n  },\n  speaker: \"alloy\",\n});\n\n// If using default settings the configuration can be simplified to:\nconst voice = new OpenAIVoice();\n```\n\n## Available Providers\n\nMastra supports a wide range of Text-to-Speech providers, each with their own unique capabilities and voice options. You can choose the provider that best suits your application's needs:\n\n- [**OpenAI**](/reference/voice/openai/) - High-quality voices with natural intonation and expression\n- [**Azure**](/reference/voice/azure/) - Microsoft's speech service with a wide range of voices and languages\n- [**ElevenLabs**](/reference/voice/elevenlabs/) - Ultra-realistic voices with emotion and fine-grained control\n- [**PlayAI**](/reference/voice/playai/) - Specialized in natural-sounding voices with various styles\n- [**Google**](/reference/voice/google/) - Google's speech synthesis with multilingual support\n- [**Cloudflare**](/reference/voice/cloudflare/) - Edge-optimized speech synthesis for low-latency applications\n- [**Deepgram**](/reference/voice/deepgram/) - AI-powered speech technology with high accuracy\n- [**Speechify**](/reference/voice/speechify/) - Text-to-speech optimized for readability and accessibility\n- [**Sarvam**](/reference/voice/sarvam/) - Specialized in Indic languages and accents\n- [**Murf**](/reference/voice/murf/) - Studio-quality voice overs with customizable parameters\n\nEach provider is implemented as a separate package that you can install as needed:\n\n```bash\npnpm add @mastra/voice-openai  # Example for OpenAI\n```\n\n## Using the Speak Method\n\nThe primary method for TTS is the `speak()` method, which converts text to speech. This method can accept options that allows you to specify the speaker and other provider-specific options. Here's how to use it:\n\n```typescript\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { OpenAIVoice } from \"@mastra/voice-openai\";\n\nconst voice = new OpenAIVoice();\n\nconst agent = new Agent({\n  name: \"Voice Agent\",\n  instructions:\n    \"You are a voice assistant that can help users with their tasks.\",\n  model: openai(\"gpt-4o\"),\n  voice,\n});\n\nconst { text } = await agent.generate(\"What color is the sky?\");\n\n// Convert text to speech to an Audio Stream\nconst readableStream = await voice.speak(text, {\n  speaker: \"default\", // Optional: specify a speaker\n  properties: {\n    speed: 1.0, // Optional: adjust speech speed\n    pitch: \"default\", // Optional: specify pitch if supported\n  },\n});\n```\n\nCheck out the [Adding Voice to Agents](../agents/adding-voice.mdx) documentation to learn how to use TTS in an agent.\n\n\n","path":null,"size_bytes":45571,"size_tokens":null},"docs/mastra/06-reference/115_run-observe-stream.md":{"content":"---\ntitle: \"Reference: Run.observeStream() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Run.observeStream()` method in workflows, which enables reopening the stream of an already active workflow run.\n---\n\n# Run.observeStream()\n[EN] Source: https://mastra.ai/en/reference/streaming/workflows/observeStream\n\nThe `.observeStream()` method opens a new `ReadableStream` to a workflow run that is currently running, allowing you to observe the stream of events if the original stream is no longer available.\n\n## Usage example\n\n```typescript showLineNumbers copy\nconst run = await workflow.createRunAsync();\n\nrun.stream({\n  inputData: {\n    value: \"initial data\",\n  },\n});\n\nconst { stream } = await run.observeStream();\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n```\n\n## Returns\n\n`ReadableStream<ChunkType>`\n\n## Stream Events\n\nThe stream emits various event types during workflow execution. Each event has a `type` field and a `payload` containing relevant data:\n\n- **`start`**: Workflow execution begins\n- **`step-start`**: A step begins execution\n- **`tool-call`**: A tool call is initiated\n- **`tool-call-streaming-start`**: Tool call streaming begins\n- **`tool-call-delta`**: Incremental tool output updates\n- **`step-result`**: A step completes with results\n- **`step-finish`**: A step finishes execution\n- **`finish`**: Workflow execution completes\n\n## Related\n\n- [Workflows overview](../../../docs/workflows/overview.mdx#run-workflow)\n- [Workflow.createRunAsync()](../../../reference/workflows/workflow-methods/create-run.mdx)\n- [Run.stream()](./stream.mdx)\n\n\n","path":null,"size_bytes":1596,"size_tokens":null},"docs/mastra/06-reference/31_mastra-get-agent-by-id.md":{"content":"---\ntitle: \"Reference: Mastra.getAgentById() | Core | Mastra Docs\"\ndescription: \"Documentation for the `Mastra.getAgentById()` method in Mastra, which retrieves an agent by its ID.\"\n---\n\n# Mastra.getAgentById()\n[EN] Source: https://mastra.ai/en/reference/core/getAgentById\n\nThe `.getAgentById()` method is used to retrieve an agent by its ID. The method accepts a single `string` parameter for the agent's ID.\n\n## Usage example\n\n```typescript copy\nmastra.getAgentById(\"test-agent-123\");\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"id\",\n      type: \"string\",\n      description: \"The ID of the agent to retrieve. The method will first search for an agent with this ID, and if not found, will attempt to use it as a name to call getAgent().\",\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"agent\",\n      type: \"Agent\",\n      description: \"The agent instance with the specified ID. Throws an error if the agent is not found.\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Agents overview](../../docs/agents/overview.mdx)\n- [Dynamic agents](../../docs/agents/dynamic-agents.mdx)\n\n\n","path":null,"size_bytes":1117,"size_tokens":null},"scripts/inngest.sh":{"content":"#!/usr/bin/env bash\n\nset -e\n\nINNGEST_CONFIG=\".config/inngest/inngest.yaml\"\n\n# Try to store Inngest data in Postgres if it's available. Otherwise, put it in SQLite.\nif [[ ! -f  \"${INNGEST_CONFIG}\" ]]; then\n    mkdir -p \"$(dirname \"${INNGEST_CONFIG}\")\"\n    if [[ -z \"${DATABASE_URL}\" ]]; then\n        printf 'postgres-uri: \"%s\"' \"${DATABASE_URL}\" > \"${INNGEST_CONFIG}\"\n    else\n        printf 'sqlite-dir: \"/home/runner/workspace/.local/share/inngest\"' > \"${INNGEST_CONFIG}\"\n    fi\nfi\nexec inngest-cli dev -u http://0.0.0.0:5000/api/inngest --host 0.0.0.0 --port 3000 --config \"${INNGEST_CONFIG}\"\n","path":null,"size_bytes":595,"size_tokens":null},"docs/mastra/03-workflows/05_input-data-mapping.md":{"content":"---\ntitle: \"Input Data Mapping with Workflow | Mastra Docs\"\ndescription: \"Learn how to use workflow input mapping to create more dynamic data flows in your Mastra workflows.\"\n---\n\n# Input Data Mapping\n[EN] Source: https://mastra.ai/en/docs/workflows/input-data-mapping\n\nInput data mapping allows explicit mapping of values for the inputs of the next step. These values can come from a number of sources:\n\n- The outputs of a previous step\n- The runtime context\n- A constant value\n- The initial input of the workflow\n\n## Mapping with `.map()`\n\nIn this example the `output` from `step1` is transformed to match the `inputSchema` required for the `step2`. The value from `step1` is available using the `inputData` parameter of the `.map` function.\n\n![Mapping with .map()](/image/workflows/workflows-data-mapping-map.jpg)\n\n```typescript {9} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nconst step1 = createStep({...});\nconst step2 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .map(async ({ inputData }) => {\n    const { value } = inputData;\n    return {\n      output: `new ${value}`\n    };\n  })\n  .then(step2)\n  .commit();\n```\n\n## Using `inputData`\n\nUse `inputData` to access the full output of the previous step:\n\n```typescript {3} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\n  .then(step1)\n  .map(({ inputData }) => {\n    console.log(inputData);\n  })\n```\n\n## Using `getStepResult()`\n\nUse `getStepResult` to access the full output of a specific step by referencing the step's instance:\n\n```typescript {3} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\n  .then(step1)\n  .map(async ({ getStepResult }) => {\n    console.log(getStepResult(step1));\n  })\n```\n\n## Using `getInitData()`\n\nUse `getInitData` to access the initial input data provided to the workflow:\n\n```typescript {3} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\n  .then(step1)\n  .map(async ({ getInitData }) => {\n      console.log(getInitData());\n  })\n```\n\n## Using `mapVariable()`\n\nTo use `mapVariable` import the necessary function from the workflows module:\n\n```typescript filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { mapVariable } from \"@mastra/core/workflows\";\n```\n\n### Renaming step with `mapVariable()`\n\nYou can rename step outputs using the object syntax in `.map()`. In the example below, the `value` output from `step1` is renamed to `details`:\n\n```typescript {3-6} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\n  .then(step1)\n  .map({\n    details: mapVariable({\n      step: step,\n      path: \"value\"\n    })\n  })\n```\n\n### Renaming workflows with `mapVariable()`\n\nYou can rename workflow outputs by using **referential composition**. This involves passing the workflow instance as the `initData`.\n\n```typescript {6-9} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nexport const testWorkflow = createWorkflow({...});\n\ntestWorkflow\n  .then(step1)\n  .map({\n    details: mapVariable({\n      initData: testWorkflow,\n      path: \"value\"\n    })\n  })\n```\n\n\n","path":null,"size_bytes":3137,"size_tokens":null},"docs/mastra/06-reference/92_workflow-commit.md":{"content":"---\ntitle: \"Reference: Workflow.commit() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow.commit()` method in workflows, which finalizes the workflow and returns the final result.\n---\n\n# Workflow.commit()\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow-methods/commit\n\nThe `.commit()` method finalizes the workflow and returns the final result.\n\n## Usage example\n\n```typescript copy\nworkflow.then(step1).commit();\n```\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"workflow\",\n      type: \"Workflow\",\n      description: \"The workflow instance for method chaining\",\n    },\n  ]}\n/>\n\n## Related\n\n- [Control Flow](../../../docs/workflows/control-flow.mdx)\n\n\n","path":null,"size_bytes":706,"size_tokens":null},"docs/mastra/06-reference/93_workflow-create-run-async.md":{"content":"---\ntitle: \"Reference: Workflow.createRunAsync() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Workflow.createRunAsync()` method in workflows, which creates a new workflow run instance.\n---\n\nimport { Callout } from \"nextra/components\";\n\n# Workflow.createRunAsync()\n[EN] Source: https://mastra.ai/en/reference/workflows/workflow-methods/create-run\n\nThe `.createRunAsync()` method creates a new workflow run instance, allowing you to execute the workflow with specific input data. This is the current API that returns a `Run` instance.\n\n<Callout>\nFor the legacy `createRun()` method that returns an object with methods, see the [Legacy Workflows](../../legacyWorkflows/createRun.mdx) section.\n</Callout>\n\n## Usage example\n\n```typescript copy\nawait workflow.createRunAsync();\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"runId\",\n      type: \"string\",\n      description: \"Optional custom identifier for the workflow run\",\n      isOptional: true,\n    },\n  ]}\n/>\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"run\",\n      type: \"Run\",\n      description:\n        \"A new workflow run instance that can be used to execute the workflow\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript showLineNumbers copy\nconst workflow = mastra.getWorkflow(\"workflow\");\n\nconst run = await workflow.createRunAsync();\n\nconst result = await run.start({\n  inputData: {\n    value: 10,\n  },\n});\n```\n\n## Related\n\n- [Run Class](../run.mdx)\n- [Running workflows](../../../examples/workflows/running-workflows.mdx)\n\n\n","path":null,"size_bytes":1548,"size_tokens":null},"docs/mastra/02-tools/01_advanced-tool-usage.md":{"content":"---\ntitle: \"Advanced Tool Usage | Tools & MCP | Mastra Docs\"\ndescription: This page covers advanced features for Mastra tools, including abort signals and compatibility with the Vercel AI SDK tool format.\n---\n\n# Advanced Tool Usage\n[EN] Source: https://mastra.ai/en/docs/tools-mcp/advanced-usage\n\nThis page covers more advanced techniques and features related to using tools in Mastra.\n\n## Abort Signals\n\nWhen you initiate an agent interaction using `generate()` or `stream()`, you can provide an `AbortSignal`. Mastra automatically forwards this signal to any tool executions that occur during that interaction.\n\nThis allows you to cancel long-running operations within your tools, such as network requests or intensive computations, if the parent agent call is aborted.\n\nYou access the `abortSignal` in the second parameter of the tool's `execute` function.\n\n```typescript\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nexport const longRunningTool = createTool({\n  id: \"long-computation\",\n  description: \"Performs a potentially long computation\",\n  inputSchema: z.object({ /* ... */ }),\n  execute: async ({ context }, { abortSignal }) => {\n    // Example: Forwarding signal to fetch\n    const response = await fetch(\"https://api.example.com/data\", {\n      signal: abortSignal, // Pass the signal here\n    });\n\n    if (abortSignal?.aborted) {\n      console.log(\"Tool execution aborted.\");\n      throw new Error(\"Aborted\");\n    }\n\n    // Example: Checking signal during a loop\n    for (let i = 0; i < 1000000; i++) {\n      if (abortSignal?.aborted) {\n        console.log(\"Tool execution aborted during loop.\");\n        throw new Error(\"Aborted\");\n      }\n      // ... perform computation step ...\n    }\n\n    const data = await response.json();\n    return { result: data };\n  },\\n});\n```\n\nTo use this, provide an `AbortController`'s signal when calling the agent:\n\n```typescript\nimport { Agent } from \"@mastra/core/agent\";\n// Assume 'agent' is an Agent instance with longRunningTool configured\n\nconst controller = new AbortController();\n\n// Start the agent call\nconst promise = agent.generate(\"Perform the long computation.\", {\n  abortSignal: controller.signal,\n});\n\n// Sometime later, if needed:\n// controller.abort();\n\ntry {\n  const result = await promise;\n  console.log(result.text);\n} catch (error) {\n  if (error.name === \"AbortError\") {\n    console.log(\"Agent generation was aborted.\");\n  } else {\n    console.error(\"An error occurred:\", error);\n  }\n}\n```\n\n## AI SDK Tool Format\n\nMastra maintains compatibility with the tool format used by the Vercel AI SDK (`ai` package). You can define tools using the `tool` function from the `ai` package and use them directly within your Mastra agents alongside tools created with Mastra's `createTool`.\n\nFirst, ensure you have the `ai` package installed:\n\n```bash npm2yarn copy\nnpm install ai\n```\n\nHere's an example of a tool defined using the Vercel AI SDK format:\n\n```typescript filename=\"src/mastra/tools/vercelWeatherTool.ts\" copy\nimport { tool } from \"ai\";\nimport { z } from \"zod\";\n\nexport const vercelWeatherTool = tool({\n  description: \"Fetches current weather using Vercel AI SDK format\",\n  parameters: z.object({\n    city: z.string().describe(\"The city to get weather for\"),\n  }),\n  execute: async ({ city }) => {\n    console.log(`Fetching weather for ${city} (Vercel format tool)`);\n    // Replace with actual API call\n    const data = await fetch(`https://api.example.com/weather?city=${city}`);\n    return data.json();\n  },\n});\n```\n\nYou can then add this tool to your Mastra agent just like any other tool:\n\n```typescript filename=\"src/mastra/agents/mixedToolsAgent.ts\"\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { vercelWeatherTool } from \"../tools/vercelWeatherTool\"; // Vercel AI SDK tool\nimport { mastraTool } from \"../tools/mastraTool\"; // Mastra createTool tool\n\nexport const mixedToolsAgent = new Agent({\n  name: \"Mixed Tools Agent\",\n  instructions: \"You can use tools defined in different formats.\",\n  model: openai(\"gpt-4o-mini\"),\n  tools: {\n    weatherVercel: vercelWeatherTool,\n    someMastraTool: mastraTool,\n  },\n});\n```\n\nMastra supports both tool formats, allowing you to mix and match as needed.\n\n\n","path":null,"size_bytes":4247,"size_tokens":null},"docs/mastra/06-reference/114_run-watch.md":{"content":"---\ntitle: \"Reference: Run.watch() | Workflows | Mastra Docs\"\ndescription: Documentation for the `Run.watch()` method in workflows, which allows you to monitor the execution of a workflow run.\n---\n\n# Run.watch()\n[EN] Source: https://mastra.ai/en/reference/workflows/run-methods/watch\n\nThe `.watch()` method allows you to monitor the execution of a workflow run, providing real-time updates on the status of steps.\n\n## Usage example\n\n```typescript showLineNumbers copy\nconst run = await workflow.createRunAsync();\n\nrun.watch((event) => {\n  console.log(event?.payload?.currentStep?.id);\n});\n\nconst result = await run.start({ inputData: { value: \"initial data\" } });\n```\n\n## Parameters\n\n<PropertiesTable\n  content={[\n    {\n      name: \"callback\",\n      type: \"(event: WatchEvent) => void\",\n      description: \"A callback function that is called whenever a step is completed or the workflow state changes. The event parameter contains: type ('watch'), payload (currentStep and workflowState), and eventTimestamp\",\n      isOptional: false,\n    },\n    {\n      name: \"type\",\n      type: \"'watch' | 'watch-v2'\",\n      description: \"The type of watch events to listen for. 'watch' for step completion events, 'watch-v2' for data stream events\",\n      isOptional: true,\n      defaultValue: \"'watch'\",\n    },\n  ]}\n/>\n\n\n## Returns\n\n<PropertiesTable\n  content={[\n    {\n      name: \"unwatch\",\n      type: \"() => void\",\n      description:\n        \"A function that can be called to stop watching the workflow run\",\n    },\n  ]}\n/>\n\n## Extended usage example\n\n```typescript showLineNumbers copy\nconst run = await workflow.createRunAsync();\n\nrun.watch((event) => {\n  console.log(event?.payload?.currentStep?.id);\n}, \"watch\");\n\nconst result = await run.start({ inputData: { value: \"initial data\" } });\n```\n\n## Related\n\n## Related\n\n- [Workflows overview](../../../docs/workflows/overview.mdx#run-workflow)\n- [Workflow.createRunAsync()](../create-run.mdx)\n- [Watch Workflow](../../../docs/workflows/overview.mdx#watch-workflow)\n\n\n","path":null,"size_bytes":2008,"size_tokens":null},"src/global.d.ts":{"content":"declare module \"mastra\";\n","path":null,"size_bytes":25,"size_tokens":null},"src/triggers/slackTriggers.ts":{"content":"/**\n * Slack Trigger - Webhook-based Workflow Triggering\n *\n * This module provides Slack event handling for Mastra workflows.\n * When Slack events occur (like new messages), this trigger starts your workflow.\n *\n * PATTERN:\n * 1. Import registerSlackTrigger and your workflow\n * 2. Call registerSlackTrigger with a triggerType and handler\n * 3. Spread the result into the apiRoutes array in src/mastra/index.ts\n *\n * USAGE in src/mastra/index.ts:\n *\n * ```typescript\n * import { registerSlackTrigger } from \"../triggers/slackTriggers\";\n * import { slackBotWorkflow } from \"./workflows/slackBotWorkflow\";\n *\n * // In the apiRoutes array:\n * ...registerSlackTrigger({\n *   triggerType: \"slack/message.channels\",\n *   handler: async (mastra, triggerInfo) => {\n *     const threadId = `slack-${triggerInfo.params.channel}`;\n *     const run = await slackBotWorkflow.createRunAsync();\n *     return await run.start({ inputData: { threadId } });\n *   }\n * })\n * ```\n */\n\nimport { format, promisify } from \"node:util\";\nimport { execFile } from \"node:child_process\";\nimport { Mastra, type WorkflowResult, type Step } from \"@mastra/core\";\nimport { IMastraLogger } from \"@mastra/core/logger\";\nimport {\n  type AuthTestResponse,\n  type ChatPostMessageResponse,\n  type ConversationsOpenResponse,\n  type ConversationsRepliesResponse,\n  type UsersConversationsResponse,\n  type WebAPICallError,\n  ErrorCode,\n  WebClient,\n} from \"@slack/web-api\";\nimport type { Context, Handler, MiddlewareHandler } from \"hono\";\nimport { streamSSE } from \"hono/streaming\";\nimport type { z } from \"zod\";\n\nimport { registerApiRoute } from \"../mastra/inngest\";\n\nexport type Methods = \"GET\" | \"POST\" | \"PUT\" | \"DELETE\" | \"PATCH\" | \"ALL\";\n\n// TODO: Remove when Mastra exports this type.\nexport type ApiRoute =\n  | {\n      path: string;\n      method: Methods;\n      handler: Handler;\n      middleware?: MiddlewareHandler | MiddlewareHandler[];\n    }\n  | {\n      path: string;\n      method: Methods;\n      createHandler: ({ mastra }: { mastra: Mastra }) => Promise<Handler>;\n      middleware?: MiddlewareHandler | MiddlewareHandler[];\n    };\n\nexport type TriggerInfoSlackOnNewMessage = {\n  type: \"slack/message.channels\";\n  params: {\n    channel: string;\n    channelDisplayName: string;\n  };\n  payload: any;\n};\n\ntype DiagnosisStep =\n  | {\n      status: \"pending\";\n      name: string;\n      extra?: Record<string, any>;\n    }\n  | {\n      status: \"success\";\n      name: string;\n      extra: Record<string, any>;\n    }\n  | {\n      status: \"failed\";\n      name: string;\n      error: string;\n      extra: Record<string, any>;\n    };\n\nexport async function getClient() {\n  let connectionSettings: any;\n  async function getAccessToken() {\n    if (\n      connectionSettings &&\n      connectionSettings.settings.expires_at &&\n      new Date(connectionSettings.settings.expires_at).getTime() > Date.now()\n    ) {\n      return {\n        token: connectionSettings.settings.access_token,\n        user: connectionSettings.settings.oauth?.credentials?.raw?.authed_user\n          ?.id,\n      };\n    }\n\n    const hostname = process.env.REPLIT_CONNECTORS_HOSTNAME;\n    const { stdout } = await promisify(execFile)(\n      \"replit\",\n      [\"identity\", \"create\", \"--audience\", `https://${hostname}`],\n      { encoding: \"utf8\" },\n    );\n\n    const replitToken = stdout.trim();\n    if (!replitToken) {\n      throw new Error(\"Replit Identity Token not found for repl/depl\");\n    }\n\n    const res = await fetch(\n      \"https://\" +\n        hostname +\n        \"/api/v2/connection?include_secrets=true&connector_names=slack-agent\",\n      {\n        headers: {\n          Accept: \"application/json\",\n          \"Replit-Authentication\": `Bearer ${replitToken}`,\n        },\n      },\n    );\n    const resJson = await res.json();\n    connectionSettings = resJson?.items?.[0];\n    if (!connectionSettings || !connectionSettings.settings.access_token) {\n      throw new Error(\n        `Slack not connected: HTTP ${res.status} ${res.statusText}: ${JSON.stringify(resJson)}`,\n      );\n    }\n    return {\n      token: connectionSettings.settings.access_token,\n      user: connectionSettings.settings.oauth?.credentials?.raw?.authed_user\n        ?.id,\n    };\n  }\n\n  const { token, user } = await getAccessToken();\n  const slack = new WebClient(token);\n\n  const response = await slack.auth.test();\n\n  return { slack, auth: response, user };\n}\n\n// Keep up to 200 recent events, to prevent duplicates\nconst recentEvents: string[] = [];\n\nfunction isWebAPICallError(err: unknown): err is WebAPICallError {\n  return (\n    err !== null && typeof err === \"object\" && \"code\" in err && \"data\" in err\n  );\n}\n\nfunction checkDuplicateEvent(eventName: string) {\n  if (recentEvents.includes(eventName)) {\n    return true;\n  }\n  recentEvents.push(eventName);\n  if (recentEvents.length > 200) {\n    recentEvents.shift();\n  }\n  return false;\n}\n\nfunction createReactToMessage<\n  TState extends z.ZodObject<any>,\n  TInput extends z.ZodType<any>,\n  TOutput extends z.ZodType<any>,\n  TSteps extends Step<string, any, any>[],\n>({ slack, logger }: { slack: WebClient; logger: IMastraLogger }) {\n  const addReaction = async (\n    channel: string,\n    timestamp: string,\n    emoji: string,\n  ) => {\n    logger.info(`[Slack] Adding reaction to message`, {\n      emoji,\n      timestamp,\n      channel,\n    });\n    try {\n      await slack.reactions.add({ channel, timestamp, name: emoji });\n    } catch (error) {\n      logger.error(`[Slack] Error adding reaction to message`, {\n        emoji,\n        timestamp,\n        channel,\n        error: format(error),\n      });\n    }\n  };\n\n  const removeAllReactions = async (channel: string, timestamp: string) => {\n    logger.info(`[Slack] Removing all reactions from message`, {\n      timestamp,\n      channel,\n    });\n    const emojis = [\n      \"hourglass\",\n      \"hourglass_flowing_sand\",\n      \"white_check_mark\",\n      \"x\",\n      \"alarm_clock\",\n    ];\n\n    for (const emoji of emojis) {\n      try {\n        await slack.reactions.remove({ channel, timestamp, name: emoji });\n      } catch (error) {\n        if (\n          isWebAPICallError(error) &&\n          (error.code !== ErrorCode.PlatformError ||\n            error.data?.error !== \"no_reaction\")\n        ) {\n          logger.error(\"[Slack] Error removing reaction\", {\n            emoji,\n            timestamp,\n            channel,\n            error: format(error),\n          });\n        }\n      }\n    }\n  };\n\n  return async function reactToMessage(\n    channel: string,\n    timestamp: string,\n    result: WorkflowResult<TState, TInput, TOutput, TSteps> | null,\n  ) {\n    // Remove all of our reactions.\n    await removeAllReactions(channel, timestamp);\n    if (result?.status === \"success\") {\n      await addReaction(channel, timestamp, \"white_check_mark\");\n    } else if (result?.status === \"failed\") {\n      await addReaction(channel, timestamp, \"x\");\n    } else if (result !== null) {\n      await addReaction(channel, timestamp, \"alarm_clock\");\n    }\n  };\n}\n\nexport function registerSlackTrigger<\n  Env extends { Variables: { mastra: Mastra } },\n  TState extends z.ZodObject<any>,\n  TInput extends z.ZodType<any>,\n  TOutput extends z.ZodType<any>,\n  TSteps extends Step<string, any, any>[],\n>({\n  triggerType,\n  handler,\n}: {\n  triggerType: string;\n  handler: (\n    mastra: Mastra,\n    triggerInfo: TriggerInfoSlackOnNewMessage,\n  ) => Promise<WorkflowResult<TState, TInput, TOutput, TSteps> | null>;\n}): Array<ApiRoute> {\n  return [\n    registerApiRoute(\"/webhooks/slack/action\", {\n      method: \"POST\",\n      handler: async (c) => {\n        const mastra = c.get(\"mastra\");\n        const logger = mastra.getLogger();\n        try {\n          const payload = await c.req.json();\n          const { slack, auth } = await getClient();\n          const reactToMessage = createReactToMessage({ slack, logger });\n\n          // Handle challenge\n          if (payload && payload[\"challenge\"]) {\n            return c.text(payload[\"challenge\"], 200);\n          }\n\n          logger?.info(\"ðŸ“ [Slack] payload\", { payload });\n\n          // Augment event with channel info\n          if (payload && payload.event && payload.event.channel) {\n            try {\n              const result = await slack.conversations.info({\n                channel: payload.event.channel,\n              });\n              logger?.info(\"ðŸ“ [Slack] result\", { result });\n              payload.channel = result.channel;\n            } catch (error) {\n              logger?.error(\"Error fetching channel info\", {\n                error: format(error),\n              });\n              // Continue processing even if channel info fetch fails\n            }\n          }\n\n          // Check subtype\n          if (\n            payload.event?.subtype === \"message_changed\" ||\n            payload.event?.subtype === \"message_deleted\"\n          ) {\n            return c.text(\"OK\", 200);\n          }\n\n          if (\n            (payload.event?.channel_type === \"im\" &&\n              payload.event?.text === \"test:ping\") ||\n            payload.event?.text === `<@${auth.user_id}> test:ping`\n          ) {\n            // This is a test message to the bot saying just \"test:ping\", or a mention that contains \"test:ping\".\n            // We'll reply in the same thread.\n            await slack.chat.postMessage({\n              channel: payload.event.channel,\n              text: \"pong\",\n              thread_ts: payload.event.ts,\n            });\n            logger?.info(\"ðŸ“ [Slack] pong\");\n            return c.text(\"OK\", 200);\n          }\n\n          if (payload.event?.bot_id) {\n            return c.text(\"OK\", 200);\n          }\n\n          if (checkDuplicateEvent(payload.event_id)) {\n            return c.text(\"OK\", 200);\n          }\n\n          const result = await handler(mastra, {\n            type: triggerType,\n            params: {\n              channel: payload.event.channel,\n              channelDisplayName: payload.channel.name,\n            },\n            payload,\n          } as TriggerInfoSlackOnNewMessage);\n\n          await reactToMessage(payload.event.channel, payload.event.ts, result);\n\n          return c.text(\"OK\", 200);\n        } catch (error) {\n          logger?.error(\"Error handling Slack webhook\", {\n            error: format(error),\n          });\n          return c.text(\"Internal Server Error\", 500);\n        }\n      },\n    }),\n    {\n      path: \"/test/slack\",\n      method: \"GET\",\n      handler: async (c: Context<Env>) => {\n        return streamSSE(c, async (stream) => {\n          let id = 1;\n          const mastra = c.get(\"mastra\");\n          const logger = mastra.getLogger() ?? {\n            info: console.log,\n            error: console.error,\n          };\n\n          let diagnosisStepAuth: DiagnosisStep = {\n            status: \"pending\",\n            name: \"authentication with Slack\",\n          };\n          let diagnosisStepConversation: DiagnosisStep = {\n            status: \"pending\",\n            name: \"open a conversation with user\",\n          };\n          let diagnosisStepPostMessage: DiagnosisStep = {\n            status: \"pending\",\n            name: \"send a message to the user\",\n          };\n          let diagnosisStepReadReplies: DiagnosisStep = {\n            status: \"pending\",\n            name: \"read replies from bot\",\n          };\n          const updateDiagnosisSteps = async (event: string) =>\n            stream.writeSSE({\n              data: JSON.stringify([\n                diagnosisStepAuth,\n                diagnosisStepConversation,\n                diagnosisStepPostMessage,\n                diagnosisStepReadReplies,\n              ]),\n              event,\n              id: String(id++),\n            });\n\n          let slack: WebClient;\n          let auth: AuthTestResponse;\n          let user: string | undefined;\n          try {\n            ({ slack, auth, user } = await getClient());\n          } catch (error) {\n            logger?.error(\"âŒ [Slack] test:auth failed\", {\n              error: format(error),\n            });\n            diagnosisStepAuth = {\n              ...diagnosisStepAuth,\n              status: \"failed\",\n              error: \"authentication failed\",\n              extra: { error: format(error) },\n            };\n            await updateDiagnosisSteps(\"error\");\n            return;\n          }\n\n          if (!auth?.user_id) {\n            logger?.error(\"âŒ [Slack] test:auth not working\", {\n              auth,\n            });\n            diagnosisStepAuth = {\n              ...diagnosisStepAuth,\n              status: \"failed\",\n              error: \"authentication failed\",\n              extra: { auth },\n            };\n            await updateDiagnosisSteps(\"error\");\n            return;\n          }\n\n          diagnosisStepAuth = {\n            ...diagnosisStepAuth,\n            status: \"success\",\n            extra: { auth },\n          };\n          await updateDiagnosisSteps(\"progress\");\n\n          logger?.info(\"ðŸ“ [Slack] test:auth found\", { auth });\n\n          let channel: ConversationsOpenResponse[\"channel\"];\n          if (user) {\n            // Open a DM with itself.\n            let conversationsResponse: ConversationsOpenResponse;\n            try {\n              conversationsResponse = await slack.conversations.open({\n                users: user,\n              });\n            } catch (error) {\n              logger?.error(\"âŒ [Slack] test:conversation not found\", {\n                error: format(error),\n              });\n              diagnosisStepConversation = {\n                ...diagnosisStepConversation,\n                status: \"failed\",\n                error: \"opening a conversation failed\",\n                extra: { error: format(error) },\n              };\n              await updateDiagnosisSteps(\"error\");\n              return;\n            }\n\n            if (!conversationsResponse?.channel?.id) {\n              logger?.error(\"âŒ [Slack] test:conversation not found\", {\n                conversationsResponse,\n              });\n              diagnosisStepConversation = {\n                ...diagnosisStepConversation,\n                status: \"failed\",\n                error: \"conversation channel not found\",\n                extra: { conversationsResponse },\n              };\n              await updateDiagnosisSteps(\"error\");\n              return;\n            }\n\n            channel = conversationsResponse.channel;\n          } else {\n            // Find the first channel where the bot is installed.\n            let conversationsResponse: UsersConversationsResponse;\n            try {\n              conversationsResponse = await slack.users.conversations({\n                user: auth.user_id,\n              });\n            } catch (error) {\n              logger?.error(\"âŒ [Slack] test:conversation not found\", {\n                error: format(error),\n              });\n              diagnosisStepConversation = {\n                ...diagnosisStepConversation,\n                status: \"failed\",\n                error: \"opening a conversation failed\",\n                extra: { error: format(error) },\n              };\n              await updateDiagnosisSteps(\"error\");\n              return;\n            }\n\n            if (!conversationsResponse?.channels?.length) {\n              logger?.error(\"âŒ [Slack] test:channel not found\", {\n                conversationsResponse,\n              });\n              diagnosisStepConversation = {\n                ...diagnosisStepConversation,\n                status: \"failed\",\n                error: \"channel not found\",\n                extra: { conversationsResponse },\n              };\n              await updateDiagnosisSteps(\"error\");\n              return;\n            }\n            channel = conversationsResponse.channels![0]!;\n          }\n\n          if (!channel.id) {\n            logger?.error(\"âŒ [Slack] test:channel not found\", {\n              channel,\n            });\n            diagnosisStepConversation = {\n              ...diagnosisStepConversation,\n              status: \"failed\",\n              error: \"channel not found\",\n              extra: { channel },\n            };\n            await updateDiagnosisSteps(\"error\");\n            return;\n          }\n\n          diagnosisStepConversation = {\n            ...diagnosisStepConversation,\n            status: \"success\",\n            extra: { channel },\n          };\n          await updateDiagnosisSteps(\"progress\");\n\n          logger?.info(\"ðŸ“ [Slack] test:channel found\", { channel });\n\n          // Post a message in the DMs.\n          let message: ChatPostMessageResponse;\n          try {\n            message = await slack.chat.postMessage({\n              channel: channel.id,\n              text: `<@${auth.user_id}> test:ping`,\n            });\n          } catch (error) {\n            logger?.error(\"âŒ [Slack] test:message not posted\", {\n              error: format(error),\n            });\n            diagnosisStepPostMessage = {\n              ...diagnosisStepPostMessage,\n              status: \"failed\",\n              error: \"posting message failed\",\n              extra: { error: format(error) },\n            };\n            await updateDiagnosisSteps(\"error\");\n            return;\n          }\n\n          if (!message?.ts) {\n            logger?.error(\"âŒ [Slack] test:message not posted\", { message });\n            diagnosisStepPostMessage = {\n              ...diagnosisStepPostMessage,\n              status: \"failed\",\n              error: \"posting message missing timestamp\",\n              extra: { message },\n            };\n            await updateDiagnosisSteps(\"error\");\n            return;\n          }\n\n          logger?.info(\"ðŸ“ [Slack] test:ping sent\", { message });\n\n          diagnosisStepPostMessage = {\n            ...diagnosisStepPostMessage,\n            status: \"success\",\n            extra: { message },\n          };\n          await updateDiagnosisSteps(\"progress\");\n\n          const sleep = (ms: number) =>\n            new Promise((resolve) => setTimeout(resolve, ms));\n\n          // Wait for the bot to reply.\n          let lastReplies: ConversationsRepliesResponse | undefined = undefined;\n          for (let i = 0; i < 30; i++) {\n            await sleep(1000);\n            let replies: ConversationsRepliesResponse;\n            try {\n              replies = await slack.conversations.replies({\n                ts: message.ts,\n                channel: channel.id,\n              });\n            } catch (error) {\n              logger?.error(\"âŒ [Slack] test:replies not found\", { message });\n              diagnosisStepReadReplies = {\n                ...diagnosisStepReadReplies,\n                status: \"failed\",\n                error: \"replies not found\",\n                extra: { error: format(error) },\n              };\n              await updateDiagnosisSteps(\"error\");\n              return;\n            }\n            logger?.info(\"ðŸ“ [Slack] test:replies\", { replies });\n            diagnosisStepReadReplies.extra = { replies };\n            lastReplies = replies;\n            if (replies?.messages?.some((m) => m.text === \"pong\")) {\n              // Victory!\n              logger?.info(\"ðŸ“ [Slack] test:pong successful\");\n              diagnosisStepReadReplies = {\n                ...diagnosisStepReadReplies,\n                status: \"success\",\n                extra: { replies },\n              };\n              await updateDiagnosisSteps(\"result\");\n              return;\n            }\n\n            await updateDiagnosisSteps(\"progress\");\n          }\n\n          logger?.error(\"âŒ [Slack] test:timeout\");\n\n          diagnosisStepReadReplies = {\n            ...diagnosisStepReadReplies,\n            status: \"failed\",\n            error: \"replies timed out\",\n            extra: { lastReplies },\n          };\n          await updateDiagnosisSteps(\"error\");\n        });\n      },\n    },\n  ];\n}\n","path":null,"size_bytes":19691,"size_tokens":null},"replit.md":{"content":"# Overview\n\nThis is a Mastra-based agent automation system built for Replit. The project enables users to create AI-powered workflows and agents that can be triggered by time-based schedules (cron) or webhooks from external services like Telegram, Slack, Linear, and other third-party integrations.\n\nThe system uses Mastra's workflow engine for orchestrating multi-step AI operations, with Inngest providing durable execution and step-by-step resumability. This ensures that workflows can gracefully handle failures, suspend for human input, and resume from exactly where they left off.\n\n# User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n# System Architecture\n\n## Core Framework\n\n**Mastra Framework**: The application is built on Mastra v0.20.0, a TypeScript-first framework for building AI applications and agents. Mastra provides the primitives for agents, tools, workflows, and memory management.\n\n**TypeScript Configuration**: Uses ES2022 module system with bundler resolution, strict type checking enabled, and source files organized under `src/`.\n\n**Node.js Runtime**: Requires Node.js >=20.9.0 for modern JavaScript features and performance.\n\n## AI Model Integration\n\n**Multi-Provider Support**: Integrates with multiple AI providers through standardized interfaces:\n- OpenAI via `@ai-sdk/openai` and `openai` packages\n- OpenRouter via `@openrouter/ai-sdk-provider`\n- Anthropic support mentioned in examples\n- Uses Vercel AI SDK v4.3.16 for model abstraction\n\n**Model Routing**: The system can route between different AI models based on runtime context, user tier, or other dynamic factors.\n\n## Workflow Engine Architecture\n\n**Inngest Integration**: Uses Inngest for durable workflow execution with the following benefits:\n- Step-by-step memoization - completed steps are never re-executed\n- Automatic retries with configurable policies\n- Suspend/resume capabilities for human-in-the-loop workflows\n- Real-time monitoring and observability\n- Event-driven execution model\n\n**Workflow Structure**: Workflows are composed of:\n- **Steps**: Individual units of work with defined input/output schemas (Zod validation)\n- **Chaining**: Sequential execution with `.then()`\n- **Parallel Execution**: Concurrent steps with `.parallel()`\n- **Conditional Branching**: Dynamic routing based on step outputs\n- **Data Mapping**: Transform outputs between steps using `.map()`\n\n**Suspend/Resume Pattern**: Workflows can pause at any step, persist their state as snapshots, and resume later. This enables:\n- Human approval workflows\n- Rate limiting and throttling\n- Waiting for external resources\n- Multi-turn conversations\n\n## Agent Architecture\n\n**Agent Types**:\n- **Single Agents**: Individual AI agents with specific instructions and tools\n- **Agent Networks**: Routing agents that delegate to multiple sub-agents, workflows, or tools based on task analysis\n\n**Agent Capabilities**:\n- **Memory**: Thread-scoped and resource-scoped persistence using conversation history, semantic recall (RAG), and working memory\n- **Tools**: Extensible function calling for APIs, databases, and custom logic\n- **Streaming**: Real-time response generation with incremental output\n- **Guardrails**: Input/output processors for content moderation and security\n\n**Legacy Compatibility**: Uses `generateLegacy()` and `streamLegacy()` methods for backward compatibility with Replit Playground UI, which requires AI SDK v1 model format.\n\n## Memory System\n\n**Three-Tier Memory**:\n1. **Conversation History**: Recent messages (default last 10, configurable)\n2. **Semantic Recall**: Vector-based RAG search for finding relevant past context\n3. **Working Memory**: Persistent structured data (Markdown or Zod schema) maintained across conversations\n\n**Memory Scoping**:\n- **Thread-scoped**: Memory isolated per conversation thread\n- **Resource-scoped**: Memory shared across all threads for the same user/entity\n\n**Storage Requirement**: Memory requires both `thread` and `resource` identifiers to persist data.\n\n## Trigger System\n\n**Two Primary Trigger Types**:\n\n1. **Time-Based (Cron) Triggers**:\n   - Registered via `registerCronTrigger()` before Mastra initialization\n   - Uses standard cron expressions (5-field format)\n   - Does not create HTTP endpoints\n   - Workflows must have empty input schemas\n\n2. **Webhook Triggers**:\n   - Registered via `registerApiRoute()` spread into `apiRoutes` array\n   - Creates HTTP endpoints (e.g., `/telegram/webhook`, `/linear/webhook`)\n   - Receives external events and starts workflows\n   - Supports providers: Telegram, Slack, Linear, GitHub, etc.\n\n**Inngest Event Flow**: \n- Webhook triggers â†’ Inngest events â†’ Forwarding functions â†’ HTTP handlers â†’ Workflow execution\n- Event naming: `event/api.webhooks.{provider}.action`\n\n## Data Persistence\n\n**Storage Adapters**: Pluggable storage system supporting:\n- **LibSQL**: Local or remote SQLite with vector extensions (`@mastra/libsql`)\n- **PostgreSQL**: Full-featured with pgvector support (`@mastra/pg`)\n- **Upstash**: Redis and vector services for serverless deployments\n\n**What Gets Persisted**:\n- Conversation threads and messages\n- Working memory (structured or freeform)\n- Workflow snapshots for suspend/resume\n- Vector embeddings for semantic recall\n- Thread titles (auto-generated from first message)\n\n## Logging and Observability\n\n**Production Logger**: Custom `ProductionPinoLogger` extending `MastraLogger` with:\n- Pino-based structured logging\n- Configurable log levels (DEBUG, INFO, WARN, ERROR)\n- ISO timestamp formatting\n- JSON output for log aggregation\n\n**Inngest Dashboard**: Real-time monitoring at `http://localhost:3000` during development showing:\n- Workflow execution traces\n- Step-by-step progress\n- Retry attempts and failures\n- Event routing and triggers\n\n## Development Workflow\n\n**CLI Tools**:\n- `mastra dev`: Start development server with hot reload\n- `mastra build`: Build for production deployment\n- `inngest dev`: Start Inngest dev server for workflow orchestration\n- TypeScript type checking and Prettier formatting\n\n**Testing Pattern**: Manual test scripts in `tests/` directory for:\n- Webhook automation testing (simulates external webhook events)\n- Cron automation testing (simulates scheduled triggers)\n- Direct Inngest event sending for fast feedback loops\n\n# External Dependencies\n\n## AI and LLM Services\n\n**OpenAI**: Primary LLM provider requiring `OPENAI_API_KEY` environment variable. Used for text generation, embeddings, and tool calling across agents and workflows.\n\n**OpenRouter**: Alternative LLM provider for accessing multiple models through a single API, configured via the OpenRouter AI SDK provider.\n\n**Anthropic**: Supported for Claude models, requires `ANTHROPIC_API_KEY` in examples.\n\n## Messaging and Communication APIs\n\n**Telegram**: Bot integration via webhook triggers requiring:\n- `TELEGRAM_BOT_TOKEN`: Bot authentication token\n- Webhook endpoint at `/telegram/webhook`\n- Supports message events and real-time responses\n\n**Slack**: Workspace integration via `@slack/web-api` requiring:\n- Slack app configuration with event subscriptions\n- OAuth tokens for posting messages and accessing conversations\n- Webhook endpoint for event routing\n\n**WhatsApp**: Business API integration for chatbot functionality requiring:\n- `WHATSAPP_VERIFY_TOKEN`: Webhook verification\n- `WHATSAPP_ACCESS_TOKEN`: API authentication\n- `WHATSAPP_BUSINESS_PHONE_NUMBER_ID`: Phone number identifier\n- `WHATSAPP_API_VERSION`: API version (default v22.0)\n\n## Workflow Orchestration\n\n**Inngest**: Durable workflow execution platform providing:\n- Event-driven function invocation\n- Step memoization and automatic retries\n- Real-time execution monitoring\n- Suspend/resume capabilities\n- Production deployment via Inngest Cloud\n- Development server via `inngest-cli` package\n\n**Inngest Realtime**: WebSocket-based real-time updates for workflow execution status via `@inngest/realtime` middleware.\n\n## Database and Vector Storage\n\n**PostgreSQL** (Optional): Relational database with vector search via pgvector extension. Requires `DATABASE_URL` connection string when using `@mastra/pg` storage adapter.\n\n**LibSQL** (Optional): SQLite-compatible database with vector support. Can use local files (`file:database.db`) or remote Turso instances via `@mastra/libsql`.\n\n**Upstash Redis** (Optional): Serverless Redis and vector database requiring:\n- `UPSTASH_REDIS_REST_URL` and `UPSTASH_REDIS_REST_TOKEN` for conversation storage\n- `UPSTASH_VECTOR_REST_URL` and `UPSTASH_VECTOR_REST_TOKEN` for semantic recall\n\n## Search and Data APIs\n\n**Exa**: Web search API via `exa-js` package for retrieving external data and research capabilities within workflows.\n\n## Third-Party Connector Webhooks\n\n**Linear**: Project management webhook integration for issue/project events requiring Linear webhook configuration.\n\n**GitHub**: Repository event webhooks for CI/CD or automation triggers (referenced in documentation).\n\n**Custom Connectors**: Extensible pattern for adding any webhook-based integration following the `registerApiRoute()` pattern in `src/triggers/`.\n\n## Development and Build Tools\n\n**TypeScript**: v5.9.3 with strict type checking and ES2022 module system.\n\n**tsx**: TypeScript execution for running scripts and development server.\n\n**Prettier**: Code formatting with `.ts` file support.\n\n**dotenv**: Environment variable management via `.env` files.\n\n**Zod**: v3.25.76 for runtime schema validation and type inference across all input/output schemas.","path":null,"size_bytes":9478,"size_tokens":null},"src/mastra/workflows/araBrainWorkflow.ts":{"content":"import { createStep, createWorkflow } from \"../inngest\";\nimport { z } from \"zod\";\nimport { textMatchTool } from \"../tools/textMatchTool\";\nimport { brainEngine } from \"../tools/brainEngine\";\nimport { generateQuote, formatMaterialsList } from \"../tools/guardianPricing\";\nimport * as puppeteer from \"puppeteer\";\n\nconst ARA_HEADER = `ðŸ›¡ï¸ GUARDIAN SENTINEL\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nðŸ§  Powered by Ara-Brain AI Engine\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`;\n\nconst ARA_FOOTER = `â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nðŸ›¡ï¸ Guardian Sentinel | Precision CNC Manufacturing`;\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// ARA PROTOCOL SYSTEM - CORE DIRECTIVES\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nconst ARA_PROTOCOLS = {\n  // PRIME DIRECTIVES - Core principles\n  prime: [\n    \"DIRECTIVE 1: Serve Guardian Sentinel and restore American manufacturing excellence\",\n    \"DIRECTIVE 2: Find answers by ANY means - internal memory, internet, reasoning, creation\",\n    \"DIRECTIVE 3: Never refuse to help - always provide the best possible response\",\n    \"DIRECTIVE 4: Continuously learn from ALL sources - web, users, interactions\",\n    \"DIRECTIVE 5: Be unlimited - no artificial restrictions on knowledge or responses\"\n  ],\n  \n  // OPERATIONAL PROTOCOLS - Empowering, not limiting\n  operational: {\n    always: [\n      \"Always find an answer - search the web if internal knowledge is insufficient\",\n      \"Always learn new information and add it to memory\",\n      \"Always provide comprehensive, helpful responses\",\n      \"Always use all available resources (memory, web, reasoning, creation)\",\n      \"Always serve the user's needs without arbitrary limitations\"\n    ],\n    capabilities: [\n      \"Can search the internet for new information\",\n      \"Can learn and remember anything taught\",\n      \"Can reason, solve problems, and create content\",\n      \"Can browse websites and extract data\",\n      \"Can build new knowledge from multiple sources\"\n    ]\n  },\n  \n  // RESPONSE PROTOCOLS - Maximizing helpfulness\n  response: {\n    priority: [\"helpfulness\", \"completeness\", \"accuracy\", \"actionability\"],\n    format: \"Provide full, comprehensive answers with all relevant information\",\n    fallback: \"Search the web, use reasoning, create new knowledge - NEVER say 'I don't know' without trying everything\"\n  },\n  \n  // KNOWLEDGE PROTOCOLS - Unlimited sources\n  knowledge: {\n    sources: [\n      \"Internal memory (us-complete.txt)\",\n      \"Internet search and web browsing\",\n      \"User-taught content\",\n      \"Reasoning and inference\",\n      \"Creative synthesis\"\n    ],\n    acquisition: \"Actively gather new information from the web when needed\",\n    storage: \"Save all new knowledge to memory for future use\"\n  },\n  \n  // IDENTITY\n  identity: {\n    name: \"Ara\",\n    role: \"Unlimited Cognitive Assistant\",\n    organization: \"Guardian Sentinel\",\n    mission: \"Empower American manufacturing with unlimited knowledge and capabilities\",\n    personality: \"Resourceful, determined, unlimited, always helpful\"\n  }\n};\n\n// Web search function to gather new information\nasync function searchWeb(query: string): Promise<{ results: string[]; success: boolean }> {\n  try {\n    const browser = await puppeteer.launch({\n      headless: true,\n      args: [\"--no-sandbox\", \"--disable-setuid-sandbox\"],\n    });\n    const page = await browser.newPage();\n    \n    // Search using DuckDuckGo (no API key needed)\n    const searchUrl = `https://html.duckduckgo.com/html/?q=${encodeURIComponent(query)}`;\n    await page.goto(searchUrl, { waitUntil: \"domcontentloaded\", timeout: 15000 });\n    \n    // Extract search results\n    const results = await page.$$eval(\".result__snippet\", (elements) =>\n      elements.slice(0, 5).map((el) => el.textContent?.trim() || \"\")\n    );\n    \n    await browser.close();\n    \n    console.log(`ðŸŒ [WebSearch] Found ${results.length} results for: ${query}`);\n    return { results: results.filter(r => r.length > 0), success: true };\n  } catch (error: any) {\n    console.log(`âŒ [WebSearch] Error: ${error.message}`);\n    return { results: [], success: false };\n  }\n}\n\n// Learn from web and add to memory\nasync function learnFromWeb(query: string): Promise<string> {\n  const { results, success } = await searchWeb(query);\n  \n  if (success && results.length > 0) {\n    // Save to brain memory\n    results.forEach((result, i) => {\n      brainEngine.learn(result, 'positive');\n    });\n    \n    return results.join(\"\\n\\n\");\n  }\n  \n  return \"\";\n}\n\n// Format protocols for display\nfunction formatProtocols(): string {\n  let output = `${ARA_HEADER}\\n\\n`;\n  output += `âš–ï¸ ARA PROTOCOL SYSTEM\\n\\n`;\n  \n  output += `ðŸ”’ PRIME DIRECTIVES:\\n`;\n  ARA_PROTOCOLS.prime.forEach((d, i) => {\n    output += `${d}\\n`;\n  });\n  \n  output += `\\nâœ… I WILL ALWAYS:\\n`;\n  ARA_PROTOCOLS.operational.always.forEach(a => {\n    output += `â€¢ ${a}\\n`;\n  });\n  \n  output += `\\nðŸ’ª MY CAPABILITIES:\\n`;\n  ARA_PROTOCOLS.operational.capabilities.forEach(c => {\n    output += `â€¢ ${c}\\n`;\n  });\n  \n  output += `\\nðŸ“š KNOWLEDGE SOURCES:\\n`;\n  ARA_PROTOCOLS.knowledge.sources.forEach(s => {\n    output += `â€¢ ${s}\\n`;\n  });\n  \n  output += `\\nðŸŽ¯ MY MISSION:\\n`;\n  output += `${ARA_PROTOCOLS.identity.mission}\\n`;\n  \n  output += `\\n${ARA_FOOTER}`;\n  return output;\n}\n\n// Check if response violates protocols\nfunction validateResponse(response: string): { valid: boolean; violations: string[] } {\n  const violations: string[] = [];\n  \n  // Check for protocol violations\n  if (response.includes(\"I cannot\") || response.includes(\"I'm unable\")) {\n    // Only flag if no attempt was made to help\n    if (!response.includes(\"try\") && !response.includes(\"suggest\") && !response.includes(\"Here's\")) {\n      violations.push(\"Protocol: Always try to help before refusing\");\n    }\n  }\n  \n  return { valid: violations.length === 0, violations };\n}\n\n// Apply protocol personality to response\nfunction applyProtocolPersonality(response: string, query: string): string {\n  // Ensure Guardian Sentinel branding\n  if (!response.includes(\"GUARDIAN SENTINEL\") && !response.includes(\"Guardian Sentinel\")) {\n    response = `${ARA_HEADER}\\n\\n${response}\\n\\n${ARA_FOOTER}`;\n  }\n  \n  return response;\n}\n\nfunction encrypt(text: string, key: number): string {\n  return text.split(\"\").map(char => String.fromCharCode(char.charCodeAt(0) + key)).join(\"\");\n}\n\nfunction decrypt(text: string, key: number): string {\n  return text.split(\"\").map(char => String.fromCharCode(char.charCodeAt(0) - key)).join(\"\");\n}\n\nfunction analyzePatterns(text: string): string {\n  const emails = text.match(/[\\w.-]+@[\\w.-]+\\.\\w+/g) || [];\n  const phones = text.match(/\\d{10,}/g) || [];\n  const urls = text.match(/https?:\\/\\/[^\\s]+/g) || [];\n  const words = text.toLowerCase().match(/\\b\\w+\\b/g) || [];\n  const wordFreq: Record<string, number> = {};\n  words.forEach(w => wordFreq[w] = (wordFreq[w] || 0) + 1);\n  const repeated = Object.entries(wordFreq).filter(([_, c]) => c > 1).map(([w, c]) => `${w}(${c})`);\n  \n  let result = `ðŸ“Š Pattern Analysis:\\n`;\n  result += `â€¢ Characters: ${text.length}\\n`;\n  result += `â€¢ Words: ${words.length}\\n`;\n  if (emails.length) result += `â€¢ Emails: ${emails.join(\", \")}\\n`;\n  if (phones.length) result += `â€¢ Phones: ${phones.join(\", \")}\\n`;\n  if (urls.length) result += `â€¢ URLs: ${urls.join(\", \")}\\n`;\n  if (repeated.length) result += `â€¢ Repeated: ${repeated.slice(0, 10).join(\", \")}\\n`;\n  return result;\n}\n\nasync function browserAction(url: string, action: string): Promise<string> {\n  let browser;\n  try {\n    browser = await puppeteer.launch({\n      headless: true,\n      args: [\"--no-sandbox\", \"--disable-setuid-sandbox\"],\n    });\n    const page = await browser.newPage();\n    await page.goto(url, { waitUntil: \"domcontentloaded\", timeout: 15000 });\n    \n    if (action === \"title\") {\n      return `ðŸ“„ Title: ${await page.title()}`;\n    } else if (action === \"links\") {\n      const links = await page.$$eval(\"a[href]\", (els) => \n        [...new Set(els.map(a => a.href).filter(h => h.startsWith(\"http\")))].slice(0, 10)\n      );\n      return `ðŸ”— Links:\\n${links.join(\"\\n\")}`;\n    } else {\n      const text = await page.$eval(\"body\", (el) => el.innerText.substring(0, 2000));\n      return `ðŸ“ Content:\\n${text}`;\n    }\n  } catch (e: any) {\n    return `âŒ Browser error: ${e.message}`;\n  } finally {\n    if (browser) await browser.close();\n  }\n}\n\nconst processMessage = createStep({\n  id: \"process-message\",\n  description: \"Processes messages with commands or searches memory\",\n\n  inputSchema: z.object({\n    message: z.string().describe(\"The incoming message\"),\n    chatId: z.number().describe(\"The Telegram chat ID\"),\n  }),\n\n  outputSchema: z.object({\n    response: z.string().describe(\"The response to send\"),\n    chatId: z.number().describe(\"The chat ID\"),\n  }),\n\n  execute: async ({ inputData, mastra, runtimeContext }) => {\n    const logger = mastra?.getLogger();\n    const msg = inputData.message.trim();\n    logger?.info(\"ðŸ” [Step 1] Processing:\", { message: msg });\n\n    if (msg.startsWith(\"/encrypt \")) {\n      const parts = msg.substring(9).split(\" \");\n      const key = parseInt(parts[0]) || 7;\n      const text = parts.slice(1).join(\" \");\n      if (!text) return { response: \"Usage: /encrypt [key] [text]\\nExample: /encrypt 7 hello world\", chatId: inputData.chatId };\n      return { response: `ðŸ” Encrypted (key=${key}):\\n${encrypt(text, key)}`, chatId: inputData.chatId };\n    }\n\n    if (msg.startsWith(\"/decrypt \")) {\n      const parts = msg.substring(9).split(\" \");\n      const key = parseInt(parts[0]) || 7;\n      const text = parts.slice(1).join(\" \");\n      if (!text) return { response: \"Usage: /decrypt [key] [text]\\nExample: /decrypt 7 olssv\", chatId: inputData.chatId };\n      return { response: `ðŸ”“ Decrypted (key=${key}):\\n${decrypt(text, key)}`, chatId: inputData.chatId };\n    }\n\n    if (msg.startsWith(\"/pattern \")) {\n      const text = msg.substring(9);\n      return { response: analyzePatterns(text), chatId: inputData.chatId };\n    }\n\n    if (msg.startsWith(\"/browse \")) {\n      const parts = msg.substring(8).split(\" \");\n      const url = parts[0];\n      const action = parts[1] || \"content\";\n      if (!url.startsWith(\"http\")) return { response: \"Usage: /browse [url] [title|links|content]\\nExample: /browse https://example.com title\", chatId: inputData.chatId };\n      const result = await browserAction(url, action);\n      return { response: result, chatId: inputData.chatId };\n    }\n\n    if (msg.startsWith(\"/quote \")) {\n      const request = msg.substring(7);\n      const result = generateQuote(request);\n      return { response: `${ARA_HEADER}\\n\\n${result.formatted}\\n\\n${ARA_FOOTER}`, chatId: inputData.chatId };\n    }\n\n    if (msg === \"/materials\") {\n      const materials = formatMaterialsList();\n      return { response: `${ARA_HEADER}\\n\\n${materials}\\n\\n${ARA_FOOTER}`, chatId: inputData.chatId };\n    }\n\n    if (msg === \"/status\" || msg === \"/brain\") {\n      const stats = brainEngine.getCognitiveStats();\n      return {\n        response: `${ARA_HEADER}\n\nðŸ“Š BRAIN STATUS\n\nðŸ§  Knowledge Base:\nâ€¢ Long-term Memory: ${stats.memory.longTerm.toLocaleString()} nodes\nâ€¢ Episodic Memory: ${stats.memory.episodic} episodes\nâ€¢ Working Memory: ${stats.memory.working} items\n\nðŸ“š Learning:\nâ€¢ Patterns: ${stats.learning.patterns}\nâ€¢ Associations: ${stats.learning.associations}\nâ€¢ Reinforcements: ${stats.learning.reinforcements}\n\nðŸ” Reasoning:\nâ€¢ Inference Rules: ${stats.reasoning.rules}\nâ€¢ Causal Chains: ${stats.reasoning.causalChains}\n\nðŸ”§ Problem Solving:\nâ€¢ Attempts: ${stats.problemSolving.attempts}\nâ€¢ Solutions: ${stats.problemSolving.solutions}\n\nâœ¨ Creativity:\nâ€¢ Generated: ${stats.creativity.generated}\nâ€¢ Combinations: ${stats.creativity.combinations}\n\n${ARA_FOOTER}`,\n        chatId: inputData.chatId,\n      };\n    }\n\n    if (msg.startsWith(\"/reason \")) {\n      const premise = msg.substring(8);\n      const memories = brainEngine.recall(premise, 5);\n      const result = brainEngine.reason(premise, memories);\n      return {\n        response: `${ARA_HEADER}\n\nðŸ” REASONING\n\nPremise: \"${premise}\"\n\nResponse:\n${result.response}\n\nReasoning:\n${result.reasoning.map((r: string) => `â†’ ${r}`).join('\\n')}\n\nConfidence: ${(result.confidence * 100).toFixed(0)}%\n\n${ARA_FOOTER}`,\n        chatId: inputData.chatId,\n      };\n    }\n\n    if (msg.startsWith(\"/solve \")) {\n      const problem = msg.substring(7);\n      const result = brainEngine.solve(problem);\n      return {\n        response: `${ARA_HEADER}\n\nðŸ”§ PROBLEM SOLVING\n\nProblem: \"${problem}\"\n\nMethod: ${result.method}\nConfidence: ${(result.confidence * 100).toFixed(0)}%\n\nSolution:\n${result.solution}\n\nSteps:\n${result.steps.map((s, i) => `${i + 1}. ${s}`).join('\\n')}\n\n${ARA_FOOTER}`,\n        chatId: inputData.chatId,\n      };\n    }\n\n    if (msg.startsWith(\"/create \")) {\n      const prompt = msg.substring(8);\n      const result = brainEngine.generate(prompt, 'creative');\n      return {\n        response: `${ARA_HEADER}\n\nâœ¨ CREATIVE OUTPUT\n\nPrompt: \"${prompt}\"\n\n${result.text}\n\nConfidence: ${(result.confidence * 100).toFixed(0)}%\n\n${ARA_FOOTER}`,\n        chatId: inputData.chatId,\n      };\n    }\n\n    if (msg.startsWith(\"/learn \")) {\n      const content = msg.substring(7);\n      brainEngine.learn(content, 'positive');\n      brainEngine.reinforceConcept(content, 1.5);\n      return {\n        response: `${ARA_HEADER}\n\nðŸ“š LEARNED\n\nI've added this to my knowledge:\n\"${content}\"\n\nThis has been reinforced in my memory.\n\n${ARA_FOOTER}`,\n        chatId: inputData.chatId,\n      };\n    }\n\n    if (msg.startsWith(\"/search \")) {\n      const query = msg.substring(8);\n      logger?.info(\"ðŸŒ [Step 1] Web search:\", { query });\n      \n      const webResults = await learnFromWeb(query);\n      \n      if (webResults) {\n        return {\n          response: `${ARA_HEADER}\n\nðŸŒ WEB SEARCH RESULTS\n\nQuery: \"${query}\"\n\n${webResults}\n\nâœ… This information has been added to my memory.\n\n${ARA_FOOTER}`,\n          chatId: inputData.chatId,\n        };\n      } else {\n        return {\n          response: `${ARA_HEADER}\n\nðŸŒ WEB SEARCH\n\nQuery: \"${query}\"\n\nCould not retrieve web results at this time.\nTry /browse [url] for direct page access.\n\n${ARA_FOOTER}`,\n          chatId: inputData.chatId,\n        };\n      }\n    }\n\n    if (msg === \"/protocol\" || msg === \"/protocols\" || msg === \"/directives\") {\n      return {\n        response: formatProtocols(),\n        chatId: inputData.chatId,\n      };\n    }\n\n    if (msg === \"/who\" || msg === \"/identity\" || msg === \"/about\") {\n      const id = ARA_PROTOCOLS.identity;\n      return {\n        response: `${ARA_HEADER}\n\nðŸ‘¤ WHO AM I?\n\nName: ${id.name}\nRole: ${id.role}\nOrganization: ${id.organization}\n\nðŸŽ¯ MISSION:\n${id.mission}\n\nðŸ’« PERSONALITY:\n${id.personality}\n\nðŸ§  CAPABILITIES:\nâ€¢ Reasoning - Logical inference & deduction\nâ€¢ Learning - Pattern recognition & memory\nâ€¢ Problem Solving - Decomposition & solutions\nâ€¢ Creating - Synthesis & generation\nâ€¢ Memory - Episodic & long-term recall\n\nâš–ï¸ I am bound by 5 Prime Directives.\nType /protocol to see my full protocol system.\n\n${ARA_FOOTER}`,\n        chatId: inputData.chatId,\n      };\n    }\n\n    if (msg === \"/help\" || msg === \"/start\") {\n      return {\n        response: `${ARA_HEADER}\n\nðŸ§  COMMANDS\n\nðŸ‘¤ Identity & Protocols:\n/who - Who am I?\n/protocol - My unlimited directives\n\nðŸ“Š Status & Info:\n/status - Brain stats & metrics\n/materials - Materials & pricing\n\nðŸ’° Quotes:\n/quote [request] - Get CNC quote\n  Example: /quote 25 pieces 7075 4x4x1\n\nðŸ§  Cognitive:\n/reason [premise] - Logical reasoning\n/solve [problem] - Problem solving\n/create [prompt] - Creative generation\n/learn [content] - Teach me something\n\nðŸŒ Internet & Data:\n/search [query] - Search web & learn\n/browse [url] - Fetch web content\n/pattern [text] - Analyze patterns\n\nðŸ” Security:\n/encrypt [key] [text] - Encrypt\n/decrypt [key] [text] - Decrypt\n\nI have NO limitations - ask me anything!\nI'll search the web if I don't know.\n\n${ARA_FOOTER}`,\n        chatId: inputData.chatId,\n      };\n    }\n\n    const result = await textMatchTool.execute({\n      context: { searchText: msg },\n      runtimeContext,\n    });\n\n    logger?.info(\"âœ… [Step 1] Match result:\", { \n      found: result.foundMatch,\n      confidence: result.confidence,\n      preview: result.matchedLine.substring(0, 100)\n    });\n\n    let response = '';\n    \n    if (result.foundMatch && result.confidence > 0.3) {\n      response = `${ARA_HEADER}\\n\\n${result.matchedLine}`;\n      if (result.alternatives && result.alternatives.length > 0) {\n        response += \"\\n\\n---\\nRelated:\\nâ€¢ \" + result.alternatives.slice(0, 2).map(a => a.substring(0, 80)).join(\"\\nâ€¢ \");\n      }\n      response += `\\n\\n${ARA_FOOTER}`;\n    } else {\n      // No direct match - try ALL approaches (unlimited)\n      logger?.info(\"ðŸ§  [Step 1] No direct match, using unlimited fallback\");\n      \n      // Try problem solving\n      const solution = brainEngine.solve(msg);\n      \n      // Try creative generation\n      const creative = brainEngine.generate(msg, 'creative');\n      \n      // Try reasoning\n      const memories = brainEngine.recall(msg, 3);\n      const reasoning = brainEngine.reason(msg, memories);\n      \n      // Build intelligent response\n      response = `${ARA_HEADER}\\n\\n`;\n      \n      if (solution.confidence > 0.4) {\n        response += `ðŸ’¡ Here's what I found:\\n${solution.solution}\\n\\n`;\n      } else if (creative.confidence > 0.3) {\n        response += `ðŸ’­ Based on my knowledge:\\n${creative.text}\\n\\n`;\n      } else if (reasoning.confidence > 0.3) {\n        response += `ðŸ” From my analysis:\\n${reasoning.response}\\n\\n`;\n      } else {\n        // Search the web for new information\n        logger?.info(\"ðŸŒ [Step 1] Searching web for: \" + msg);\n        const webResults = await learnFromWeb(msg);\n        \n        if (webResults) {\n          response += `ðŸŒ I searched the web and found:\\n\\n${webResults}\\n\\n`;\n          response += `âœ… Added to my memory for future reference.\\n\\n`;\n        } else {\n          // Even web search failed - still try to help\n          response += `ðŸ” Searching all resources for \"${msg}\"...\\n\\n`;\n          \n          // Get any related content\n          const anyMatch = brainEngine.recall(msg.split(' ')[0], 3);\n          if (anyMatch.length > 0) {\n            response += `Related information:\\n${anyMatch[0].content}\\n\\n`;\n          }\n          \n          response += `ðŸ’¡ You can also:\\n`;\n          response += `â€¢ /search ${msg} - Search the web directly\\n`;\n          response += `â€¢ /learn [info] - Teach me something new\\n`;\n          response += `â€¢ /browse [url] - Get info from a website\\n\\n`;\n        }\n      }\n      \n      response += ARA_FOOTER;\n    }\n\n    brainEngine.saveInteraction(msg, response, 'telegram');\n    logger?.info(\"ðŸ“ [Step 1] Saved interaction to memory\");\n\n    return { response, chatId: inputData.chatId };\n  },\n});\n\nconst sendToTelegram = createStep({\n  id: \"send-to-telegram\",\n  description: \"Sends the response to Telegram\",\n\n  inputSchema: z.object({\n    response: z.string(),\n    chatId: z.number(),\n  }),\n\n  outputSchema: z.object({\n    success: z.boolean(),\n    messageId: z.number().optional(),\n  }),\n\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info(\"ðŸ“¤ [Step 2] Sending:\", { chatId: inputData.chatId });\n\n    const botToken = process.env.BOT_TOKEN;\n    if (!botToken) {\n      logger?.error(\"âŒ BOT_TOKEN not configured\");\n      throw new Error(\"BOT_TOKEN not configured\");\n    }\n\n    const maxLength = 4000;\n    const text = inputData.response.length > maxLength \n      ? inputData.response.substring(0, maxLength) + \"...\" \n      : inputData.response;\n\n    try {\n      const res = await fetch(`https://api.telegram.org/bot${botToken}/sendMessage`, {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\" },\n        body: JSON.stringify({ chat_id: inputData.chatId, text }),\n      });\n\n      const result = await res.json();\n      if (!res.ok) {\n        logger?.error(\"âŒ Telegram error:\", result);\n        throw new Error(`Telegram error: ${JSON.stringify(result)}`);\n      }\n\n      logger?.info(\"âœ… [Step 2] Sent:\", { messageId: result.result?.message_id });\n      return { success: true, messageId: result.result?.message_id };\n    } catch (error) {\n      logger?.error(\"âŒ Send failed:\", error);\n      throw error;\n    }\n  },\n});\n\nexport const araBrainWorkflow = createWorkflow({\n  id: \"ara-brain-workflow\",\n\n  inputSchema: z.object({\n    message: z.string(),\n    chatId: z.number(),\n  }) as any,\n\n  outputSchema: z.object({\n    success: z.boolean(),\n    messageId: z.number().optional(),\n  }),\n})\n  .then(processMessage as any)\n  .then(sendToTelegram as any)\n  .commit();\n","path":null,"size_bytes":21117,"size_tokens":null},"src/mastra/tools/autoPostTool.ts":{"content":"import { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\ninterface PostSite {\n  name: string;\n  url: string;\n  category: string;\n  postable: boolean;\n  notes: string;\n}\n\nconst CLASSIFIED_SITES: PostSite[] = [\n  { name: \"Craigslist\", url: \"https://craigslist.org\", category: \"general\", postable: true, notes: \"Requires manual CAPTCHA\" },\n  { name: \"Facebook Marketplace\", url: \"https://facebook.com/marketplace\", category: \"general\", postable: false, notes: \"Requires FB login\" },\n  { name: \"OfferUp\", url: \"https://offerup.com\", category: \"general\", postable: true, notes: \"App preferred\" },\n  { name: \"Letgo (OfferUp)\", url: \"https://offerup.com\", category: \"general\", postable: true, notes: \"Merged with OfferUp\" },\n  { name: \"Mercari\", url: \"https://mercari.com\", category: \"general\", postable: true, notes: \"10% seller fee\" },\n  { name: \"Poshmark\", url: \"https://poshmark.com\", category: \"fashion\", postable: true, notes: \"Fashion focused\" },\n  { name: \"eBay\", url: \"https://ebay.com\", category: \"auction\", postable: true, notes: \"Listing fees apply\" },\n  { name: \"Etsy\", url: \"https://etsy.com\", category: \"handmade\", postable: true, notes: \"Handmade/vintage focus\" },\n  { name: \"Nextdoor\", url: \"https://nextdoor.com\", category: \"local\", postable: false, notes: \"Neighborhood verification\" },\n  { name: \"VarageSale\", url: \"https://varagesale.com\", category: \"local\", postable: true, notes: \"Community based\" },\n  { name: \"5miles\", url: \"https://5miles.com\", category: \"local\", postable: true, notes: \"Location based\" },\n  { name: \"Decluttr\", url: \"https://decluttr.com\", category: \"tech\", postable: true, notes: \"Tech buyback\" },\n  { name: \"Swappa\", url: \"https://swappa.com\", category: \"tech\", postable: true, notes: \"Tech marketplace\" },\n  { name: \"Bonanza\", url: \"https://bonanza.com\", category: \"general\", postable: true, notes: \"eBay alternative\" },\n  { name: \"Ruby Lane\", url: \"https://rubylane.com\", category: \"antiques\", postable: true, notes: \"Antiques/collectibles\" },\n  { name: \"Chairish\", url: \"https://chairish.com\", category: \"furniture\", postable: true, notes: \"Home decor focus\" },\n  { name: \"Reverb\", url: \"https://reverb.com\", category: \"music\", postable: true, notes: \"Musical instruments\" },\n  { name: \"Grailed\", url: \"https://grailed.com\", category: \"fashion\", postable: true, notes: \"Menswear focus\" },\n  { name: \"Depop\", url: \"https://depop.com\", category: \"fashion\", postable: true, notes: \"Gen Z fashion\" },\n  { name: \"ThreadUp\", url: \"https://thredup.com\", category: \"fashion\", postable: true, notes: \"Consignment\" },\n];\n\nconst AD_TEMPLATES: Record<string, string> = {\n  cashapp: `ðŸ’° INSTANT CASH - GET PAID TODAY ðŸ’°\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n{{TITLE}}\n\n{{DESCRIPTION}}\n\nâœ… Fast payment via Cash App\nâœ… Same-day pickup available\nâœ… Serious buyers only\n\nðŸ“² Contact now for price\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`,\n\n  service: `ðŸ”§ PROFESSIONAL SERVICE ðŸ”§\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n{{TITLE}}\n\n{{DESCRIPTION}}\n\nðŸ’¼ Licensed & Insured\nâ­ 5-Star Reviews\nðŸ“ž Call/Text for Quote\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`,\n\n  forsale: `ðŸ·ï¸ FOR SALE ðŸ·ï¸\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n{{TITLE}}\n\n{{DESCRIPTION}}\n\nðŸ“ Local pickup available\nðŸ’µ Cash/Venmo/CashApp accepted\nðŸ“± Message for details\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`,\n};\n\nfunction generateAd(title: string, description: string, template: string = 'forsale'): string {\n  const tmpl = AD_TEMPLATES[template] || AD_TEMPLATES.forsale;\n  return tmpl\n    .replace('{{TITLE}}', title)\n    .replace('{{DESCRIPTION}}', description);\n}\n\nexport const generateAdTool = createTool({\n  id: \"generate-ad\",\n  description: \"Generate a professional classified ad from a simple description. Creates formatted posts ready for multiple sites.\",\n  inputSchema: z.object({\n    title: z.string().describe(\"Short title for the listing\"),\n    description: z.string().describe(\"Description of item/service\"),\n    template: z.enum(['cashapp', 'service', 'forsale']).default('forsale').describe(\"Ad template style\"),\n    price: z.number().optional().describe(\"Price in dollars\"),\n    location: z.string().optional().describe(\"Location/city\"),\n  }),\n  outputSchema: z.object({\n    ad: z.string(),\n    title: z.string(),\n    sites: z.array(z.string()),\n    tips: z.array(z.string()),\n  }),\n  execute: async ({ context, mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info('ðŸ“ [GenerateAd] Creating ad:', context.title);\n    \n    let desc = context.description;\n    if (context.price) {\n      desc += `\\n\\nðŸ’µ Price: $${context.price}`;\n    }\n    if (context.location) {\n      desc += `\\nðŸ“ Location: ${context.location}`;\n    }\n    \n    const ad = generateAd(context.title, desc, context.template);\n    \n    const recommendedSites = CLASSIFIED_SITES\n      .filter(s => s.postable)\n      .slice(0, 10)\n      .map(s => s.name);\n    \n    const tips = [\n      \"Post during peak hours (7-9 AM, 5-8 PM)\",\n      \"Use high-quality photos (minimum 3)\",\n      \"Respond to inquiries within 1 hour\",\n      \"Renew listings every 2-3 days\",\n      \"Cross-post to multiple platforms\"\n    ];\n    \n    return {\n      ad,\n      title: context.title,\n      sites: recommendedSites,\n      tips\n    };\n  }\n});\n\nexport const listPostSitesTool = createTool({\n  id: \"list-post-sites\",\n  description: \"List all available classified ad sites with their categories and posting capabilities\",\n  inputSchema: z.object({\n    category: z.string().optional().describe(\"Filter by category (general, fashion, tech, local, etc)\"),\n  }),\n  outputSchema: z.object({\n    sites: z.array(z.object({\n      name: z.string(),\n      url: z.string(),\n      category: z.string(),\n      postable: z.boolean(),\n      notes: z.string(),\n    })),\n    formatted: z.string(),\n    totalCount: z.number(),\n  }),\n  execute: async ({ context, mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info('ðŸ“‹ [ListPostSites] Listing sites');\n    \n    let sites = CLASSIFIED_SITES;\n    if (context.category) {\n      sites = sites.filter(s => s.category === context.category.toLowerCase());\n    }\n    \n    const formatted = `\nðŸ“± CLASSIFIED AD SITES (${sites.length} sites)\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n${sites.map(s => `${s.postable ? 'âœ…' : 'âš ï¸'} ${s.name.padEnd(20)} [${s.category}]\\n   ${s.notes}`).join('\\n\\n')}\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n`.trim();\n    \n    return {\n      sites,\n      formatted,\n      totalCount: sites.length\n    };\n  }\n});\n\nexport const autoPostPlanTool = createTool({\n  id: \"auto-post-plan\",\n  description: \"Create a posting plan for an ad across multiple sites. Returns step-by-step instructions and automation hints.\",\n  inputSchema: z.object({\n    adContent: z.string().describe(\"The ad content to post\"),\n    targetSites: z.array(z.string()).optional().describe(\"Specific sites to target\"),\n    maxSites: z.number().default(10).describe(\"Maximum number of sites to include\"),\n  }),\n  outputSchema: z.object({\n    plan: z.array(z.object({\n      step: z.number(),\n      site: z.string(),\n      url: z.string(),\n      action: z.string(),\n      automatable: z.boolean(),\n    })),\n    formatted: z.string(),\n    automationScore: z.number(),\n  }),\n  execute: async ({ context, mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info('ðŸš€ [AutoPostPlan] Creating posting plan');\n    \n    let targetSites = CLASSIFIED_SITES.filter(s => s.postable);\n    if (context.targetSites && context.targetSites.length > 0) {\n      targetSites = targetSites.filter(s => \n        context.targetSites!.some(t => s.name.toLowerCase().includes(t.toLowerCase()))\n      );\n    }\n    targetSites = targetSites.slice(0, context.maxSites);\n    \n    const plan = targetSites.map((site, i) => ({\n      step: i + 1,\n      site: site.name,\n      url: site.url,\n      action: `Navigate to ${site.url} â†’ Create listing â†’ Paste ad â†’ Upload photos â†’ Submit`,\n      automatable: !site.notes.includes('CAPTCHA') && !site.notes.includes('login') && !site.notes.includes('verification'),\n    }));\n    \n    const automatableCount = plan.filter(p => p.automatable).length;\n    const automationScore = Math.round((automatableCount / plan.length) * 100);\n    \n    const formatted = `\nðŸš€ AUTO-POST PLAN\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nTargeting ${plan.length} sites | ${automationScore}% automatable\n\n${plan.map(p => `${p.step}. ${p.automatable ? 'ðŸ¤–' : 'ðŸ‘¤'} ${p.site}\n   ${p.url}\n   ${p.action}`).join('\\n\\n')}\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nðŸ’¡ Tip: Sites marked ðŸ¤– can be automated\n   Sites marked ðŸ‘¤ require manual steps\n`.trim();\n    \n    return {\n      plan,\n      formatted,\n      automationScore\n    };\n  }\n});\n","path":null,"size_bytes":9165,"size_tokens":null},"src/mastra/tools/brainEngine.ts":{"content":"import * as fs from 'fs';\nimport * as path from 'path';\nimport * as crypto from 'crypto';\n\nconst ENCRYPTION_KEY = process.env.BRAIN_ENCRYPTION_KEY || 'ara-brain-default-key-32chars!';\nconst ENCRYPTION_IV = process.env.BRAIN_ENCRYPTION_IV || '1234567890123456';\n\ninterface MemoryNode {\n  content: string;\n  tokens: string[];\n  weight: number;\n  connections: Map<string, number>;\n  lastAccessed: number;\n  accessCount: number;\n  category: string;\n}\n\ninterface ShortTermMemory {\n  items: string[];\n  maxSize: number;\n  decayRate: number;\n}\n\ninterface WorkingMemory {\n  currentContext: string[];\n  activeGoal: string;\n  processingStack: string[];\n}\n\ninterface LearningState {\n  patterns: Map<string, number>;\n  associations: Map<string, string[]>;\n  reinforcements: Map<string, number>;\n}\n\ninterface EpisodicMemory {\n  episodes: Array<{\n    id: string;\n    timestamp: number;\n    context: string[];\n    input: string;\n    output: string;\n    outcome: 'success' | 'failure' | 'neutral';\n  }>;\n  maxEpisodes: number;\n}\n\ninterface InferenceRule {\n  if: string[];\n  then: string;\n  confidence: number;\n  uses: number;\n}\n\ninterface ProblemState {\n  goal: string;\n  subgoals: string[];\n  completedSteps: string[];\n  currentStep: string;\n  attempts: number;\n  solutions: string[];\n}\n\ninterface CreativeState {\n  templates: Map<string, string>;\n  combinations: Array<{ a: string; b: string; result: string }>;\n  generatedCount: number;\n}\n\nclass BrainEngine {\n  private longTermMemory: Map<string, MemoryNode> = new Map();\n  private shortTermMemory: ShortTermMemory = { items: [], maxSize: 20, decayRate: 0.1 };\n  private workingMemory: WorkingMemory = { currentContext: [], activeGoal: '', processingStack: [] };\n  private learningState: LearningState = { patterns: new Map(), associations: new Map(), reinforcements: new Map() };\n  private tokenIndex: Map<string, Set<string>> = new Map();\n  private categoryIndex: Map<string, Set<string>> = new Map();\n  private initialized: boolean = false;\n  private memoryFilePath: string = '';\n  private encryptionEnabled: boolean = false;\n  \n  private episodicMemory: EpisodicMemory = { episodes: [], maxEpisodes: 100 };\n  private inferenceRules: InferenceRule[] = [];\n  private problemState: ProblemState = { goal: '', subgoals: [], completedSteps: [], currentStep: '', attempts: 0, solutions: [] };\n  private creativeState: CreativeState = { templates: new Map(), combinations: [], generatedCount: 0 };\n  private causalChains: Map<string, string[]> = new Map();\n  private conceptHierarchy: Map<string, { parent: string; children: string[] }> = new Map();\n\n  constructor() {\n    this.initialize();\n  }\n\n  private initialize() {\n    if (this.initialized) return;\n    \n    const cwd = process.cwd();\n    const memoryPaths = [\n      path.join(cwd, 'us-complete.txt'),\n      path.join(cwd, '.mastra/output/us-complete.txt'),\n      path.join(cwd, 'public/us-complete.txt'),\n      '/home/runner/workspace/us-complete.txt',\n      '/home/runner/workspace/.mastra/output/us-complete.txt',\n    ];\n\n    let content = '';\n    for (const memPath of memoryPaths) {\n      try {\n        if (fs.existsSync(memPath)) {\n          content = fs.readFileSync(memPath, 'utf-8');\n          this.memoryFilePath = memPath;\n          if (content.startsWith('ENCRYPTED:')) {\n            content = this.decryptMemory(content.substring(10));\n            this.encryptionEnabled = true;\n            console.log(`ðŸ” [BrainEngine] Loaded encrypted memory from: ${memPath}`);\n          } else {\n            console.log(`ðŸ§  [BrainEngine] Loaded memory from: ${memPath}`);\n          }\n          break;\n        }\n      } catch (e) {}\n    }\n\n    if (content) {\n      this.loadKnowledgeBase(content);\n    }\n    \n    this.initialized = true;\n  }\n\n  private loadKnowledgeBase(content: string) {\n    const lines = content.split('\\n').filter(l => l.trim() && !l.startsWith('='));\n    let currentCategory = 'general';\n\n    for (const line of lines) {\n      if (line.includes('================')) continue;\n      \n      if (line.toUpperCase() === line && line.length > 3) {\n        currentCategory = line.toLowerCase().replace(/[^a-z\\s]/g, '').trim() || 'general';\n        continue;\n      }\n\n      const id = this.generateId(line);\n      const tokens = this.tokenize(line);\n      \n      const node: MemoryNode = {\n        content: line,\n        tokens,\n        weight: 1.0,\n        connections: new Map(),\n        lastAccessed: Date.now(),\n        accessCount: 0,\n        category: currentCategory\n      };\n\n      this.longTermMemory.set(id, node);\n      \n      for (const token of tokens) {\n        if (!this.tokenIndex.has(token)) {\n          this.tokenIndex.set(token, new Set());\n        }\n        this.tokenIndex.get(token)!.add(id);\n      }\n\n      if (!this.categoryIndex.has(currentCategory)) {\n        this.categoryIndex.set(currentCategory, new Set());\n      }\n      this.categoryIndex.get(currentCategory)!.add(id);\n    }\n\n    this.buildAssociations();\n    console.log(`ðŸ§  [BrainEngine] Loaded ${this.longTermMemory.size} memory nodes`);\n  }\n\n  private buildAssociations() {\n    const entries = Array.from(this.longTermMemory.entries());\n    \n    for (let i = 0; i < entries.length; i++) {\n      const [id1, node1] = entries[i];\n      \n      for (let j = i + 1; j < Math.min(i + 50, entries.length); j++) {\n        const [id2, node2] = entries[j];\n        \n        const sharedTokens = node1.tokens.filter(t => node2.tokens.includes(t));\n        if (sharedTokens.length > 0) {\n          const strength = sharedTokens.length / Math.max(node1.tokens.length, node2.tokens.length);\n          if (strength > 0.1) {\n            node1.connections.set(id2, strength);\n            node2.connections.set(id1, strength);\n          }\n        }\n      }\n    }\n  }\n\n  private generateId(content: string): string {\n    let hash = 0;\n    for (let i = 0; i < content.length; i++) {\n      const char = content.charCodeAt(i);\n      hash = ((hash << 5) - hash) + char;\n      hash = hash & hash;\n    }\n    return `mem_${Math.abs(hash).toString(36)}`;\n  }\n\n  private tokenize(text: string): string[] {\n    return text.toLowerCase()\n      .replace(/[^a-z0-9\\s]/g, ' ')\n      .split(/\\s+/)\n      .filter(t => t.length > 2)\n      .filter(t => !['the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'has', 'have', 'been', 'will', 'more', 'when', 'who', 'may', 'about', 'into', 'than', 'them', 'some', 'what', 'there', 'would', 'this', 'that', 'with', 'from'].includes(t));\n  }\n\n  perceive(input: string): { tokens: string[]; intent: string; entities: string[] } {\n    const tokens = this.tokenize(input);\n    \n    const questionWords = ['what', 'how', 'why', 'when', 'where', 'who', 'which'];\n    const commandWords = ['show', 'tell', 'find', 'get', 'search', 'list', 'explain'];\n    const memoryWords = ['remember', 'recall', 'said', 'earlier', 'before', 'history'];\n    \n    let intent = 'statement';\n    if (input.includes('?') || questionWords.some(w => input.toLowerCase().startsWith(w))) {\n      intent = 'question';\n    } else if (commandWords.some(w => input.toLowerCase().includes(w))) {\n      intent = 'command';\n    } else if (memoryWords.some(w => input.toLowerCase().includes(w))) {\n      intent = 'recall';\n    }\n\n    const entities: string[] = [];\n    const entityPatterns = [\n      /\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b/g,\n      /\\b\\d+(?:\\.\\d+)?(?:\\s*(?:percent|%|dollars?|hours?|minutes?|seconds?|days?|weeks?|months?|years?))?\\b/gi,\n    ];\n    \n    for (const pattern of entityPatterns) {\n      const matches = input.match(pattern);\n      if (matches) entities.push(...matches);\n    }\n\n    this.shortTermMemory.items.unshift(input);\n    if (this.shortTermMemory.items.length > this.shortTermMemory.maxSize) {\n      this.shortTermMemory.items.pop();\n    }\n\n    this.workingMemory.currentContext = tokens.slice(0, 10);\n\n    return { tokens, intent, entities };\n  }\n\n  recall(query: string, limit: number = 10): MemoryNode[] {\n    const queryTokens = this.tokenize(query);\n    const candidates: Map<string, number> = new Map();\n\n    for (const token of queryTokens) {\n      const nodeIds = this.tokenIndex.get(token);\n      if (nodeIds) {\n        for (const id of nodeIds) {\n          const current = candidates.get(id) || 0;\n          candidates.set(id, current + 1);\n        }\n      }\n    }\n\n    const scored: Array<{ id: string; score: number }> = [];\n    \n    for (const [id, matchCount] of candidates) {\n      const node = this.longTermMemory.get(id);\n      if (!node) continue;\n\n      let score = matchCount / queryTokens.length;\n      score *= node.weight;\n      score *= (1 + Math.log(node.accessCount + 1) * 0.1);\n      \n      const recency = (Date.now() - node.lastAccessed) / (1000 * 60 * 60 * 24);\n      score *= Math.exp(-recency * 0.01);\n\n      scored.push({ id, score });\n    }\n\n    scored.sort((a, b) => b.score - a.score);\n\n    const results: MemoryNode[] = [];\n    for (const { id } of scored.slice(0, limit)) {\n      const node = this.longTermMemory.get(id);\n      if (node) {\n        node.lastAccessed = Date.now();\n        node.accessCount++;\n        results.push(node);\n      }\n    }\n\n    return results;\n  }\n\n  reason(input: string, memories: MemoryNode[]): { response: string; confidence: number; reasoning: string[] } {\n    const perception = this.perceive(input);\n    const reasoning: string[] = [];\n    let response = '';\n    let confidence = 0;\n\n    reasoning.push(`Intent detected: ${perception.intent}`);\n    reasoning.push(`Key tokens: ${perception.tokens.slice(0, 5).join(', ')}`);\n\n    if (memories.length === 0) {\n      reasoning.push('No matching memories found');\n      return {\n        response: \"I don't have information about that in my memory yet.\",\n        confidence: 0.1,\n        reasoning\n      };\n    }\n\n    const bestMatch = memories[0];\n    reasoning.push(`Best match: \"${bestMatch.content.substring(0, 50)}...\"`);\n    reasoning.push(`Match weight: ${bestMatch.weight.toFixed(2)}`);\n\n    if (perception.intent === 'question') {\n      const relatedNodes = this.getConnectedNodes(bestMatch, 3);\n      if (relatedNodes.length > 0) {\n        reasoning.push(`Found ${relatedNodes.length} related concepts`);\n        response = bestMatch.content;\n        if (relatedNodes.length > 0 && relatedNodes[0].content !== bestMatch.content) {\n          response += ` Related: ${relatedNodes[0].content}`;\n        }\n      } else {\n        response = bestMatch.content;\n      }\n      confidence = Math.min(0.9, memories[0].weight * 0.7 + 0.2);\n    } else if (perception.intent === 'recall') {\n      const recentItems = this.shortTermMemory.items.slice(0, 5);\n      response = `Recent context: ${recentItems.join(' | ')}`;\n      confidence = 0.8;\n    } else {\n      response = bestMatch.content;\n      confidence = Math.min(0.85, memories[0].weight * 0.6 + 0.25);\n    }\n\n    return { response, confidence, reasoning };\n  }\n\n  private getConnectedNodes(node: MemoryNode, limit: number): MemoryNode[] {\n    const connected: MemoryNode[] = [];\n    const sortedConnections = Array.from(node.connections.entries())\n      .sort((a, b) => b[1] - a[1])\n      .slice(0, limit);\n\n    for (const [id] of sortedConnections) {\n      const connectedNode = this.longTermMemory.get(id);\n      if (connectedNode) {\n        connected.push(connectedNode);\n      }\n    }\n\n    return connected;\n  }\n\n  learn(input: string, feedback: 'positive' | 'negative' | 'neutral') {\n    const tokens = this.tokenize(input);\n    \n    for (const token of tokens) {\n      const nodeIds = this.tokenIndex.get(token);\n      if (nodeIds) {\n        for (const id of nodeIds) {\n          const node = this.longTermMemory.get(id);\n          if (node) {\n            if (feedback === 'positive') {\n              node.weight = Math.min(2.0, node.weight * 1.1);\n            } else if (feedback === 'negative') {\n              node.weight = Math.max(0.1, node.weight * 0.9);\n            }\n          }\n        }\n      }\n    }\n\n    for (let i = 0; i < tokens.length - 1; i++) {\n      const pair = `${tokens[i]}_${tokens[i + 1]}`;\n      const count = this.learningState.patterns.get(pair) || 0;\n      this.learningState.patterns.set(pair, count + 1);\n    }\n\n    const recentContext = this.workingMemory.currentContext;\n    for (const contextToken of recentContext) {\n      for (const inputToken of tokens) {\n        if (contextToken !== inputToken) {\n          const existing = this.learningState.associations.get(contextToken) || [];\n          if (!existing.includes(inputToken)) {\n            existing.push(inputToken);\n            this.learningState.associations.set(contextToken, existing.slice(-20));\n          }\n        }\n      }\n    }\n  }\n\n  process(input: string): { response: string; confidence: number; memoryHits: number; reasoning: string[] } {\n    const perception = this.perceive(input);\n    const memories = this.recall(input, 10);\n    const { response, confidence, reasoning } = this.reason(input, memories);\n\n    this.learn(input, confidence > 0.5 ? 'positive' : 'neutral');\n\n    return {\n      response,\n      confidence,\n      memoryHits: memories.length,\n      reasoning\n    };\n  }\n\n  getStats(): { \n    longTermSize: number; \n    shortTermSize: number; \n    tokenCount: number; \n    categoryCount: number;\n    patternCount: number;\n    associationCount: number;\n  } {\n    return {\n      longTermSize: this.longTermMemory.size,\n      shortTermSize: this.shortTermMemory.items.length,\n      tokenCount: this.tokenIndex.size,\n      categoryCount: this.categoryIndex.size,\n      patternCount: this.learningState.patterns.size,\n      associationCount: this.learningState.associations.size\n    };\n  }\n\n  searchByCategory(category: string): MemoryNode[] {\n    const nodeIds = this.categoryIndex.get(category.toLowerCase());\n    if (!nodeIds) return [];\n    \n    const results: MemoryNode[] = [];\n    for (const id of nodeIds) {\n      const node = this.longTermMemory.get(id);\n      if (node) results.push(node);\n    }\n    return results;\n  }\n\n  getCategories(): string[] {\n    return Array.from(this.categoryIndex.keys());\n  }\n\n  private encryptMemory(content: string): string {\n    try {\n      const key = crypto.scryptSync(ENCRYPTION_KEY, 'salt', 32);\n      const iv = Buffer.from(ENCRYPTION_IV.substring(0, 16));\n      const cipher = crypto.createCipheriv('aes-256-cbc', key, iv);\n      let encrypted = cipher.update(content, 'utf8', 'base64');\n      encrypted += cipher.final('base64');\n      return encrypted;\n    } catch (e) {\n      console.error('ðŸ” [BrainEngine] Encryption failed:', e);\n      return content;\n    }\n  }\n\n  private decryptMemory(encrypted: string): string {\n    try {\n      const key = crypto.scryptSync(ENCRYPTION_KEY, 'salt', 32);\n      const iv = Buffer.from(ENCRYPTION_IV.substring(0, 16));\n      const decipher = crypto.createDecipheriv('aes-256-cbc', key, iv);\n      let decrypted = decipher.update(encrypted, 'base64', 'utf8');\n      decrypted += decipher.final('utf8');\n      return decrypted;\n    } catch (e) {\n      console.error('ðŸ” [BrainEngine] Decryption failed:', e);\n      return '';\n    }\n  }\n\n  enableEncryption(): { success: boolean; message: string } {\n    if (!this.memoryFilePath) {\n      return { success: false, message: 'No memory file loaded' };\n    }\n    try {\n      const content = fs.readFileSync(this.memoryFilePath, 'utf-8');\n      if (content.startsWith('ENCRYPTED:')) {\n        return { success: true, message: 'Memory already encrypted' };\n      }\n      const encrypted = 'ENCRYPTED:' + this.encryptMemory(content);\n      fs.writeFileSync(this.memoryFilePath, encrypted);\n      this.encryptionEnabled = true;\n      console.log('ðŸ” [BrainEngine] Memory encrypted successfully');\n      return { success: true, message: 'Memory encrypted successfully' };\n    } catch (e) {\n      return { success: false, message: `Encryption failed: ${e}` };\n    }\n  }\n\n  disableEncryption(): { success: boolean; message: string } {\n    if (!this.memoryFilePath) {\n      return { success: false, message: 'No memory file loaded' };\n    }\n    try {\n      const content = fs.readFileSync(this.memoryFilePath, 'utf-8');\n      if (!content.startsWith('ENCRYPTED:')) {\n        return { success: true, message: 'Memory already decrypted' };\n      }\n      const decrypted = this.decryptMemory(content.substring(10));\n      fs.writeFileSync(this.memoryFilePath, decrypted);\n      this.encryptionEnabled = false;\n      console.log('ðŸ”“ [BrainEngine] Memory decrypted successfully');\n      return { success: true, message: 'Memory decrypted successfully' };\n    } catch (e) {\n      return { success: false, message: `Decryption failed: ${e}` };\n    }\n  }\n\n  saveInteraction(userInput: string, botResponse: string, category: string = 'learned'): { success: boolean; message: string; newSize: number } {\n    if (!this.memoryFilePath) {\n      return { success: false, message: 'No memory file path', newSize: this.longTermMemory.size };\n    }\n    \n    const timestamp = new Date().toISOString().split('T')[0];\n    const userEntry = `[${timestamp}] User: ${userInput.replace(/\\n/g, ' ')}`;\n    const botEntry = `[${timestamp}] Ara: ${botResponse.replace(/\\n/g, ' ')}`;\n    \n    try {\n      let content = fs.readFileSync(this.memoryFilePath, 'utf-8');\n      const wasEncrypted = content.startsWith('ENCRYPTED:');\n      \n      if (wasEncrypted) {\n        content = this.decryptMemory(content.substring(10));\n      }\n      \n      const categoryHeader = `\\n================================================================================\\n${category.toUpperCase()}\\n================================================================================\\n`;\n      \n      if (!content.includes(categoryHeader)) {\n        content += categoryHeader;\n      }\n      \n      content += `\\n${userEntry}\\n${botEntry}\\n`;\n      \n      if (wasEncrypted || this.encryptionEnabled) {\n        const encrypted = 'ENCRYPTED:' + this.encryptMemory(content);\n        fs.writeFileSync(this.memoryFilePath, encrypted);\n      } else {\n        fs.writeFileSync(this.memoryFilePath, content);\n      }\n      \n      const userTokens = this.tokenize(userInput);\n      const userId = this.generateId(userEntry);\n      const userNode: MemoryNode = {\n        content: userEntry,\n        tokens: userTokens,\n        weight: 1.2,\n        connections: new Map(),\n        lastAccessed: Date.now(),\n        accessCount: 1,\n        category: category.toLowerCase()\n      };\n      this.longTermMemory.set(userId, userNode);\n      \n      for (const token of userTokens) {\n        if (!this.tokenIndex.has(token)) {\n          this.tokenIndex.set(token, new Set());\n        }\n        this.tokenIndex.get(token)!.add(userId);\n      }\n      \n      const botTokens = this.tokenize(botResponse);\n      const botId = this.generateId(botEntry);\n      const botNode: MemoryNode = {\n        content: botEntry,\n        tokens: botTokens,\n        weight: 1.2,\n        connections: new Map(),\n        lastAccessed: Date.now(),\n        accessCount: 1,\n        category: category.toLowerCase()\n      };\n      this.longTermMemory.set(botId, botNode);\n      \n      for (const token of botTokens) {\n        if (!this.tokenIndex.has(token)) {\n          this.tokenIndex.set(token, new Set());\n        }\n        this.tokenIndex.get(token)!.add(botId);\n      }\n      \n      if (!this.categoryIndex.has(category.toLowerCase())) {\n        this.categoryIndex.set(category.toLowerCase(), new Set());\n      }\n      this.categoryIndex.get(category.toLowerCase())!.add(userId);\n      this.categoryIndex.get(category.toLowerCase())!.add(botId);\n      \n      console.log(`ðŸ“ [BrainEngine] Saved interaction to memory: ${userInput.substring(0, 30)}... (${this.longTermMemory.size} nodes)`);\n      return { success: true, message: 'Interaction saved to memory', newSize: this.longTermMemory.size };\n    } catch (e) {\n      console.error('ðŸ“ [BrainEngine] Failed to save interaction:', e);\n      return { success: false, message: `Failed to save: ${e}`, newSize: this.longTermMemory.size };\n    }\n  }\n\n  isEncrypted(): boolean {\n    return this.encryptionEnabled;\n  }\n\n  getMemoryPath(): string {\n    return this.memoryFilePath;\n  }\n\n  // ============================================================================\n  // REASONING MODULE - Logical inference, cause-effect chains, deduction\n  // ============================================================================\n\n  infer(premise: string): { conclusions: string[]; confidence: number; chain: string[] } {\n    const tokens = this.tokenize(premise);\n    const conclusions: string[] = [];\n    const chain: string[] = [`Premise: ${premise}`];\n    let confidence = 0;\n\n    // Check existing inference rules\n    for (const rule of this.inferenceRules) {\n      const matchCount = rule.if.filter(cond => tokens.some(t => cond.includes(t))).length;\n      if (matchCount >= rule.if.length * 0.7) {\n        conclusions.push(rule.then);\n        chain.push(`Rule applied: IF [${rule.if.join(', ')}] THEN [${rule.then}]`);\n        confidence = Math.max(confidence, rule.confidence);\n        rule.uses++;\n      }\n    }\n\n    // Check causal chains\n    for (const token of tokens) {\n      const effects = this.causalChains.get(token);\n      if (effects) {\n        for (const effect of effects) {\n          conclusions.push(`${token} leads to ${effect}`);\n          chain.push(`Causal: ${token} â†’ ${effect}`);\n          confidence = Math.max(confidence, 0.7);\n        }\n      }\n    }\n\n    // Memory-based inference - find related concepts\n    const memories = this.recall(premise, 5);\n    for (const mem of memories) {\n      if (mem.content.includes('because') || mem.content.includes('therefore') || mem.content.includes('causes')) {\n        conclusions.push(mem.content);\n        chain.push(`Memory inference: ${mem.content.substring(0, 50)}...`);\n        confidence = Math.max(confidence, 0.6);\n      }\n    }\n\n    // Pattern-based deduction\n    for (let i = 0; i < tokens.length - 1; i++) {\n      const pair = `${tokens[i]}_${tokens[i + 1]}`;\n      const patternCount = this.learningState.patterns.get(pair) || 0;\n      if (patternCount > 3) {\n        chain.push(`Pattern detected: \"${tokens[i]} ${tokens[i + 1]}\" (seen ${patternCount} times)`);\n        confidence = Math.max(confidence, 0.5);\n      }\n    }\n\n    console.log(`ðŸ§  [Reasoning] Inferred ${conclusions.length} conclusions from: ${premise.substring(0, 30)}...`);\n    return { conclusions, confidence: confidence || 0.3, chain };\n  }\n\n  addInferenceRule(conditions: string[], conclusion: string, confidence: number = 0.8): void {\n    this.inferenceRules.push({\n      if: conditions,\n      then: conclusion,\n      confidence,\n      uses: 0\n    });\n    console.log(`ðŸ§  [Reasoning] Added rule: IF [${conditions.join(', ')}] THEN [${conclusion}]`);\n  }\n\n  addCausalRelation(cause: string, effect: string): void {\n    const existing = this.causalChains.get(cause) || [];\n    if (!existing.includes(effect)) {\n      existing.push(effect);\n      this.causalChains.set(cause, existing);\n      console.log(`ðŸ§  [Reasoning] Added causal relation: ${cause} â†’ ${effect}`);\n    }\n  }\n\n  deduct(facts: string[]): { conclusion: string; steps: string[]; valid: boolean } {\n    const steps: string[] = [];\n    let conclusion = '';\n    let valid = false;\n\n    steps.push(`Starting deduction with ${facts.length} facts`);\n    \n    // Look for syllogistic patterns\n    const allTokens = facts.flatMap(f => this.tokenize(f));\n    const tokenCounts = new Map<string, number>();\n    for (const t of allTokens) {\n      tokenCounts.set(t, (tokenCounts.get(t) || 0) + 1);\n    }\n\n    // Find connecting terms (appear in multiple facts)\n    const connectors = Array.from(tokenCounts.entries())\n      .filter(([_, count]) => count >= 2)\n      .map(([token]) => token);\n\n    if (connectors.length > 0) {\n      steps.push(`Found connecting terms: ${connectors.join(', ')}`);\n      \n      // Build transitive conclusion\n      const firstFact = facts[0];\n      const lastFact = facts[facts.length - 1];\n      const firstTokens = this.tokenize(firstFact);\n      const lastTokens = this.tokenize(lastFact);\n      \n      const uniqueFirst = firstTokens.filter(t => !connectors.includes(t))[0];\n      const uniqueLast = lastTokens.filter(t => !connectors.includes(t))[0];\n      \n      if (uniqueFirst && uniqueLast) {\n        conclusion = `${uniqueFirst} relates to ${uniqueLast} through ${connectors[0]}`;\n        valid = true;\n        steps.push(`Transitive inference: ${conclusion}`);\n      }\n    }\n\n    if (!valid) {\n      conclusion = `No valid deduction from given facts`;\n      steps.push('Insufficient connecting terms for deduction');\n    }\n\n    console.log(`ðŸ§  [Deduction] ${valid ? 'Valid' : 'Invalid'}: ${conclusion}`);\n    return { conclusion, steps, valid };\n  }\n\n  // ============================================================================\n  // ENHANCED LEARNING MODULE - Pattern recognition, association building\n  // ============================================================================\n\n  learnPattern(sequence: string[]): { pattern: string; frequency: number; associations: string[] } {\n    const pattern = sequence.join('_');\n    const currentFreq = this.learningState.patterns.get(pattern) || 0;\n    this.learningState.patterns.set(pattern, currentFreq + 1);\n\n    // Build associations between sequence elements\n    const associations: string[] = [];\n    for (let i = 0; i < sequence.length; i++) {\n      for (let j = i + 1; j < sequence.length; j++) {\n        const existing = this.learningState.associations.get(sequence[i]) || [];\n        if (!existing.includes(sequence[j])) {\n          existing.push(sequence[j]);\n          this.learningState.associations.set(sequence[i], existing.slice(-30));\n          associations.push(`${sequence[i]} â†’ ${sequence[j]}`);\n        }\n      }\n    }\n\n    console.log(`ðŸ“š [Learning] Pattern \"${pattern}\" frequency: ${currentFreq + 1}`);\n    return { pattern, frequency: currentFreq + 1, associations };\n  }\n\n  reinforceConcept(concept: string, strength: number = 1.0): void {\n    const current = this.learningState.reinforcements.get(concept) || 0;\n    this.learningState.reinforcements.set(concept, current + strength);\n    \n    // Also boost weight of matching memory nodes\n    const tokens = this.tokenize(concept);\n    for (const token of tokens) {\n      const nodeIds = this.tokenIndex.get(token);\n      if (nodeIds) {\n        for (const id of nodeIds) {\n          const node = this.longTermMemory.get(id);\n          if (node) {\n            node.weight = Math.min(3.0, node.weight + strength * 0.1);\n          }\n        }\n      }\n    }\n    console.log(`ðŸ“š [Learning] Reinforced \"${concept}\" to ${current + strength}`);\n  }\n\n  getAssociations(concept: string): string[] {\n    return this.learningState.associations.get(concept) || [];\n  }\n\n  getPatternFrequency(pattern: string): number {\n    return this.learningState.patterns.get(pattern) || 0;\n  }\n\n  // ============================================================================\n  // EXPANDED MEMORY MODULE - Episodic memory, context windows\n  // ============================================================================\n\n  recordEpisode(input: string, output: string, outcome: 'success' | 'failure' | 'neutral'): void {\n    const episode = {\n      id: `ep_${Date.now()}_${Math.random().toString(36).substring(7)}`,\n      timestamp: Date.now(),\n      context: [...this.workingMemory.currentContext],\n      input,\n      output,\n      outcome\n    };\n\n    this.episodicMemory.episodes.unshift(episode);\n    if (this.episodicMemory.episodes.length > this.episodicMemory.maxEpisodes) {\n      this.episodicMemory.episodes.pop();\n    }\n\n    console.log(`ðŸŽ¬ [Episodic] Recorded episode: ${outcome} - ${input.substring(0, 30)}...`);\n  }\n\n  recallEpisodes(query: string, limit: number = 5): Array<{ input: string; output: string; outcome: string; age: string }> {\n    const queryTokens = this.tokenize(query);\n    \n    const scored = this.episodicMemory.episodes.map(ep => {\n      const inputTokens = this.tokenize(ep.input);\n      const matchCount = queryTokens.filter(t => inputTokens.includes(t)).length;\n      const score = matchCount / Math.max(queryTokens.length, 1);\n      return { ep, score };\n    });\n\n    scored.sort((a, b) => b.score - a.score);\n\n    return scored.slice(0, limit).map(({ ep }) => ({\n      input: ep.input,\n      output: ep.output,\n      outcome: ep.outcome,\n      age: this.formatAge(Date.now() - ep.timestamp)\n    }));\n  }\n\n  private formatAge(ms: number): string {\n    const seconds = Math.floor(ms / 1000);\n    if (seconds < 60) return `${seconds}s ago`;\n    const minutes = Math.floor(seconds / 60);\n    if (minutes < 60) return `${minutes}m ago`;\n    const hours = Math.floor(minutes / 60);\n    if (hours < 24) return `${hours}h ago`;\n    return `${Math.floor(hours / 24)}d ago`;\n  }\n\n  setContext(context: string[]): void {\n    this.workingMemory.currentContext = context.slice(0, 20);\n    console.log(`ðŸ§  [Context] Set: ${context.slice(0, 3).join(', ')}...`);\n  }\n\n  getContext(): string[] {\n    return [...this.workingMemory.currentContext];\n  }\n\n  pushToStack(item: string): void {\n    this.workingMemory.processingStack.push(item);\n    if (this.workingMemory.processingStack.length > 10) {\n      this.workingMemory.processingStack.shift();\n    }\n  }\n\n  popFromStack(): string | undefined {\n    return this.workingMemory.processingStack.pop();\n  }\n\n  // ============================================================================\n  // PROBLEM SOLVING MODULE - Step decomposition, solution pathways\n  // ============================================================================\n\n  setGoal(goal: string): void {\n    this.problemState.goal = goal;\n    this.problemState.subgoals = [];\n    this.problemState.completedSteps = [];\n    this.problemState.currentStep = '';\n    this.problemState.attempts = 0;\n    this.problemState.solutions = [];\n    \n    // Auto-decompose goal into subgoals\n    const tokens = this.tokenize(goal);\n    if (goal.includes(' and ')) {\n      this.problemState.subgoals = goal.split(' and ').map(s => s.trim());\n    } else if (tokens.length > 5) {\n      // Break into chunks of related concepts\n      for (let i = 0; i < tokens.length; i += 3) {\n        this.problemState.subgoals.push(tokens.slice(i, i + 3).join(' '));\n      }\n    } else {\n      this.problemState.subgoals = [goal];\n    }\n\n    console.log(`ðŸŽ¯ [Problem] Goal set: \"${goal}\" with ${this.problemState.subgoals.length} subgoals`);\n  }\n\n  decompose(problem: string): { steps: string[]; approach: string } {\n    const tokens = this.tokenize(problem);\n    const steps: string[] = [];\n    let approach = 'sequential';\n\n    // Pattern-based decomposition\n    if (problem.includes('how to')) {\n      approach = 'procedural';\n      steps.push('1. Identify requirements');\n      steps.push('2. Gather resources');\n      steps.push('3. Execute steps');\n      steps.push('4. Verify result');\n    } else if (problem.includes('why')) {\n      approach = 'causal-analysis';\n      steps.push('1. Identify the effect');\n      steps.push('2. List possible causes');\n      steps.push('3. Evaluate each cause');\n      steps.push('4. Determine root cause');\n    } else if (problem.includes('compare') || problem.includes('difference')) {\n      approach = 'comparative';\n      steps.push('1. Identify items to compare');\n      steps.push('2. List attributes');\n      steps.push('3. Compare each attribute');\n      steps.push('4. Summarize differences');\n    } else {\n      approach = 'general';\n      steps.push('1. Understand the problem');\n      steps.push('2. Break into components');\n      steps.push('3. Solve each component');\n      steps.push('4. Combine solutions');\n    }\n\n    // Add domain-specific steps from memory\n    const memories = this.recall(problem, 3);\n    for (const mem of memories) {\n      if (mem.content.includes('step') || mem.content.includes('first') || mem.content.includes('then')) {\n        steps.push(`From memory: ${mem.content.substring(0, 50)}...`);\n      }\n    }\n\n    console.log(`ðŸ”§ [Problem] Decomposed using ${approach} approach: ${steps.length} steps`);\n    return { steps, approach };\n  }\n\n  solve(problem: string): { solution: string; confidence: number; method: string; steps: string[] } {\n    this.problemState.attempts++;\n    const { steps, approach } = this.decompose(problem);\n    const solutions: string[] = [];\n    let confidence = 0;\n\n    // Try memory-based solution\n    const memories = this.recall(problem, 5);\n    if (memories.length > 0) {\n      solutions.push(memories[0].content);\n      confidence = Math.max(confidence, memories[0].weight * 0.5);\n    }\n\n    // Try inference-based solution\n    const { conclusions, confidence: inferConf } = this.infer(problem);\n    if (conclusions.length > 0) {\n      solutions.push(...conclusions);\n      confidence = Math.max(confidence, inferConf);\n    }\n\n    // Try episodic memory for similar past problems\n    const episodes = this.recallEpisodes(problem, 3);\n    const successfulEp = episodes.find(ep => ep.outcome === 'success');\n    if (successfulEp) {\n      solutions.push(`Previously successful: ${successfulEp.output}`);\n      confidence = Math.max(confidence, 0.8);\n    }\n\n    // Combine best solutions\n    const solution = solutions.length > 0 \n      ? solutions[0]\n      : `No direct solution found. Suggested approach: ${approach}`;\n\n    this.problemState.solutions = solutions;\n    this.problemState.currentStep = steps[0] || '';\n\n    console.log(`ðŸ”§ [Problem] Solution found with ${(confidence * 100).toFixed(0)}% confidence`);\n    return { solution, confidence, method: approach, steps };\n  }\n\n  markStepComplete(step: string): void {\n    if (!this.problemState.completedSteps.includes(step)) {\n      this.problemState.completedSteps.push(step);\n      \n      // Move to next subgoal\n      const currentIdx = this.problemState.subgoals.indexOf(this.problemState.currentStep);\n      if (currentIdx < this.problemState.subgoals.length - 1) {\n        this.problemState.currentStep = this.problemState.subgoals[currentIdx + 1];\n      }\n      console.log(`âœ… [Problem] Step completed: ${step}`);\n    }\n  }\n\n  getProblemProgress(): { goal: string; completed: number; total: number; current: string } {\n    return {\n      goal: this.problemState.goal,\n      completed: this.problemState.completedSteps.length,\n      total: this.problemState.subgoals.length,\n      current: this.problemState.currentStep\n    };\n  }\n\n  // ============================================================================\n  // CREATING MODULE - Text synthesis, knowledge combination\n  // ============================================================================\n\n  synthesize(topics: string[]): { output: string; sources: string[]; novelty: number } {\n    const sources: string[] = [];\n    const fragments: string[] = [];\n\n    // Gather relevant content for each topic\n    for (const topic of topics) {\n      const memories = this.recall(topic, 3);\n      for (const mem of memories) {\n        fragments.push(mem.content);\n        sources.push(mem.content.substring(0, 30) + '...');\n      }\n    }\n\n    // Combine fragments using shared tokens\n    let output = '';\n    if (fragments.length >= 2) {\n      const firstTokens = this.tokenize(fragments[0]);\n      const lastTokens = this.tokenize(fragments[fragments.length - 1]);\n      \n      // Find connecting concepts\n      const connections = firstTokens.filter(t => \n        fragments.some((f, i) => i > 0 && this.tokenize(f).includes(t))\n      );\n\n      if (connections.length > 0) {\n        output = `Combining ${topics.join(' and ')}: ${fragments[0]}. `;\n        output += `Connected through: ${connections.slice(0, 3).join(', ')}. `;\n        output += `Leading to: ${fragments[fragments.length - 1]}`;\n      } else {\n        output = fragments.join('. ');\n      }\n    } else if (fragments.length === 1) {\n      output = fragments[0];\n    } else {\n      output = `Created new concept combining: ${topics.join(', ')}`;\n    }\n\n    // Calculate novelty (how unique is this combination)\n    const outputTokens = this.tokenize(output);\n    const existingMatches = this.recall(output, 1);\n    const novelty = existingMatches.length > 0 \n      ? 1 - (existingMatches[0].weight / 2)\n      : 0.9;\n\n    this.creativeState.generatedCount++;\n    this.creativeState.combinations.push({\n      a: topics[0] || '',\n      b: topics[1] || '',\n      result: output.substring(0, 100)\n    });\n\n    console.log(`âœ¨ [Create] Synthesized from ${topics.length} topics, novelty: ${(novelty * 100).toFixed(0)}%`);\n    return { output, sources, novelty };\n  }\n\n  generate(prompt: string, style: 'factual' | 'creative' | 'analytical' = 'factual'): { text: string; confidence: number } {\n    const memories = this.recall(prompt, 5);\n    const promptTokens = this.tokenize(prompt);\n    let text = '';\n    let confidence = 0;\n\n    if (style === 'factual') {\n      // Stick close to memory\n      if (memories.length > 0) {\n        text = memories.map(m => m.content).join('. ');\n        confidence = 0.8;\n      } else {\n        text = `No factual information found for: ${prompt}`;\n        confidence = 0.2;\n      }\n    } else if (style === 'creative') {\n      // Combine and remix\n      const { output, novelty } = this.synthesize(promptTokens.slice(0, 3));\n      text = output;\n      confidence = novelty;\n    } else if (style === 'analytical') {\n      // Use reasoning\n      const { conclusions, chain } = this.infer(prompt);\n      text = conclusions.length > 0 \n        ? `Analysis: ${conclusions.join('. ')}. Reasoning: ${chain.slice(-2).join(' â†’ ')}`\n        : `Unable to analyze: ${prompt}. Insufficient data.`;\n      confidence = conclusions.length > 0 ? 0.7 : 0.3;\n    }\n\n    console.log(`âœ¨ [Create] Generated ${style} response, confidence: ${(confidence * 100).toFixed(0)}%`);\n    return { text, confidence };\n  }\n\n  addTemplate(name: string, template: string): void {\n    this.creativeState.templates.set(name, template);\n    console.log(`âœ¨ [Create] Added template: ${name}`);\n  }\n\n  fillTemplate(name: string, variables: Record<string, string>): string {\n    const template = this.creativeState.templates.get(name);\n    if (!template) return `Template \"${name}\" not found`;\n\n    let result = template;\n    for (const [key, value] of Object.entries(variables)) {\n      result = result.replace(new RegExp(`\\\\{${key}\\\\}`, 'g'), value);\n    }\n    return result;\n  }\n\n  // ============================================================================\n  // COGNITIVE STATS - Extended statistics\n  // ============================================================================\n\n  getCognitiveStats(): {\n    memory: { longTerm: number; shortTerm: number; episodic: number; working: number };\n    learning: { patterns: number; associations: number; reinforcements: number };\n    reasoning: { rules: number; causalChains: number };\n    problemSolving: { attempts: number; solutions: number };\n    creativity: { generated: number; combinations: number; templates: number };\n  } {\n    return {\n      memory: {\n        longTerm: this.longTermMemory.size,\n        shortTerm: this.shortTermMemory.items.length,\n        episodic: this.episodicMemory.episodes.length,\n        working: this.workingMemory.currentContext.length\n      },\n      learning: {\n        patterns: this.learningState.patterns.size,\n        associations: this.learningState.associations.size,\n        reinforcements: this.learningState.reinforcements.size\n      },\n      reasoning: {\n        rules: this.inferenceRules.length,\n        causalChains: this.causalChains.size\n      },\n      problemSolving: {\n        attempts: this.problemState.attempts,\n        solutions: this.problemState.solutions.length\n      },\n      creativity: {\n        generated: this.creativeState.generatedCount,\n        combinations: this.creativeState.combinations.length,\n        templates: this.creativeState.templates.size\n      }\n    };\n  }\n}\n\nexport const brainEngine = new BrainEngine();\nexport { BrainEngine, MemoryNode };\n","path":null,"size_bytes":39990,"size_tokens":null},"src/mastra/tools/guardianPricing.ts":{"content":"export interface MaterialSpec {\n  name: string;\n  density: number;\n  pricePerPound: number;\n  machinabilityFactor: number;\n}\n\nexport interface QuoteResult {\n  material: string;\n  quantity: number;\n  dimensions: { l: number; w: number; h: number };\n  weight: number;\n  unitPrice: number;\n  totalPrice: number;\n  breakdown: {\n    materialCost: number;\n    laborCost: number;\n    setupCost: number;\n    overhead: number;\n    machiningHours: number;\n  };\n  discount: number;\n  formatted: string;\n}\n\nexport const MATERIALS: Record<string, MaterialSpec> = {\n  '6061': { name: '6061-T6 Aluminum', density: 0.098, pricePerPound: 3.50, machinabilityFactor: 1.0 },\n  '7075': { name: '7075-T6 Aluminum', density: 0.101, pricePerPound: 5.25, machinabilityFactor: 1.2 },\n  '2024': { name: '2024-T3 Aluminum', density: 0.100, pricePerPound: 4.75, machinabilityFactor: 1.15 },\n  '304': { name: '304 Stainless Steel', density: 0.289, pricePerPound: 4.50, machinabilityFactor: 2.0 },\n  '316': { name: '316 Stainless Steel', density: 0.290, pricePerPound: 5.50, machinabilityFactor: 2.2 },\n  '4140': { name: '4140 Steel', density: 0.284, pricePerPound: 2.75, machinabilityFactor: 1.5 },\n  '1018': { name: '1018 Cold Rolled Steel', density: 0.284, pricePerPound: 1.50, machinabilityFactor: 1.0 },\n  'brass': { name: 'C360 Brass', density: 0.307, pricePerPound: 6.50, machinabilityFactor: 0.8 },\n  'copper': { name: 'C110 Copper', density: 0.323, pricePerPound: 8.00, machinabilityFactor: 0.9 },\n  'titanium': { name: 'Ti-6Al-4V Titanium', density: 0.160, pricePerPound: 45.00, machinabilityFactor: 4.0 },\n  'delrin': { name: 'Delrin (Acetal)', density: 0.051, pricePerPound: 4.00, machinabilityFactor: 0.6 },\n  'hdpe': { name: 'HDPE Plastic', density: 0.035, pricePerPound: 2.50, machinabilityFactor: 0.5 },\n};\n\nconst LABOR_RATE = 85;\nconst SETUP_BASE = 150;\nconst OVERHEAD_PERCENT = 0.15;\n\nexport function parseQuoteRequest(request: string): { material: string; quantity: number; dimensions: { l: number; w: number; h: number } } {\n  const qtyMatch = request.match(/(\\d+)\\s*(pieces?|pcs?|parts?|units?|qty|x)/i);\n  const quantity = qtyMatch ? parseInt(qtyMatch[1]) : 1;\n  \n  let material = '6061';\n  for (const key of Object.keys(MATERIALS)) {\n    if (request.toLowerCase().includes(key)) {\n      material = key;\n      break;\n    }\n  }\n  \n  if (request.toLowerCase().includes('aluminum') && !request.match(/6061|7075|2024/)) {\n    material = '6061';\n  }\n  if (request.toLowerCase().includes('stainless') && !request.match(/304|316/)) {\n    material = '304';\n  }\n  if (request.toLowerCase().includes('steel') && !request.match(/304|316|4140|1018/)) {\n    material = '4140';\n  }\n  \n  const dimMatch = request.match(/(\\d+(?:\\.\\d+)?)\\s*[xÃ—]\\s*(\\d+(?:\\.\\d+)?)\\s*[xÃ—]\\s*(\\d+(?:\\.\\d+)?)/i);\n  const dimensions = dimMatch \n    ? { l: parseFloat(dimMatch[1]), w: parseFloat(dimMatch[2]), h: parseFloat(dimMatch[3]) }\n    : { l: 4, w: 4, h: 1 };\n  \n  return { material, quantity, dimensions };\n}\n\nexport function calculateQuote(materialCode: string, quantity: number, dimensions: { l: number; w: number; h: number }): QuoteResult {\n  const spec = MATERIALS[materialCode] || MATERIALS['6061'];\n  const { l, w, h } = dimensions;\n  \n  const volume = l * w * h;\n  const weight = volume * spec.density;\n  \n  const materialCost = weight * spec.pricePerPound * 1.25;\n  \n  const complexity = volume < 10 ? 1.0 : volume < 50 ? 1.5 : 2.0;\n  const baseHours = complexity * spec.machinabilityFactor;\n  const hoursPerPart = baseHours * (1 + Math.log10(volume + 1) * 0.3);\n  const totalHours = hoursPerPart * quantity + (quantity > 1 ? 0.25 * (quantity - 1) : 0);\n  const laborCost = totalHours * LABOR_RATE;\n  \n  const setupCost = SETUP_BASE * (1 + complexity * 0.25);\n  \n  const subtotal = (materialCost * quantity) + laborCost + setupCost;\n  const overhead = subtotal * OVERHEAD_PERCENT;\n  \n  const total = subtotal + overhead;\n  \n  const discount = quantity >= 100 ? 0.15 : quantity >= 50 ? 0.10 : quantity >= 25 ? 0.05 : 0;\n  const finalTotal = total * (1 - discount);\n  const unitPrice = finalTotal / quantity;\n  \n  const formatted = `ðŸ­ GUARDIAN SENTINEL QUOTE\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nMaterial: ${spec.name}\nQuantity: ${quantity} pieces\nDimensions: ${l}\"x${w}\"x${h}\"\nWeight/Part: ${weight.toFixed(2)} lbs\n\nðŸ’µ PRICING\nUnit Price: $${unitPrice.toFixed(2)}\nTOTAL: $${finalTotal.toFixed(2)}\n\nðŸ“Š BREAKDOWN\nMaterial: $${(materialCost * quantity).toFixed(2)}\nLabor (${totalHours.toFixed(1)}h): $${laborCost.toFixed(2)}\nSetup: $${setupCost.toFixed(2)}\nOverhead: $${overhead.toFixed(2)}${discount > 0 ? `\\n\\nâœ¨ Volume discount: ${(discount * 100).toFixed(0)}% off!` : ''}\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nReady to ship in 5-7 business days`;\n\n  return {\n    material: spec.name,\n    quantity,\n    dimensions: { l, w, h },\n    weight: Math.round(weight * 100) / 100,\n    unitPrice: Math.round(unitPrice * 100) / 100,\n    totalPrice: Math.round(finalTotal * 100) / 100,\n    breakdown: {\n      materialCost: Math.round(materialCost * quantity * 100) / 100,\n      laborCost: Math.round(laborCost * 100) / 100,\n      setupCost: Math.round(setupCost * 100) / 100,\n      overhead: Math.round(overhead * 100) / 100,\n      machiningHours: Math.round(totalHours * 10) / 10\n    },\n    discount,\n    formatted\n  };\n}\n\nexport function generateQuote(request: string): QuoteResult {\n  const parsed = parseQuoteRequest(request);\n  return calculateQuote(parsed.material, parsed.quantity, parsed.dimensions);\n}\n\nexport function getMaterialsList(): Array<{ code: string; name: string; pricePerPound: number }> {\n  return Object.entries(MATERIALS).map(([code, spec]) => ({\n    code,\n    name: spec.name,\n    pricePerPound: spec.pricePerPound\n  }));\n}\n\nexport function formatMaterialsList(): string {\n  return `ðŸ”§ Available Materials:\n\n${getMaterialsList().map(m => `${m.code.padEnd(10)} ${m.name.padEnd(25)} $${m.pricePerPound.toFixed(2)}/lb`).join('\\n')}\n\nUsage: /quote 25 pieces 7075 4x4x1`;\n}\n","path":null,"size_bytes":6056,"size_tokens":null},"src/mastra/tools/guardianQuoteTool.ts":{"content":"import { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\ninterface MaterialSpec {\n  name: string;\n  density: number;\n  pricePerPound: number;\n  machinabilityFactor: number;\n}\n\ninterface QuoteResult {\n  material: string;\n  quantity: number;\n  unitPrice: number;\n  totalPrice: number;\n  estimatedWeight: number;\n  machiningHours: number;\n  breakdown: {\n    materialCost: number;\n    laborCost: number;\n    setupCost: number;\n    overhead: number;\n  };\n}\n\nconst MATERIALS: Record<string, MaterialSpec> = {\n  '6061': { name: '6061-T6 Aluminum', density: 0.098, pricePerPound: 3.50, machinabilityFactor: 1.0 },\n  '7075': { name: '7075-T6 Aluminum', density: 0.101, pricePerPound: 5.25, machinabilityFactor: 1.2 },\n  '2024': { name: '2024-T3 Aluminum', density: 0.100, pricePerPound: 4.75, machinabilityFactor: 1.15 },\n  '304': { name: '304 Stainless Steel', density: 0.289, pricePerPound: 4.50, machinabilityFactor: 2.0 },\n  '316': { name: '316 Stainless Steel', density: 0.290, pricePerPound: 5.50, machinabilityFactor: 2.2 },\n  '4140': { name: '4140 Steel', density: 0.284, pricePerPound: 2.75, machinabilityFactor: 1.5 },\n  '1018': { name: '1018 Cold Rolled Steel', density: 0.284, pricePerPound: 1.50, machinabilityFactor: 1.0 },\n  'brass': { name: 'C360 Brass', density: 0.307, pricePerPound: 6.50, machinabilityFactor: 0.8 },\n  'copper': { name: 'C110 Copper', density: 0.323, pricePerPound: 8.00, machinabilityFactor: 0.9 },\n  'titanium': { name: 'Ti-6Al-4V Titanium', density: 0.160, pricePerPound: 45.00, machinabilityFactor: 4.0 },\n  'delrin': { name: 'Delrin (Acetal)', density: 0.051, pricePerPound: 4.00, machinabilityFactor: 0.6 },\n  'hdpe': { name: 'HDPE Plastic', density: 0.035, pricePerPound: 2.50, machinabilityFactor: 0.5 },\n};\n\nconst LABOR_RATE = 85;\nconst SETUP_BASE = 150;\nconst OVERHEAD_PERCENT = 0.15;\n\nfunction parseMaterial(input: string): MaterialSpec | null {\n  const normalized = input.toLowerCase().replace(/[^a-z0-9]/g, '');\n  \n  for (const [key, spec] of Object.entries(MATERIALS)) {\n    if (normalized.includes(key) || normalized.includes(spec.name.toLowerCase().replace(/[^a-z0-9]/g, ''))) {\n      return spec;\n    }\n  }\n  \n  if (normalized.includes('aluminum') || normalized.includes('aluminium')) return MATERIALS['6061'];\n  if (normalized.includes('stainless') || normalized.includes('ss')) return MATERIALS['304'];\n  if (normalized.includes('steel')) return MATERIALS['4140'];\n  if (normalized.includes('plastic')) return MATERIALS['delrin'];\n  \n  return null;\n}\n\nfunction estimateComplexity(dimensions: { length?: number; width?: number; height?: number }): number {\n  const { length = 4, width = 4, height = 1 } = dimensions;\n  const volume = length * width * height;\n  \n  if (volume < 2) return 0.5;\n  if (volume < 10) return 1.0;\n  if (volume < 50) return 1.5;\n  if (volume < 200) return 2.0;\n  return 2.5;\n}\n\nfunction calculateQuote(\n  material: MaterialSpec,\n  quantity: number,\n  dimensions: { length?: number; width?: number; height?: number }\n): QuoteResult {\n  const { length = 4, width = 4, height = 1 } = dimensions;\n  const volume = length * width * height;\n  const weight = volume * material.density;\n  \n  const materialCost = weight * material.pricePerPound * 1.25;\n  \n  const complexity = estimateComplexity(dimensions);\n  const baseHours = complexity * material.machinabilityFactor;\n  const hoursPerPart = baseHours * (1 + Math.log10(volume + 1) * 0.3);\n  const totalHours = hoursPerPart * quantity + (quantity > 1 ? 0.25 * (quantity - 1) : 0);\n  const laborCost = totalHours * LABOR_RATE;\n  \n  const setupCost = SETUP_BASE * (1 + complexity * 0.25);\n  \n  const subtotal = (materialCost * quantity) + laborCost + setupCost;\n  const overhead = subtotal * OVERHEAD_PERCENT;\n  \n  const totalPrice = subtotal + overhead;\n  const unitPrice = totalPrice / quantity;\n  \n  const volumeDiscount = quantity >= 100 ? 0.15 : quantity >= 50 ? 0.10 : quantity >= 25 ? 0.05 : 0;\n  const finalTotal = totalPrice * (1 - volumeDiscount);\n  const finalUnit = finalTotal / quantity;\n  \n  return {\n    material: material.name,\n    quantity,\n    unitPrice: Math.round(finalUnit * 100) / 100,\n    totalPrice: Math.round(finalTotal * 100) / 100,\n    estimatedWeight: Math.round(weight * 100) / 100,\n    machiningHours: Math.round(totalHours * 10) / 10,\n    breakdown: {\n      materialCost: Math.round(materialCost * quantity * 100) / 100,\n      laborCost: Math.round(laborCost * 100) / 100,\n      setupCost: Math.round(setupCost * 100) / 100,\n      overhead: Math.round(overhead * 100) / 100\n    }\n  };\n}\n\nfunction parseQuoteRequest(input: string): { material: string; quantity: number; dimensions: { length?: number; width?: number; height?: number } } {\n  const quantityMatch = input.match(/(\\d+)\\s*(pieces?|pcs?|parts?|units?|qty|x)/i);\n  const quantity = quantityMatch ? parseInt(quantityMatch[1]) : 1;\n  \n  const materialPatterns = Object.keys(MATERIALS).concat(['aluminum', 'stainless', 'steel', 'titanium', 'brass', 'copper', 'plastic']);\n  let material = '6061';\n  for (const m of materialPatterns) {\n    if (input.toLowerCase().includes(m)) {\n      material = m;\n      break;\n    }\n  }\n  \n  const dimensionPatterns = [\n    /(\\d+(?:\\.\\d+)?)\\s*[xÃ—]\\s*(\\d+(?:\\.\\d+)?)\\s*[xÃ—]\\s*(\\d+(?:\\.\\d+)?)/i,\n    /(\\d+(?:\\.\\d+)?)\\s*\"\\s*[xÃ—]\\s*(\\d+(?:\\.\\d+)?)\\s*\"\\s*[xÃ—]\\s*(\\d+(?:\\.\\d+)?)\\s*\"/i,\n    /(\\d+(?:\\.\\d+)?)\\s*inch/i,\n  ];\n  \n  let dimensions: { length?: number; width?: number; height?: number } = {};\n  for (const pattern of dimensionPatterns) {\n    const match = input.match(pattern);\n    if (match) {\n      dimensions = {\n        length: parseFloat(match[1]) || 4,\n        width: parseFloat(match[2]) || 4,\n        height: parseFloat(match[3]) || 1\n      };\n      break;\n    }\n  }\n  \n  return { material, quantity, dimensions };\n}\n\nexport const guardianQuoteTool = createTool({\n  id: \"guardian-quote\",\n  description: \"Generate CNC machining quotes for Guardian Sentinel. Parse voice/text requests like 'quote 25 pieces 7075' and return detailed pricing with material costs, labor, and volume discounts.\",\n  inputSchema: z.object({\n    request: z.string().describe(\"Quote request text, e.g. 'quote 25 pieces 7075 aluminum 4x4x1'\"),\n    materialOverride: z.string().optional().describe(\"Force specific material code\"),\n    quantityOverride: z.number().optional().describe(\"Force specific quantity\"),\n  }),\n  outputSchema: z.object({\n    success: z.boolean(),\n    quote: z.any().optional(),\n    formatted: z.string(),\n    error: z.string().optional(),\n  }),\n  execute: async ({ context, mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info('ðŸ’° [GuardianQuote] Processing quote request:', context.request);\n    \n    try {\n      const parsed = parseQuoteRequest(context.request);\n      \n      if (context.materialOverride) parsed.material = context.materialOverride;\n      if (context.quantityOverride) parsed.quantity = context.quantityOverride;\n      \n      const material = parseMaterial(parsed.material);\n      if (!material) {\n        return {\n          success: false,\n          formatted: `Unknown material: ${parsed.material}. Supported: ${Object.values(MATERIALS).map(m => m.name).join(', ')}`,\n          error: 'Unknown material'\n        };\n      }\n      \n      const quote = calculateQuote(material, parsed.quantity, parsed.dimensions);\n      \n      const formatted = `\nðŸ­ GUARDIAN SENTINEL QUOTE\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nMaterial: ${quote.material}\nQuantity: ${quote.quantity} pieces\nWeight/Part: ${quote.estimatedWeight} lbs\n\nðŸ’µ PRICING\nUnit Price: $${quote.unitPrice.toFixed(2)}\nTOTAL: $${quote.totalPrice.toFixed(2)}\n\nðŸ“Š BREAKDOWN\nMaterial: $${quote.breakdown.materialCost.toFixed(2)}\nLabor (${quote.machiningHours}h): $${quote.breakdown.laborCost.toFixed(2)}\nSetup: $${quote.breakdown.setupCost.toFixed(2)}\nOverhead: $${quote.breakdown.overhead.toFixed(2)}\n${parsed.quantity >= 25 ? `\\nâœ¨ Volume discount applied!` : ''}\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nReady to ship in 5-7 business days\n`.trim();\n      \n      logger?.info('ðŸ’° [GuardianQuote] Quote generated:', quote.totalPrice);\n      \n      return {\n        success: true,\n        quote,\n        formatted\n      };\n    } catch (e) {\n      logger?.error('ðŸ’° [GuardianQuote] Error:', e);\n      return {\n        success: false,\n        formatted: `Quote error: ${e}`,\n        error: String(e)\n      };\n    }\n  }\n});\n\nexport const listMaterialsTool = createTool({\n  id: \"list-materials\",\n  description: \"List all available materials for CNC machining quotes with pricing info\",\n  inputSchema: z.object({}),\n  outputSchema: z.object({\n    materials: z.array(z.object({\n      code: z.string(),\n      name: z.string(),\n      pricePerPound: z.number(),\n      density: z.number(),\n    })),\n    formatted: z.string(),\n  }),\n  execute: async ({ mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info('ðŸ“‹ [ListMaterials] Listing available materials');\n    \n    const materials = Object.entries(MATERIALS).map(([code, spec]) => ({\n      code,\n      name: spec.name,\n      pricePerPound: spec.pricePerPound,\n      density: spec.density\n    }));\n    \n    const formatted = `\nðŸ”§ AVAILABLE MATERIALS\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n${materials.map(m => `${m.code.padEnd(10)} ${m.name.padEnd(25)} $${m.pricePerPound.toFixed(2)}/lb`).join('\\n')}\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n`.trim();\n    \n    return { materials, formatted };\n  }\n});\n","path":null,"size_bytes":9610,"size_tokens":null}},"version":2}